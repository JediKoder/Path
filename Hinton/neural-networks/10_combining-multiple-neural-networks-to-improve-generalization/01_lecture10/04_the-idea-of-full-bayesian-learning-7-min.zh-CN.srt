1
00:00:00,000 --> 00:00:05,463
在这个视频中 我将回到
完全贝叶斯学习 

2
00:00:05,463 --> 00:00:10,664
并且再多解释一下它的计算原理 
在接下来的视频中

3
00:00:10,664 --> 00:00:16,017
我将展示它是否实用? 
在完全贝叶斯学习中

4
00:00:16,017 --> 00:00:19,280
我们不尝试寻找参数的最佳设定值   

5
00:00:19,280 --> 00:00:24,207
相反 我们尝试并寻找所有可能设定的
完备后验分布

6
00:00:24,207 --> 00:00:27,337
即对于所有可能的设定值

7
00:00:27,337 --> 00:00:32,598
我们需要一个后验概率密度
然后把所有的概率加在一起.

8
00:00:32,598 --> 00:00:35,795
即使是对所有的最简单的模型进行计算

9
00:00:35,795 --> 00:00:38,991
也会有极大的计算量

10
00:00:38,991 --> 00:00:43,852
因此 在之前的例子中
我们对有偏硬币进行了此类计算

11
00:00:43,852 --> 00:00:48,971
因为它只有一个描述偏移量的参数
但是一般情况下 对于一个神经网

12
00:00:48,971 --> 00:00:52,522
这是不可能的
在我们计算了

13
00:00:52,522 --> 00:00:57,323
参数的所有可能设定值的后验概率之后
我们可以通过

14
00:00:57,323 --> 00:01:02,124
获得所有参数可能设定值的预测值

15
00:01:02,124 --> 00:01:05,367
然后平均所有预测值

16
00:01:05,367 --> 00:01:08,484
把它的后验概率作为权重值

17
00:01:08,484 --> 00:01:11,228
这个操作同样计算量非常大

18
00:01:11,228 --> 00:01:16,465
这样做的好处是
如果我们使用完全贝叶斯算法时

19
00:01:16,465 --> 00:01:19,770
我们可以使用更加复杂的模型
即使我们数据量不够

20
00:01:19,770 --> 00:01:23,200
因此,这里有一个有趣的
哲学辩证点

21
00:01:24,200 --> 00:01:29,735
我们现在对过拟合的概念很熟悉了
当你在使用一个复杂模型

22
00:01:29,735 --> 00:01:34,094
对一个小数据集进行拟合时称为过拟合
但是这并不

23
00:01:34,094 --> 00:01:38,869
影响计算参数的完整后验概率分布

24
00:01:38,869 --> 00:01:44,266
因此通常来说
如果你的数据集较小

25
00:01:44,266 --> 00:01:45,788
你应该使用简单的模型
这是对的

26
00:01:45,788 --> 00:01:51,254
但是这只有当你假设
拟合一个模型意味着寻找一组参数

27
00:01:51,254 --> 00:01:56,677
最佳设定值的时候正确
如果你能找到完整的后验概率分布

28
00:01:56,677 --> 00:02:00,030
这能避免过拟合

29
00:02:00,030 --> 00:02:05,243
如果数据太少
完整后验概率分布将给出

30
00:02:05,243 --> 00:02:10,588
非常模糊的预测结果
因为会产生不同预测值的参数设定值

31
00:02:10,588 --> 00:02:15,541
会有更大的后验概率

32
00:02:15,541 --> 00:02:20,624
如果你获得更多数据
后验概率会更加集中于

33
00:02:20,624 --> 00:02:25,838
参数的几个设定值当中
于是后验预测结果将更加集中

34
00:02:25,838 --> 00:02:28,406
因此这是一个过拟合的经典例子

35
00:02:28,406 --> 00:02:34,018
我们有了6个数据点
并且我们拟合了5阶多项式

36
00:02:34,018 --> 00:02:38,310
因此在很大程度上
它将直接通过所有的数据点

37
00:02:38,310 --> 00:02:42,000
我们同样指定一条只有两阶自由度的直线

38
00:02:42,600 --> 00:02:47,658
因此 你相信哪一个模型
有6个系数能够

39
00:02:47,658 --> 00:02:53,059
完美符合数据的模型
或者只有两个系数

40
00:02:53,059 --> 00:02:58,186
无法很好符合数据的模型
很明显 复杂的模型

41
00:02:58,186 --> 00:03:03,313
拟合的更好 但是你不会相信它
这不合算 而且它将产生

42
00:03:03,313 --> 00:03:06,800
愚蠢的预测值
因此如果你看着这个蓝色的箭头

43
00:03:07,120 --> 00:03:12,033
如果这是输入值
而你要尝试预测输出值

44
00:03:12,033 --> 00:03:16,753
红色的曲线将给出一个
低于所有观察值的预测值

45
00:03:16,753 --> 00:03:21,731
这看起来很疯狂
而绿色线将预测一个

46
00:03:21,731 --> 00:03:24,511
但是任何事情都是会变的

47
00:03:24,511 --> 00:03:29,683
如果不是拟合一个五阶多项式
我们开始之前定一个前提条件

48
00:03:29,683 --> 00:03:34,080
例如系数不能过大

49
00:03:34,080 --> 00:03:39,583
然后 如果我们计算五阶多项式的
完整后验分布

50
00:03:39,610 --> 00:03:44,067
我曾经在图中向你展示过它的分布

51
00:03:44,067 --> 00:03:47,940
加粗的线表示后验概率更大

52
00:03:49,160 --> 00:03:53,510
所以 你将看到这些细线
偏离某一些数据点很远

53
00:03:53,510 --> 00:03:57,920
但是不管怎么说
他们离大部分数据点很近

54
00:03:58,320 --> 00:04:03,160
现在 我们得到了很模糊
但是很灵敏的预测值

55
00:04:03,420 --> 00:04:07,860
因此 在那个蓝色箭头指向的地方
你可以看到不同的模型预测出

56
00:04:07,860 --> 00:04:11,280
不同的值
但是平均下来他们的预测值

57
00:04:11,280 --> 00:04:14,460
和绿线的预测值很相近

58
00:04:14,460 --> 00:04:19,320
从贝叶斯学者的角度看
数据量不应该影响

59
00:04:19,320 --> 00:04:23,520
你的前提假设
和模型的复杂度

60
00:04:24,280 --> 00:04:29,700
一个真正的贝叶斯学者会说
你对于复杂度的提前预知

61
00:04:29,700 --> 00:04:34,990
仅仅是因为你没有收集任何的数据
这并不意味着

62
00:04:34,990 --> 00:04:38,844
你认为事情会非常简单
所以 我们可以在一个神经网络里

63
00:04:38,844 --> 00:04:43,220
近似使用完全贝叶斯算法
如果这个神经网络里只有很少的参数

64
00:04:43,960 --> 00:04:48,020
这个想法是我们给参数空间
分割成网格

65
00:04:48,520 --> 00:04:53,658
每一个参数只能有几个可能值
然后我们

66
00:04:53,658 --> 00:04:57,412
对所有参数的所有可能取值取叉积

67
00:04:57,412 --> 00:05:01,496
现在 我们得到了许多参数空间中的格点

68
00:05:01,496 --> 00:05:06,832
并且在每一个点上
我们可以看到我们的模型预测的如何

69
00:05:06,832 --> 00:05:11,773
如果我们在做监督学习 那便是
模型预测目标输出的好坏

70
00:05:11,773 --> 00:05:15,796
而且我们可以说在那个格点上的

71
00:05:15,796 --> 00:05:23,034
后验概率是
衡量模型预测数据的好坏的产物

72
00:05:23,034 --> 00:05:28,636
在前提条件下 它有多大的几率发生
并且因为所有的数据都是标准化的

73
00:05:28,636 --> 00:05:31,344
后验概率也是[]

74
00:05:32,180 --> 00:05:36,669
这是非常昂贵的 但是注意到它有
一些吸引人的特征

75
00:05:36,669 --> 00:05:41,221
没有包含梯度下降法 也没有
局部最优的问题

76
00:05:41,221 --> 00:05:46,334
在这个空间中我们不沿着一条路径
我们仅仅评估一组点集

77
00:05:46,334 --> 00:05:49,895
一旦我们选定用后验概率分布来
分配每一个格点

78
00:05:49,895 --> 00:05:55,188
然后我们会用它们的全部在测试集上
做预测

79
00:05:55,188 --> 00:05:57,675
那是非常高的代价

80
00:05:57,675 --> 00:06:02,840
但是当没有太多数据时 它比最大似然估计和
最大后验估计取得的效果要好很多

81
00:06:02,840 --> 00:06:08,561
所以我们预测测试输出时的方法是

82
00:06:08,561 --> 00:06:15,164
给定测试输入 用我们的话来说（is we say）是
测试输出的概率，给定测试输入

83
00:06:15,164 --> 00:06:19,783
是网格点概率的总和

84
00:06:19,783 --> 00:06:24,260
网格点是很好的模型 是所有网格点
网格概率的总和

85
00:06:24,260 --> 00:06:29,385
给定数据和先验，

86
00:06:29,385 --> 00:06:33,407
数倍于（times）我们将获得的测试输出概率

87
00:06:33,407 --> 00:06:38,403
给定输入 给定网格点  换句话说 我们必须

88
00:06:38,403 --> 00:06:43,787
把在产生测试答案之前时 我们可能会把噪
声添加到输出网络上的事实考虑在内

89
00:06:43,787 --> 00:06:47,226
这是一幅全贝叶斯学习的图片

90
00:06:47,226 --> 00:06:50,641
我们在这有很小的网络 有4个权重参数
和2个偏置参数

91
00:06:50,641 --> 00:06:54,815
如果我们允许，每一个权重和偏置都有
9种可能的取值

92
00:06:54,815 --> 00:06:59,325
对于这6个网格点（grid+points）将会有9种不同的取值

93
00:06:59,326 --> 00:07:03,155
在参数空间中 参数的总数是一个很大
但是能处理的数目

94
00:07:03,155 --> 00:07:08,162
对于网格点中每一个     我们可以计算
在所有训练案例中观察输出值的概率

95
00:07:08,162 --> 00:07:11,460
比如 我们把依赖于权重值的网格点

96
00:07:11,460 --> 00:07:15,702
乘以先验概率

97
00:07:15,702 --> 00:07:19,884
然后 我们将重新归一化来得到所有
网格点的后验概率分布

98
00:07:19,884 --> 00:07:22,829
因此 我们可以用这些网格点进行预测

99
00:07:22,829 --> 00:07:27,660
但是对每一个预测的权重需要通过
它的后验概率
翻译 Lilian Li