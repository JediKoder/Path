在这个视频中 我将探讨为什么我们想要使用多个模型的结合 进行预测 如果我有一个模型 我必须 选择模型的容量 如果模型的容量太小 该模型 将不能适应训练数据的一致性 如果模型的容量过大 该模型将不能适应训练数据的样本错误 通过结合多个模型 我们可以 在拟合数据的一致性和过度拟合样本的误差之间 取得一个更好的权衡 在这一讲的视频的开头 我将证明当你一起使用各个模型
你可以期待这些模型 将会比任何单一模型有更好性能 当这些模型 每个模型都做出了非常不同的预测 在这一讲视频的结束 我将会讨论 可以鼓励不同模型
做出非常不同的预测的多个方法 正如我们之前所见
当我们的训练数据有限 我们容易 过度拟合这些数据 如果我们平均各个模型的 不同预测 我们通常可以减少过度拟合 这非常有用 特别是当各个模型都做出了非常不同的预测 对于回归分析
squared arrow可以被分解为一个偏差项和 一个方差项 这可以帮助我们分析正在发生什么 如果模型只有很小的容量去适应数据 偏差项会比较大 它能测量这个模型 与真正的函数有多么不同 如果模型 的容量很大 在我们特定的训练集里
可以很好地处理样本误差 方差项会比较大 所以称之为方差 因为如果我们有另外一个训练集
有着相同的大小和分布 我们的模型 将会对这个训练集有非常不同的拟合 因为它有不同的样本误差 因此当用模型拟合不同的训练集时
我们将会得到方差 如果我们将多个模型放到一起平均
我们正在做的是平均掉方差 这将允许我们使用单独的 有着大容量 以及高方差的模型 这些大容量模型通常有较低的偏差 所以当我们有了更低的偏差
并且可以使用平均减少方差 避免出现高方差
现在我们尝试一下并分析 比较独立的模型和多个模型的平均吧 在任何测试下 一些独立模型可能会比 混合模型有更好的预测结果
不同的独立模型会 在不同的情况下有更好的结果
如果这些独立模型的结果非常不同 当我们平均所有测试例 混合模型通常会有更好的预测结果 所以我们要使独立模型有不同的结果
并且不会使他们 性能下降
方法是使每个独立模型 之间都有非常不同的误差
但是它们每个都是相当准确的 现在我们来看看数学
已经当我们组合网络时会发生什么 我们将会比较两种期望平方误差 第一个期望平方误差是
如果我们随机选择一个模型 用它做出预测 然后我们平均所有的模型
如果我们按照这个方式 得到的误差就是第一个期望平方误差
所以Y平均是 所有模型预测结果的平均
Yi是其中一个模型的结果 所以Y平均只是对Yi中所有独立模型i的总体期望 我将会用这些尖括号表示期望 所以尖括号就是表示
它是一个总期望 我们可以将它写成
N分之1乘上所有N个模型的和 现在如果我们观察期望平方误差
将会知道我们是否随机选择了 模型
我们需要做的是比较 模型和目标的
并对差进行平方 再对所有模型平均
它也在左手边 如果我简单得加上一个Y平均再减去一个Y平均
我不会改变它的值 现在我们可以更容易地
对式子做一些整理 我现在可以展开平方式
在期望括号里有t 减去y平均并平方 yi减去y平均
t减去y平均 乘上yi减去y平均 这样c会消失
所以第一项 T减去Y平均的差的平方 不再有i
所以我们可以将这一项从期望括号里面提出来 然后得到单独的T减去Y平均的差的平方 如果你比较模型的平均和目标
这就是平方误差 我们的目标是 证明左边部分更大
就是通过使用平均 我们可以减少 期望平方误差 所以右边的多余的项 是y i 减去y平均的差的平方的期望 也就是yi的平方差
它是yi和y平均之间的 期望平方差 然后消掉最后一项 我们可以消掉它是因为
我们期望的Y到Yi的差是与 网络的平均产生的误差和目标之间的差
是无关的 所以我们乘上两个部分的 平均是0 且无关的
我们期望平均是0 所以结果我们通过随机选择模型
得到的期望平方误差 它比我们通过平均模型的输出方差
得到期望平方误差大 这是我们通过一个平均提高的量 现在来看看这个图示 沿着平行的线
我们有输出的可能值 在这种情况下
所有不同的模型预测的值都太高 这些离平均t很远的预测
会产生更大的平均方差 就像那个红色的bad guy
那些比平均值离t更近的模型 会产生更小的平方差 第一种影响占主要位置
因为我们将会用方差 所以从数学角度来说
让我们假设good guy和bad guy 距离平均同样远
所以他们产生的平均方差 是Y平均减去epsilon的平方加上
Y的平均加上epsilon的平方 当我们算出这个式子
我们就得到的方差就是 模型预测结果平均的平方加上epsilon的平方
所以我们通过平均预测结果 在将他们与目标比较前得到我们想要的
这并不总是对的 这非常依赖于使用平方差 举例来说 如果你有很多钟表 你想通过平均它们显示时间的方式
将它们调得更精确 这无疑是一场灾难
因为你所期待的这些钟表的误差 并不是高斯分布
你期待的是 很多表只有很小的误差 其中很少一部分不走
或者有很大误差 如果你平均的话
你只会让他们的误差更加错得离谱 这同样可以应用到离散分布 因为我们有标记分级的概率 所以假设我们有两个模型
其中一个给了Pi概率一个正确的标签 另外一个给了Pj概率一个正确的标签 这时候最好随机选择
还是最好将两者的概率平均 并预测Pi和Pj的平均 如果我有一个log概率的可能性得到正确的结果
会怎么样? 那么Pi和Pj的平均的log
将会好过Pi的log加上Pj的log然后再平均 使用图像的话更好理解 因为它是一个log函数的形状
所以黑色的线是log函数 在水平轴上 我画了Pi和Pj 那条黄色线 连接log Pi到log Pj 你可以看到如果你首先从Pi和Pj一起开始 在蓝色箭头上得到的平均
然后我们计算log 得到蓝色点 虽然如果你首先计算Pi的log
并分开计算Pj的log 然后我们平均两个log
得到黄色线的中点 这个点比蓝色点低
所以我们那样做非常有优势 我们希望我们的模型非常不同
要达到这个目的有很多种方式 你可以直接依赖学习算法 但效果并不好
每次都会卡在不同的局部优化 所以说这样方式很不明智 但是它依然值得去尝试
你可以尝试很多不同类型的模型 包括不是神经网络的那些 这使得尝试决策树变得很有意义
高斯处理模型 支持向量机 我不会在这个课程中介绍它们 在Coursera上Andrew Ng的机器学习课程中 你可以学到那些知识
虽然你可以尝试很多不同的模型 如果你真的想用一堆不同的神经网络模型 你可以把它们变得不同
通过使用不同数量的 隐藏层
或在每个层使用不同数量的单元 或者不同类型的单元
类似于你在一些网络中可以使用 修正线性单元
在另外一些网络你使用逻辑单元 可以使用不同类型或强度的权重衰减 所以你可能对一些网络使用提前停止策略 以及对一些的一个L2权重惩罚
对另外一些的一个L1权重惩罚 你可以使用不同的学习算法 所以如果你的数据集足够小
对于一些例子来说你可以使用全批量full batch 对另外一些实用迷你全量 你也可以通过使用不同训练数据训练模型的方式
得到不同的模型 有一种方式最初由Leo Breiman介绍 称为套袋
你可以使用不同的数据子集得到不同的模型 并且你能通过对训练集取样替换
得到子集 所以我们对一个有样本A B C D的训练集取样 得到五个样本 但是我们会漏掉一些或者重复一些
我们训练一个模型在这个特定的训练集上 这些会在一个称为随机森林的方法下完成 它使用bagging结合了决策树
这也是由Leo Breiman参与发明的 当你使用bagging训练决策树 然后将它们平均
会好过任何单个决策树的 事实上 连接箱子使用了随机森林 来将关于深度的信息转换成
关于你的主体部分在哪的信息 我们可以使用bagging结合神经网络 但是这样代价很大
如果你想使用这种方式训练 比如说20个 不同的神经网络
你需要有20个不同的训练集 与训练单个网络相比
这将花费20倍的时间 这对于决策树是没有关系的 因为它们训练速度很快
并且在测试时 你需要运行 20个不同的网络
对于决策树来说 这依然不是问题 因为他们在测试时运行非常快 另外一个制作不同训练数据的方法是
使用整个数据集训练每个模型 但是给不同列子不同的权重 所以在boosting 我们通常使用一系列
更小容量的模型 然后我们给每个模型的训练例不同的权重 我们将要做的事就是在前一个模型出错的时候
提高权重 在前一个模型正确的时候
降低权重 所以序列中的下一个模型
不会浪费它自己的时间去尝试对 已经正确的情况做匹配
它使用它的资源去处理那些其他模型出错的例子 boosting的一个早期使用目的是结合神经网络
处理MNIST 在电脑非常慢的时候 其中一个大优势就是它 集中使用它的资源处理那些需要处理的例 这样不会浪费很多时间
一次又一次得处理简单的例子