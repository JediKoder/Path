1
00:00:00,000 --> 00:00:05,974
在这个视频中  我将描述一种组合大量
神经网络模型

2
00:00:05,974 --> 00:00:11,803
而无须分别训练大量模型的新方法

3
00:00:11,803 --> 00:00:15,155
这是一种叫做 “dropout”的方法

4
00:00:15,155 --> 00:00:19,090
最近在赢得比赛中获得了很大的成功

5
00:00:19,090 --> 00:00:23,136
对于每一个训练实例 我们随机省略了一些
隐结点

6
00:00:23,136 --> 00:00:27,245
因此 对于不同的训练案例  我们以
使用不同结构的神经网络而结束

7
00:00:27,245 --> 00:00:31,789
我们可以把它认为不同的训练案例有
不同的模型

8
00:00:31,789 --> 00:00:36,333
然后 问题是我们如何能在只有一种训练案例时
训练一种模型

9
00:00:36,333 --> 00:00:41,500
我们如何能在训练时把这些模型
有效地平均到一起（average together）

10
00:00:41,500 --> 00:00:44,426
答案就是我们需要用大量的
权重共享的策略

11
00:00:44,426 --> 00:00:47,477
我们想以结合

12
00:00:47,477 --> 00:00:51,150
多个模型的输出来描述这
两种不同的方法

13
00:00:51,150 --> 00:00:57,260
在混合模型中 我们通过平均输出概率来
结合模型

14
00:00:57,520 --> 00:01:03,299
所以 如果模型A分配的概率是0.3 0.2 0.5
有三种不同的答案 

15
00:01:03,299 --> 00:01:09,247
模型B分配的概率是 0.1 0.8 0.1 

16
00:01:09,247 --> 00:01:13,519
结合的模型将简单地分配
概率的平均值

17
00:01:13,519 --> 00:01:20,387
结合模型的一种不同的方法是用
概率的乘积 这里

18
00:01:20,387 --> 00:01:24,660
我们对相同的概率取集合平均值

19
00:01:25,040 --> 00:01:30,730
所以 像之前一样 模型A和模型B
将又分配相同的概率

20
00:01:30,730 --> 00:01:36,493
但是现在 我们是这样做的 把概率
中每一对乘在一起 然后

21
00:01:36,493 --> 00:01:40,505
取平方根 那是几何平均值

22
00:01:40,505 --> 00:01:44,517
几何平均值通常总和小于1

23
00:01:44,517 --> 00:01:49,697
所以 我们必须除以几何平均值的总和
来归一化概率的分布

24
00:01:49,697 --> 00:01:52,980
以使它的加和能再一次等于 1

25
00:01:54,440 --> 00:02:01,573
你将注意到在一个乘积中 一个模型的小的
概率输出

26
00:02:01,573 --> 00:02:07,085
对于其他的模型具有否决权 现在我想描述

27
00:02:07,085 --> 00:02:12,043
一种平均大量神经网络的有效的方法  该方法能给
我们一种做正确贝叶斯事情的可供替代的方法

28
00:02:12,043 --> 00:02:15,823
这种可供替代的方法可能没有

29
00:02:15,823 --> 00:02:20,100
像做正确的贝叶斯的事情那么好 但是
它却很实用

30
00:02:20,100 --> 00:02:23,750
因此 考虑到含有一个隐藏层的神经网络
如右图所示

31
00:02:23,750 --> 00:02:29,649
每一次我们展示一个训练的例子时

32
00:02:29,649 --> 00:02:32,412
我们将要做的事随机地以 0.5 的概率

33
00:02:32,412 --> 00:02:35,520
以 0.5 的概率省略（emit是否为omit）每一个
隐藏层中的单元 

34
00:02:35,520 --> 00:02:38,905
所以  这里我们删去隐藏层的三个单元

35
00:02:38,905 --> 00:02:43,740
我们通过缺乏那些隐藏单元的网络
来运行这个实例

36
00:02:43,740 --> 00:02:48,953
这意味着我们要从2^H种结构中随机进行采样

37
00:02:48,953 --> 00:02:53,775
这里H是隐藏单元的数目 这是
大量的结构体系

38
00:02:53,775 --> 00:02:57,033
当然 所有的这些结构表现的是权重

39
00:02:57,033 --> 00:03:02,116
那就是说无论何时我们用一个隐藏层单元 
它将在其它的结构中获得相同的权重

40
00:03:02,116 --> 00:03:07,634
所以 我们可以把 "dropout"认为是
一种模型平均值的形式

41
00:03:07,634 --> 00:03:11,993
我们从2^H种模型中进行采样

42
00:03:11,993 --> 00:03:15,740
事实上 这些模型中的绝大多数将从不
被采样

43
00:03:16,180 --> 00:03:20,140
一个模型的采样将仅仅得到一个训练的实例

44
00:03:20,560 --> 00:03:25,905
那是一种非常极端的包装形式 对于不同模型的
训练集都是不同的

45
00:03:25,905 --> 00:03:29,062
但它们也都很小

46
00:03:29,062 --> 00:03:34,214
所有这些模型之间的权重共享的策略
意味着每一个模型都被其他的模型

47
00:03:34,214 --> 00:03:39,238
强烈的正则约束 这是一种

48
00:03:39,238 --> 00:03:43,811
比L2范数或L1范数惩罚项更好的正则化矩阵
因子 使权重更加接近于0

49
00:03:43,811 --> 00:03:46,645
通过与其他的模型共享权重

50
00:03:46,645 --> 00:03:51,605
模型获得的正则约束将趋向于
把权重拉近正确的值

51
00:03:51,605 --> 00:03:56,281
问题仍然是测试时间我们做什么

52
00:03:56,281 --> 00:03:59,459
所以 我们能够采样很多结构 

53
00:03:59,459 --> 00:04:03,870
可能是一百 然后取输出分布的
几何平均值

54
00:04:03,870 --> 00:04:06,688
但那将是很大的工作量

55
00:04:06,688 --> 00:04:12,662
这是我们能做的很简单的事情 我们
用所有的隐藏层单元

56
00:04:12,662 --> 00:04:17,456
但是我们有它们的输出权重 所以在采样时
它们实际上有和预期相同的效果

57
00:04:17,456 --> 00:04:23,953
事实证明用全部隐藏神经元的
一半的输出权重

58
00:04:23,953 --> 00:04:29,694
能准确地计算几何平均值

59
00:04:29,694 --> 00:04:35,147
所有2^H个模型已经用到的预测值
假如我们用 softmax 的输出组

60
00:04:35,147 --> 00:04:40,706
如果我们不仅有一层隐藏层

61
00:04:40,706 --> 00:04:44,020
我们可以简单地在每一层用 以概率为0.5 的
"dropout"的策略

62
00:04:44,600 --> 00:04:49,500
在测试时 我们有全部隐含层的输出权重

63
00:04:49,760 --> 00:04:53,153
这给了我们我平时所说的
均值的网络

64
00:04:53,153 --> 00:04:58,600
所以 我们用一个有所有单元
但权重减半的网络

65
00:04:59,520 --> 00:05:05,157
当我们有多个隐含层时 这和计算每一个
“dropout”模型是不太一样的

66
00:05:05,157 --> 00:05:09,980
但是它是一种好的近似而且
它的速度也很快

67
00:05:10,840 --> 00:05:16,063
我们能用"dropout"的方法运行了很多随机
模型 然后对那些随机模型取平均

68
00:05:16,063 --> 00:05:20,609
这个均值网络将有一个优点

69
00:05:20,609 --> 00:05:23,808
在答案中它将给我们一个
不确定性的观点

70
00:05:23,808 --> 00:05:27,432
输入层怎么样?

71
00:05:27,432 --> 00:05:30,236
我们也能很好地在那里使用相同的技巧

72
00:05:30,236 --> 00:05:35,912
我们在输入中使用“dropout”的策略 但我们用较高
的概率保持输入的值

73
00:05:35,912 --> 00:05:41,519
这个技巧已经在叫做去噪的自动编码机中
所应用

74
00:05:41,519 --> 00:05:46,511
该方法是由蒙特利尔大学的 Pascal Vincent, Hugo Laracholle和Yoshua Bengio所研制的

75
00:05:46,511 --> 00:05:50,882
它取得的效果非常好 "dropout"方法是如何工作的?

76
00:05:50,882 --> 00:05:55,528
由Alex Krizhevsky研发的破纪录的目标识别的网络

77
00:05:55,528 --> 00:05:59,823
已经打破了记录 即使没有用到"dropout"的策略

78
00:05:59,823 --> 00:06:05,666
但是使用"dropout"的策略能打破的更多一些 
通常来讲 如果你有一个深度神经网络

79
00:06:05,666 --> 00:06:10,735
而且它是过度拟合的   "dropout"的策略 
将极大地减少错误的数目

80
00:06:10,735 --> 00:06:13,624
我认为任何网络都需要提前终止

81
00:06:13,624 --> 00:06:17,736
以防止过度拟合 但是使用"dropout"的方法
能做得更好

82
00:06:17,736 --> 00:06:22,376
当然 这需要更长的训练时间
也需要对更多的隐含单元取均值

83
00:06:22,376 --> 00:06:25,078
如果你得到一个深度神经网络并且
它不是过拟合的

84
00:06:25,078 --> 00:06:29,776
你应该使用更大的网络并且使用
"dropout"的策略

85
00:06:29,776 --> 00:06:32,420
那是假定你有足够的计算能力

86
00:06:32,420 --> 00:06:37,195
这是思考"dropout"策略的另一种方式 这是
我最初想到的方法

87
00:06:37,195 --> 00:06:41,576
你将看到它和混合专家的网络有点相关

88
00:06:41,576 --> 00:06:46,728
当所有的专家合作时将出现什么问题

89
00:06:46,728 --> 00:06:51,672
什么避免了专门化?  如果一个隐含神经元
知道其他哪几个隐含神经元起作用

90
00:06:51,672 --> 00:06:57,451
它将会在训练集上相互适应其它的隐含的单元

91
00:06:57,451 --> 00:07:00,723
这意味着什么  训练一个隐含的神经元的
真实的信号是

92
00:07:00,723 --> 00:07:06,294
试着去修复当所有其它的隐含
神经元遗留的错误的说

93
00:07:06,294 --> 00:07:11,638
那是反向传递来训练每一个隐含单元的权重

94
00:07:11,638 --> 00:07:16,412
现在 这将造成在隐含神经元之间的
复杂的相互自适应

95
00:07:16,412 --> 00:07:21,808
当数据发生变化时将出现什么问题

96
00:07:21,808 --> 00:07:25,130
因此 一个新的测试数据集

97
00:07:25,130 --> 00:07:30,249
如果你依赖复杂的相互自适应策略
在训练集上得到正确的事情

98
00:07:30,249 --> 00:07:34,120
在新的测试集上很可能效果不会非常好

99
00:07:34,120 --> 00:07:38,575
就像这个观点 一个大的复杂的密谋团体
总会包括一些确定会出错的人

100
00:07:38,575 --> 00:07:43,030
因为总有一些你没有想到的事情

101
00:07:43,030 --> 00:07:47,601
如果有大量的人被卷入其中 他们中总有人
会表现出意想不到的行为

102
00:07:47,601 --> 00:07:50,667
于是其他的人也会做错事

103
00:07:50,667 --> 00:07:53,791
如果你想密谋 最好有一些小的
密谋的团体

104
00:07:53,791 --> 00:07:58,189
然后 当意外的事情发生时

105
00:07:58,189 --> 00:08:02,412
这些小的密谋团体中的很多将失败 
但是它们中的一些仍会成功

106
00:08:02,412 --> 00:08:06,137
因此 使用"dropout"的策略 我们会迫使
一个隐含的神经元

107
00:08:06,137 --> 00:08:09,560
与许多其它集合中的隐含的单元组合地工作

108
00:08:09,560 --> 00:08:13,523
这使它更有可能做对对个案有用的事情

109
00:08:13,523 --> 00:08:17,436
而不仅仅是有用 由于其他的特别的隐含的单元
与它进行合作

110
00:08:17,436 --> 00:08:20,180
但这也通常趋于去做对个体有用的事情

111
00:08:20,180 --> 00:08:24,397
与其他隐含单元做的事情有一定的区别

112
00:08:24,397 --> 00:08:27,222
它需要一些边际效应的事情

113
00:08:27,222 --> 00:08:30,266
鉴于它的合作者通常会实现的事情

114
00:08:30,266 --> 00:08:35,238
我想这是有"dropout"的策略的网络有
很好的表现性能的原因