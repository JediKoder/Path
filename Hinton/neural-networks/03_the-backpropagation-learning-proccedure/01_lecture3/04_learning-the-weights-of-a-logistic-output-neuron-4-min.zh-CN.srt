1
00:00:00,000 --> 00:00:05,060
要将线性神经元的学习规则扩展到

2
00:00:05,060 --> 00:00:09,036
多层非线性神经网络，需要两个步骤。

3
00:00:09,036 --> 00:00:14,021
首先，我们需要将学习规则扩展到非线性神经元。

4
00:00:14,021 --> 00:00:19,040
虽然也可以使用其他非线性神经元，
这里我们使用逻辑斯蒂神经元为例。

5
00:00:19,040 --> 00:00:25,012
现在我们要学习规则进行通用化，

6
00:00:25,012 --> 00:00:31,141
从线性神经元扩展到非线性的逻辑斯蒂神经元。

7
00:00:31,141 --> 00:00:38,048
逻辑斯蒂神经元，

8
00:00:38,048 --> 00:00:45,038
使用bias 加上所有的输入xi乘以权重wi的加和，
得到其逻辑，z。

9
00:00:45,038 --> 00:00:50,088
然后逻辑函数给出y，其是一个光滑的非线性函数。

10
00:00:50,088 --> 00:00:57,019
如图所示，

11
00:00:57,019 --> 00:01:02,090
当是z很大的负数的时候，输出近似于0；

12
00:01:02,090 --> 00:01:07,091
z是很大的正数的时候，输出近似1。
两者之间的时候，其输出是非线性但是是光滑的。

13
00:01:07,091 --> 00:01:13,021
其变化的连续性使得求有着良好的导数，

14
00:01:13,021 --> 00:01:18,011
让学习容易。
要得到逻辑斯蒂神经元

15
00:01:18,011 --> 00:01:23,681
相当于权重的导数，
首先要得到逻辑，z

16
00:01:23,681 --> 00:01:29,379
自己的导数，这个很简单。

17
00:01:29,379 --> 00:01:35,051
其仅仅是bias 加上

18
00:01:35,051 --> 00:01:40,539
所有的输入xi乘以权重wi的加和。

19
00:01:40,539 --> 00:01:45,338
所以对wi求导，我们得到xi。

20
00:01:45,338 --> 00:01:50,387
所以，z对wi求导得到xi，同样的

21
00:01:50,387 --> 00:01:53,696
对xi求导得到wi。

22
00:01:53,696 --> 00:02:01,310
如果输出以z的形式来表达，

23
00:02:01,310 --> 00:02:10,872
其对z求导也很简单。
输出是一除以一加上e的负z次方。

24
00:02:10,872 --> 00:02:16,086
dy对dz等于y乘以1减去y。这里省略了推导过程。

25
00:02:16,086 --> 00:02:21,012
如果对推导感兴趣，
我在下一张幻灯片里面列出了详细过程。

26
00:02:21,012 --> 00:02:25,056
过程比较繁琐，但是非常直观，
你可以自己推导一遍。

27
00:02:25,056 --> 00:02:30,687
现在你得到相应的相应的导数，

28
00:02:30,687 --> 00:02:37,067
输出对z的导数，z对权重的导数

29
00:02:37,067 --> 00:02:44,026
我们就能得到输出相对于权重的导数了。

30
00:02:44,026 --> 00:02:47,055
我们可以使用链式法则。

31
00:02:47,055 --> 00:02:54,703
dy对dw的等于dz对dw乘以dy对dz。
dz对dw，我们刚刚提过，其实就是xi

32
00:02:54,703 --> 00:03:00,635
dy对dz，等于y乘以1减去y。
现在我们有了逻辑斯蒂神经元的学习规则。

33
00:03:00,635 --> 00:03:05,256
我们有了dy对dw，然后需要再使用一次链式法则，

34
00:03:05,256 --> 00:03:10,277
将其乘以de对dy。

35
00:03:10,277 --> 00:03:15,404
然后你得到一个很类似于德尔塔法则的式子。

36
00:03:15,404 --> 00:03:23,002
所以我们改变权重的方向，de对dwi，等于

37
00:03:23,002 --> 00:03:29,627
等于输入xi乘以残差，

38
00:03:29,627 --> 00:03:35,971
即目标值和逻辑斯蒂神经元输出值的差，

39
00:03:35,971 --> 00:03:41,201
但是这里还有格外的项，

40
00:03:41,201 --> 00:03:47,062
即逻辑斯蒂函数的斜率，等于yn乘以1减去yn。

41
00:03:47,062 --> 00:03:53,714
所以，训练一个逻辑斯蒂单元的梯度下降算法

42
00:03:53,714 --> 00:03:56,082
是一个轻微修改过的德尔塔规则。