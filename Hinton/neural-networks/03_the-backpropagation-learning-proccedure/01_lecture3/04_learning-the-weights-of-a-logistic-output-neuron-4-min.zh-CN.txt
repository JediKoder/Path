要将线性神经元的学习规则扩展到 多层非线性神经网络，需要两个步骤。 首先，我们需要将学习规则扩展到非线性神经元。 虽然也可以使用其他非线性神经元，
这里我们使用逻辑斯蒂神经元为例。 现在我们要学习规则进行通用化， 从线性神经元扩展到非线性的逻辑斯蒂神经元。 逻辑斯蒂神经元， 使用bias 加上所有的输入xi乘以权重wi的加和，
得到其逻辑，z。 然后逻辑函数给出y，其是一个光滑的非线性函数。 如图所示， 当是z很大的负数的时候，输出近似于0； z是很大的正数的时候，输出近似1。
两者之间的时候，其输出是非线性但是是光滑的。 其变化的连续性使得求有着良好的导数， 让学习容易。
要得到逻辑斯蒂神经元 相当于权重的导数，
首先要得到逻辑，z 自己的导数，这个很简单。 其仅仅是bias 加上 所有的输入xi乘以权重wi的加和。 所以对wi求导，我们得到xi。 所以，z对wi求导得到xi，同样的 对xi求导得到wi。 如果输出以z的形式来表达， 其对z求导也很简单。
输出是一除以一加上e的负z次方。 dy对dz等于y乘以1减去y。这里省略了推导过程。 如果对推导感兴趣，
我在下一张幻灯片里面列出了详细过程。 过程比较繁琐，但是非常直观，
你可以自己推导一遍。 现在你得到相应的相应的导数， 输出对z的导数，z对权重的导数 我们就能得到输出相对于权重的导数了。 我们可以使用链式法则。 dy对dw的等于dz对dw乘以dy对dz。
dz对dw，我们刚刚提过，其实就是xi dy对dz，等于y乘以1减去y。
现在我们有了逻辑斯蒂神经元的学习规则。 我们有了dy对dw，然后需要再使用一次链式法则， 将其乘以de对dy。 然后你得到一个很类似于德尔塔法则的式子。 所以我们改变权重的方向，de对dwi，等于 等于输入xi乘以残差， 即目标值和逻辑斯蒂神经元输出值的差， 但是这里还有格外的项， 即逻辑斯蒂函数的斜率，等于yn乘以1减去yn。 所以，训练一个逻辑斯蒂单元的梯度下降算法 是一个轻微修改过的德尔塔规则。