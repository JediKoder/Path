1
00:00:00,000 --> 00:00:04,041
这个视频将介绍线性神经元学习算法。

2
00:00:04,041 --> 00:00:09,050
其形式和感知器算法很类似，
但是达成的结果不一样。

3
00:00:09,050 --> 00:00:13,058
感知器中，参数发生变化，

4
00:00:13,058 --> 00:00:17,045
参数不停向好的参数集合逼近。

5
00:00:17,045 --> 00:00:22,082
在线性神经元算法中，
输出不停的向目标输出逼近。

6
00:00:25,053 --> 00:00:30,081
感知器算法收敛，
是因为我们确保了每次我们改变参数权重，

7
00:00:30,081 --> 00:00:33,095
我们都向好的参数集合逼近。

8
00:00:33,095 --> 00:00:38,042
这个保证不能扩展到复杂的网络。

9
00:00:38,042 --> 00:00:44,003
在复杂的网络中，
当你对两个好的参数的集合取平均时，

10
00:00:44,003 --> 00:00:48,034
你有可能得到一个坏的参数集合。
所以在多层神经网络中，

11
00:00:48,034 --> 00:00:51,049
我们不使用感知器学习算法。

12
00:00:51,049 --> 00:00:57,021
要证明复杂神经网络在提高其学习效果，
我们不能使用同样的证明方法。

13
00:00:57,021 --> 00:01:01,057
他们不能被称为多层感知器。

14
00:01:01,057 --> 00:01:05,073
这部分也有我的错在里面，抱歉了。

15
00:01:05,073 --> 00:01:10,065
对于多层神经网络，
我们需要使用其他方法来证明算法的确在工作。

16
00:01:10,065 --> 00:01:14,075
这里我们不再证明参数正在逼近正确的参数，

17
00:01:14,075 --> 00:01:19,073
相反，我们会证明实际的输出正在逼近真实的输出。

18
00:01:19,073 --> 00:01:24,077
这种证明方式，对于

19
00:01:24,077 --> 00:01:29,064
非凸问题也有效，
非凸问题中，两个好的解的平均值并不能给出一个好的解。

20
00:01:29,064 --> 00:01:33,092
感知器不是这样。

21
00:01:33,092 --> 00:01:39,057
在感知器学习中，
当参数向好的参数逼近时，

22
00:01:39,057 --> 00:01:44,068
输出作为一个整体有可能更加偏离实际的输出。

23
00:01:45,043 --> 00:01:50,085
让输出逼近实际输出的最简单的例子

24
00:01:50,085 --> 00:01:56,007
就是平方误差度量下线性神经元学习。

25
00:01:56,084 --> 00:02:01,074
线性神经元，在电气工程中被称为线性滤波，

26
00:02:01,074 --> 00:02:06,040
简单的讲输入带权重加和以后输出。

27
00:02:06,070 --> 00:02:13,034
神经元对于目标输出的估计，输出Y，

28
00:02:13,034 --> 00:02:18,071
等于所有输入i上，输入向量乘以权重向量后的加和。

29
00:02:18,071 --> 00:02:25,009
我们可以将其用加法公式或者使用矢量公式形式化下来。

30
00:02:26,075 --> 00:02:32,090
学习的目标是使所有的训练样本上错误的加和最小。

31
00:02:33,080 --> 00:02:39,044
我们需要一个错误的度量，为了简单起见，

32
00:02:39,044 --> 00:02:43,093
我们使用真实输出和实际输出误差的平方。

33
00:02:45,024 --> 00:02:48,092
有个问题是我们干嘛不直接求解，

34
00:02:48,092 --> 00:02:53,066
对于每一个训练样本，我们都可以写下一个等式，

35
00:02:53,066 --> 00:02:57,009
然后很直接的对其进行求解，得到最优的权重。

36
00:02:57,036 --> 00:03:02,018
这是标准的工程方法，为什么不这么做捏？

37
00:03:02,086 --> 00:03:07,091
第一个答案，或者说从科研而言，
我们试图理解真正的神经元如何工作，

38
00:03:07,091 --> 00:03:13,031
其应该不是去推导求解了一堆等式。

39
00:03:13,031 --> 00:03:16,089
从工程角度，

40
00:03:16,089 --> 00:03:21,047
我们希望得到一种能泛化到多层，非线性网络的方法。

41
00:03:21,081 --> 00:03:27,014
解析解局限于线性神经元和平方误差度量。

42
00:03:27,014 --> 00:03:32,041
下面要谈的迭代法，效率较低，

43
00:03:32,041 --> 00:03:35,030
但是容易泛化到复杂的系统。

44
00:03:36,050 --> 00:03:42,013
下面我要讲解一个简单的例子，
说明迭代法是如何求解线性神经元的权重参数的。

45
00:03:42,013 --> 00:03:47,063
假设每天中午你都在一个咖啡厅吃午餐。

46
00:03:47,063 --> 00:03:51,003
你的食谱是鱼，薯条和番茄酱。

47
00:03:51,003 --> 00:03:54,085
每天你都点不同份的鱼，薯条和番茄酱，

48
00:03:54,085 --> 00:03:58,041
每天都不同份。

49
00:03:58,041 --> 00:04:03,000
收银员仅仅告诉你这一餐的总价，
一些日子以后，

50
00:04:03,000 --> 00:04:07,071
你应该能够说出每一种食物的单价。

51
00:04:07,071 --> 00:04:13,030
迭代法中，你一开始随机的猜测食品的价格，

52
00:04:13,030 --> 00:04:19,007
然后你对猜测进行调整

53
00:04:19,007 --> 00:04:23,034
使得其更加符合收银员给出的价格，

54
00:04:23,034 --> 00:04:26,043
即这一餐的总价。

55
00:04:27,039 --> 00:04:32,095
所以每一餐，你都得到一个总价，
其给出了单个食品价格的一个线性约束。

56
00:04:32,095 --> 00:04:38,089
这一餐的总价，等于

57
00:04:38,089 --> 00:04:44,094
鱼的份数，x fish，乘以每份鱼的价格，w sifh，

58
00:04:44,094 --> 00:04:48,007
对于薯条和番茄酱，同样如此。

59
00:04:49,045 --> 00:04:53,009
食品的价格在这里类似于线性神经元的权重。

60
00:04:53,009 --> 00:04:57,025
我们可以把整个权重向量看作为一份鱼，

61
00:04:57,025 --> 00:05:01,019
一份薯条，一份番茄酱的价格。

62
00:05:02,068 --> 00:05:06,086
我们开始猜测这些价格，

63
00:05:06,086 --> 00:05:11,027
对其微调，使得其符合收银员给出的数字。

64
00:05:12,016 --> 00:05:18,007
假设收银员收钱时使用的真实权重为

65
00:05:18,007 --> 00:05:23,069
150一份鱼，50一份薯条，100一份番茄酱。

66
00:05:23,069 --> 00:05:28,038
图示的一餐，花费了850。

67
00:05:28,038 --> 00:05:32,069
这是我们的目标值，

68
00:05:33,051 --> 00:05:39,042
假设我们开始猜测，每种食物的价格都是50。

69
00:05:40,022 --> 00:05:45,025
对于两份鱼，5份薯条，3份番茄酱的一餐，

70
00:05:45,025 --> 00:05:48,053
我们初始认为其价格是500。

71
00:05:48,053 --> 00:05:53,009
这个使得我们有了350的残差，

72
00:05:53,009 --> 00:05:58,025
残差试着我们使用目前的权重猜测的价格和
收银员给出来的价格的差异。

73
00:05:58,025 --> 00:06:03,012
然后我们使用德尔塔定律来修正食品的价格。

74
00:06:03,012 --> 00:06:09,055
我们对权重进行变化，delta Wi

75
00:06:09,055 --> 00:06:15,562
等于学习率epsilon乘以第i个食品的份数，乘以残差，

76
00:06:15,562 --> 00:06:21,077
即目标输出和我们估计值的差值。

77
00:06:21,077 --> 00:06:27,013
这里我们将学习率设为1/35，

78
00:06:27,013 --> 00:06:34,094
在这里学习率乘以残差等于10，计算会简单，

79
00:06:34,094 --> 00:06:42,000
所以，鱼价格权重的变化应该为2乘以10，

80
00:06:42,000 --> 00:06:46,050
我们要把鱼的价格加20.

81
00:06:46,050 --> 00:06:50,092
薯条价格权重的变化是5乘以10。

82
00:06:50,092 --> 00:06:55,087
番茄酱价格权重的变化为3乘以10。

83
00:06:56,062 --> 00:06:59,062
这里我们得到70,100和80。

84
00:06:59,062 --> 00:07:03,003
请注意，薯条的价格权重变得错误了。

85
00:07:03,003 --> 00:07:08,044
这种学习算法不能保证每个单独的权重始终会变好。

86
00:07:08,044 --> 00:07:12,045
变好的是收银员给出的价格

87
00:07:12,045 --> 00:07:15,060
和我们估计的价格的差异。

88
00:07:17,013 --> 00:07:20,004
现在我们来推导德尔塔定律，

89
00:07:21,007 --> 00:07:26,072
我们从定义箭头所指的误差度量开始，
即训练集上的残差平方和。

90
00:07:26,072 --> 00:07:32,022
其是目标值和神经网络或者线性神经元预测值差的平方。

91
00:07:32,022 --> 00:07:37,043
其是目标值和神经网络或者线性神经元预测值差的平方。

92
00:07:37,043 --> 00:07:43,044
我们在前面放置了一个1/2，其可以消去求导时的2。

93
00:07:43,044 --> 00:07:50,036
然后我们开始用某个权重，Wi，对误导度量进行求导。

94
00:07:50,036 --> 00:07:56,040
求导时我们需要使用链式法则。

95
00:07:56,040 --> 00:07:59,086
链式法则说明了我们改变权重时错误度量是如何改变的，

96
00:07:59,086 --> 00:08:04,085
其等于我们变化改变权重时输出的变化，

97
00:08:04,085 --> 00:08:08,062
乘以我们改变输出是错误度量的变化。

98
00:08:08,062 --> 00:08:13,352
链式法则很容易记忆，
你可以消去这两个DY，

99
00:08:13,352 --> 00:08:16,093
不过有数学家在场可不要这么做哦。

100
00:08:17,031 --> 00:08:22,087
第一个DY对DW，用了花体的D，这是因为其是偏导。

101
00:08:22,087 --> 00:08:27,000
就是说，你可以改变很多的权重来改变输出，

102
00:08:27,000 --> 00:08:32,012
在这里，你仅仅改变权重 i。

103
00:08:32,012 --> 00:08:37,068
所以 DY 对DWi 求导，等于Xi。

104
00:08:37,068 --> 00:08:45,099
因为 Y等于 Wi乘以Xi。DE对DY求导，等于T减去Y，

105
00:08:45,099 --> 00:08:53,089
这是因为我们对T减去Y的平方求导，然后用1/2消去了2，
最后得到了T减去Y。

106
00:08:53,089 --> 00:09:01,045
所以我们得到了学习的规则，

107
00:09:01,045 --> 00:09:07,760
我们使用学习率 epsilon乘以错误度量E对权重wi的导数，

108
00:09:07,760 --> 00:09:12,001
来对权重进行调整。

109
00:09:12,001 --> 00:09:17,062
最前方加了负号，是因为我们想让错误变小。

110
00:09:17,062 --> 00:09:24,023
这个负号可以和我们上面公式中得到的负号对消。

111
00:09:24,023 --> 00:09:31,034
权重的变化等于所有训练样本上

112
00:09:31,034 --> 00:09:37,077
学习率乘以输入值，乘以目标值和实际输出值之差的总和。

113
00:09:39,077 --> 00:09:44,052
我们现在可以问，这个学习过程，德尔塔定律，是如何运行的咧？

114
00:09:44,052 --> 00:09:48,013
其最终能给出正确结果么？

115
00:09:49,062 --> 00:09:53,098
也许没有完美的答案。
我们可以给线性神经元一系列的训练样本

116
00:09:53,098 --> 00:09:56,064
以及期望的输出，

117
00:09:56,064 --> 00:10:00,029
但是并不存在能够产生期望输出的权重。

118
00:10:00,029 --> 00:10:05,001
但是仍然存在一些权重，能够最好的近似训练样本，

119
00:10:05,001 --> 00:10:07,061
最小化错误度量。

120
00:10:07,061 --> 00:10:11,080
如果学习率最够的小，

121
00:10:11,080 --> 00:10:16,052
学习时间最够长，我们是能逼近最好的解的。

122
00:10:16,052 --> 00:10:22,008
另一个问题是我们多块能得到最优解。

123
00:10:22,008 --> 00:10:27,021
即便是一个线性系统，

124
00:10:27,021 --> 00:10:31,068
在这种复杂的学习过程中，学习也可能很慢。

125
00:10:31,068 --> 00:10:36,091
如果两个输入维度高度相关，就很难

126
00:10:36,091 --> 00:10:41,582
把两者之和分别对应到每个输入维度上去。

127
00:10:41,582 --> 00:10:45,053
比如说，你总是点同样分数的番茄酱和薯条，

128
00:10:45,053 --> 00:10:50,354
你就无法说明总价中那部分来自番茄酱，

129
00:10:50,354 --> 00:10:53,678
那部分来自薯条。

130
00:10:53,678 --> 00:10:58,702
如果它们总是保持这个比例 要想学到正确的

131
00:10:58,702 --> 00:11:02,944
来正确区分番茄酱和薯条的价格。

132
00:11:02,944 --> 00:11:08,035
在德尔塔定律和感知器学习定律之间，存在有趣的联系。

133
00:11:08,035 --> 00:11:11,541
如果你使用德尔塔定律的在线版本，

134
00:11:11,541 --> 00:11:15,971
在每个训练样本上修改权重，

135
00:11:15,971 --> 00:11:20,501
就和感知器学习很相似了。

136
00:11:20,501 --> 00:11:24,716
感知器学习中，预测错误时，

137
00:11:24,716 --> 00:11:28,666
我们使用输入向量来增加或者减小权重向量。
德尔塔定律的在线版中，

138
00:11:28,666 --> 00:11:32,665
我们也根据输入向量来增加或者减小权重向量，

139
00:11:32,665 --> 00:11:36,729
不过还要乘以学习率和残差。

140
00:11:36,729 --> 00:11:41,259
这里令人恼怒的是我们要选择一个合适的学习率。

141
00:11:41,259 --> 00:11:46,248
如果选得太大，系统会不稳定。

142
00:11:46,248 --> 00:11:51,599
如果选择的太小，则会花费很长时间才能

143
00:11:51,599 --> 00:11:54,074
得到有意义的权重。