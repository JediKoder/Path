1
00:00:00,000 --> 00:00:04,095
这个视频，我们将会讨论线性神经元的误差曲面。

2
00:00:04,095 --> 00:00:10,016
理解了误差曲面的形状，
我们就会对

3
00:00:10,016 --> 00:00:13,007
线性神经元的学习过程中所发生的事情了解的更多。

4
00:00:13,007 --> 00:00:18,025
在训练线性神经元的权重时，
我们可以对所发生的事情有一个很好的几何学理解。

5
00:00:18,025 --> 00:00:23,080
我们可以假设一个

6
00:00:23,080 --> 00:00:29,027
类似于之前理解感知器时所用的空间

7
00:00:29,027 --> 00:00:32,080
但是多了一个维度。
现在假设一个空间，

8
00:00:32,080 --> 00:00:36,026
其所有水平的维度都是权重，

9
00:00:36,026 --> 00:00:40,095
此外还有一个垂直的维度，代表着误差。

10
00:00:40,095 --> 00:00:46,085
在这个空间里，
水平平面上的点代表不同的权重集合。

11
00:00:46,085 --> 00:00:50,050
高度代表相应权重上，

12
00:00:50,050 --> 00:00:55,064
所有训练数据错误的加和。

13
00:00:55,096 --> 00:01:03,073
对线性神经元而言，
每组权重的集合对于的错误组成了误差曲面。

14
00:01:03,073 --> 00:01:07,597
这个曲面是个二次的碗状。

15
00:01:07,597 --> 00:01:10,362
如果垂直切开，

16
00:01:10,362 --> 00:01:15,260
你总能得到一个抛物线。
如果水平切开，总是一个椭圆形。

17
00:01:15,260 --> 00:01:20,956
这仅仅适用于平方误差。

18
00:01:20,956 --> 00:01:25,162
当我们开始讨论多层，非线性神经网络时，

19
00:01:25,162 --> 00:01:29,202
误差曲面会变得更加复杂。

20
00:01:29,202 --> 00:01:35,067
当参数还不是很多的时候，
误差曲面还会是光滑的，

21
00:01:35,067 --> 00:01:42,091
但可能存在局部最优。
使用上述误差曲面，

22
00:01:42,091 --> 00:01:47,741
我们可以得到当我们使用
德尔塔定律来进行梯度下降学习时的场景。

23
00:01:47,741 --> 00:01:51,645
德尔塔定律，

24
00:01:51,645 --> 00:01:55,984
计算了误差相对于参数的导数。

25
00:01:55,984 --> 00:02:00,961
我们相当于在误差曲面进行最急速的下降。

26
00:02:00,961 --> 00:02:05,598
当我们使用得到的导数来改变权重时，

27
00:02:05,598 --> 00:02:11,264
换个方式理解，
当我们从上方来观察误差曲线时，

28
00:02:11,264 --> 00:02:15,956
我们会看到椭圆的轮廓线。

29
00:02:15,956 --> 00:02:21,219
德尔塔定律，如上图，
在椭圆轮廓线上指明了正确的方向。

30
00:02:21,219 --> 00:02:26,757
其发生在批量学习的场景，
在这个场景，梯度是在所有的训练样本上加和得到的。

31
00:02:26,757 --> 00:02:31,905
此外，我们也进行在线学习，

32
00:02:31,905 --> 00:02:38,582
对于每个训练样本，
我们都会依照这个样本上计算出来的梯度来修改权重。

33
00:02:38,582 --> 00:02:44,079
这和我们在感知器学习中所做的类似。

34
00:02:44,079 --> 00:02:48,493
你可以看到，
权重的调整使我们

35
00:02:48,493 --> 00:02:53,241
向一个其中一个控制面移动。

36
00:02:53,241 --> 00:02:57,617
在右图中，
有两个训练样本。

37
00:02:57,617 --> 00:03:04,021
要让第一个样本正确，
我们必须在其中一条蓝线上，

38
00:03:04,021 --> 00:03:09,883
要让第二个样本正确，我们必须在另一条蓝线上。

39
00:03:09,883 --> 00:03:14,094
因为，我们从其中一个红点开始，

40
00:03:14,094 --> 00:03:20,391
然后我们开始计算第一个样本的梯度，
德尔塔定律

41
00:03:20,391 --> 00:03:25,526
会让我们垂直的奔向那条线。
当我们考虑另一个样本的时候，

42
00:03:25,526 --> 00:03:28,927
我们则垂直的奔向另一条线。

43
00:03:28,927 --> 00:03:33,692
当我们在两个样本间不停交换时，

44
00:03:33,692 --> 00:03:38,368
我们呈锯齿状移动，直至最终解，
即最终两条线的交点。

45
00:03:38,368 --> 00:03:40,971
这一点上的权重参数可以同时满足两个样本。

46
00:03:40,971 --> 00:03:47,297
使用误差曲面，

47
00:03:47,297 --> 00:03:52,940
我们也能理解何种条件可以让训练很慢。

48
00:03:52,940 --> 00:03:59,315
如果训练样本对应的先几乎平行，
那么误差曲面的椭圆会很扁，

49
00:03:59,315 --> 00:04:05,923
当我们观察梯度时，

50
00:04:05,923 --> 00:04:09,612
其有个不好的属性。

51
00:04:09,612 --> 00:04:14,362
请看图中红色的箭头，
梯度在我们不想移动太多的方向很大，

52
00:04:14,362 --> 00:04:20,253
但在我们需要移动很长的方向则很小。

53
00:04:20,253 --> 00:04:26,227
所以梯度很快带领我们越过谷底，

54
00:04:26,227 --> 00:04:31,584
即椭圆的短轴。

55
00:04:31,584 --> 00:04:35,166
我们要很长的时间，

56
00:04:35,166 --> 00:04:38,880
才能沿着峡谷即椭圆的长轴前进。

57
00:04:38,880 --> 00:04:43,643
这不是我们想要的。
我们想得到一个梯度，在越过峡谷方向前进一点点，

58
00:04:43,643 --> 00:04:48,088
沿着峡谷方向一大步，但是我们得到不是这个。

59
00:04:48,088 --> 00:04:54,022
所以，简单的最快梯度下降，
每次使用学习率乘以误差的导数来调整参数权重，

60
00:04:54,022 --> 00:04:59,056
在图示中这种很扁的

61
00:04:59,056 --> 00:05:03,001
误差曲面的情况下会遇到极大的困难。