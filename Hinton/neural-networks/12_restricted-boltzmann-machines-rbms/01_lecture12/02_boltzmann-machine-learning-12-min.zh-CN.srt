1
00:00:00,000 --> 00:00:05,048
在前面视频中 我讲解了玻尔兹曼机

2
00:00:05,048 --> 00:00:08,910
是如何对二值数据向量的集合

3
00:00:08,910 --> 00:00:14,213
在本次课中 我们将走进玻尔兹曼机算法

4
00:00:14,213 --> 00:00:17,646
这是一个有着优雅理论证明的简单学习算法

5
00:00:17,646 --> 00:00:22,285
但是在实际应用中

6
00:00:22,285 --> 00:00:25,319
它却速度慢 噪音大 并不那么实用

7
00:00:25,319 --> 00:00:29,482
在多年以来 人们认为 波尔曼兹机并不能被实际应用

8
00:00:29,482 --> 00:00:32,931
后来我们发现

9
00:00:32,931 --> 00:00:35,489
能够提高学习算法的速度

10
00:00:35,489 --> 00:00:39,890
现在波尔曼兹机算法很实用 并且实际上

11
00:00:39,890 --> 00:00:44,767
也被用作为一个百万美元的机器学习竞争获胜的一部分

12
00:00:44,767 --> 00:00:49,168
博尔顿机学习算法是一个无监督学习算法

13
00:00:49,168 --> 00:00:54,023
并不像典型的用户反向传播

14
00:00:54,023 --> 00:00:58,892
有着输入矩阵 我们提供给它理想的输出

15
00:00:58,892 --> 00:01:03,086
在波尔兹莫机器学习中 我们只是给它一个输入矩阵

16
00:01:03,086 --> 00:01:08,767
有q个标签 而算法要做的则是通过输入矩阵集建立一个模型

17
00:01:08,767 --> 00:01:13,840
也可以把他们当成输出矩阵

18
00:01:15,980 --> 00:01:21,127
我们想要做的就是最大化概率的乘积

19
00:01:21,127 --> 00:01:23,762
也就是波尔兹莫机派出的 训练集的二项矩阵集

20
00:01:23,762 --> 00:01:29,520
这等同于最大化对数概率的玻尔兹曼机

21
00:01:29,520 --> 00:01:34,320
分配给训练矢量的总和

22
00:01:34,320 --> 00:01:38,102
如果我们通过以下的方法运行波尔兹莫机

23
00:01:38,102 --> 00:01:42,697
这也等同于最大化 我们获得精确训练结果的概率

24
00:01:42,697 --> 00:01:49,301
首先我们使它多次平稳分布

25
00:01:49,301 --> 00:01:53,620
没有外界输入

26
00:01:54,900 --> 00:02:01,621
我们从【可见】向量取样 并且我们再次放入数据

27
00:02:01,621 --> 00:02:03,740
并且再次从中取样 诸如此多次

28
00:02:07,140 --> 00:02:11,221
现在来讲一下 为什么这种学习会比较难

29
00:02:11,221 --> 00:02:14,300
这可能是最重要的原因

30
00:02:14,600 --> 00:02:20,773
想象一下有一组元素 这组单元除两端可见外 中间各项不可见

31
00:02:20,773 --> 00:02:26,865
如果我们用包括0 1 1 0 的训练集

32
00:02:26,865 --> 00:02:30,446
也就是说 我们希望两个可见元素处于相反的状态

33
00:02:30,446 --> 00:02:34,839
那么为达到这一目标 

34
00:02:34,839 --> 00:02:38,148
我们需要保证这组元素的乘积为负数

35
00:02:38,148 --> 00:02:43,082
所以 比如说 如果总乘积为正数  W1将会倾向于转向第一个不可见元素

36
00:02:43,082 --> 00:02:47,054
它又将转向第二个不可见元素 等等

37
00:02:47,054 --> 00:02:50,784
第四个不可见元素将会转向另一个可见元素

38
00:02:50,784 --> 00:02:55,658
如果这其中有权重为负数

39
00:02:55,658 --> 00:03:00,060
那么我们将会得到两个可见元素间的反校正

40
00:03:01,000 --> 00:03:07,457
这意味着 如果我们考虑学习W1的权重

41
00:03:07,457 --> 00:03:09,635
我们需要知道其他的权重 然后得出W1的

42
00:03:09,635 --> 00:03:13,758
想知道如何改变这个权重 我们需要知道W3

43
00:03:13,758 --> 00:03:19,905
我们需要知道W3的信息 因为如果W3是负值

44
00:03:19,905 --> 00:03:25,740
此时我们要对W1做的 与 W3为正值时 我们要对W1做的相反

45
00:03:28,560 --> 00:03:33,149
所以鉴于此权重 需要了解其他的权重

46
00:03:33,149 --> 00:03:37,794
以便能够即使在正确的方向来改变

47
00:03:37,794 --> 00:03:42,440
令人惊讶的是 这是一个很简单的学习算法 该算法只需要本地信息

48
00:03:44,200 --> 00:03:49,642
所以事实证明 一个权重需要知道的

49
00:03:49,642 --> 00:03:54,819
所有其他权重和有关数据的一切 都包含在两个相关的差之中

50
00:03:54,819 --> 00:03:59,864
另一种描述方法是

51
00:03:59,864 --> 00:04:03,050
 你取波尔兹曼机分配可视向量V的对数概率

52
00:04:03,050 --> 00:04:10,199
并去了解赋予一定权重的对数概率导数 Wij

53
00:04:10,199 --> 00:04:17,436
它是i j状态乘积的期望值的差值

54
00:04:17,436 --> 00:04:24,236
当网络通过在可见单元加以v下标 以解决热平衡方程

55
00:04:24,236 --> 00:04:28,614
这就是当可见单元有v下标时 i和j同时出现的频次

56
00:04:28,614 --> 00:04:35,606
并且整个网络处于热平衡方程 减去当v不处于可见单元的下标位置时的量

57
00:04:35,606 --> 00:04:39,820
...

58
00:04:39,820 --> 00:04:45,301
因为可见单元的对数概率的导数是一个简单的相关性间的差值

59
00:04:45,301 --> 00:04:50,853
我们可以根据 平均所有训练集中的可见单元之中的

60
00:04:50,853 --> 00:04:56,475
积的期望值 成一定的比例

61
00:04:56,475 --> 00:05:00,200
也就是我们所说的数据

62
00:05:00,460 --> 00:05:05,616
减去 不加下标的两项的乘积

63
00:05:05,616 --> 00:05:10,840
在没有外界干预的情况下 网络达到热平衡

64
00:05:11,400 --> 00:05:14,097
这是一个非常有趣的学习法则

65
00:05:14,097 --> 00:05:19,001
学习法则的第一项表示 

66
00:05:19,001 --> 00:05:23,231
在你呈现数据时 按比例提高各项乘积的权重

67
00:05:23,231 --> 00:05:27,400
这是赫布学习法则的最简单的表述方式

68
00:05:27,880 --> 00:05:32,842
唐纳德·赫布 在很久以前 20世纪40或50年代( 1940~ 1950) 曾指出

69
00:05:32,842 --> 00:05:37,437
大脑中的突触的工作原理的规则正是如此

70
00:05:37,437 --> 00:05:42,277
但如若只考虑这一规则 突触会变得越来越强大

71
00:05:42,277 --> 00:05:46,688
所有权重都会被增强 整个系统会变得膨胀

72
00:05:46,688 --> 00:05:51,712
你必须让所有事情处于掌控之中 并且这种算法就是通过使用第二项条例使事情处于控制之中

73
00:05:51,712 --> 00:05:55,082
当通过模型的分布采样时

74
00:05:55,082 --> 00:05:59,983
它以两项单元同时出现的频次为根据 按照一定的比例降低了他们的权重

75
00:05:59,983 --> 00:06:04,719
你同样可以把它当成如储存条例对于霍普费尔德网一样

76
00:06:04,719 --> 00:06:07,854
把它当成第一条例

77
00:06:07,854 --> 00:06:12,461
第二条例是为了摆脱虚假极小值

78
00:06:12,461 --> 00:06:16,044
事实上这种理解方式是正确的

79
00:06:16,044 --> 00:06:19,500
这个规则是为了告诉你确切还有多少的未学习

80
00:06:21,660 --> 00:06:25,260
一个明显的问题是为什么它的导数如此简单

81
00:06:26,400 --> 00:06:31,471
在热平衡方程下的一个配比的概率

82
00:06:31,471 --> 00:06:36,543
也就是你放入一组数的概率 是它能量值的指数方程

83
00:06:36,543 --> 00:06:40,060
这个概率和e 能量的负数相关

84
00:06:41,180 --> 00:06:46,494
因此在方程中 我们寻求的是对数概率和能量方程的线性关系

85
00:06:46,494 --> 00:06:52,775
现在能量方程是跟【权重】成线性关系

86
00:06:52,775 --> 00:06:56,093
也就是说权重和对数概率成线性关系

87
00:06:56,093 --> 00:07:01,306
那么我们通过控制权重来控制对数概率

88
00:07:01,306 --> 00:07:05,775
是一个正确的想法

89
00:07:05,775 --> 00:07:12,097
它是一个对数线性模型 事实上关系就是这么简单

90
00:07:12,097 --> 00:07:17,643
也就是关于特定权重Wij的能量的导数

91
00:07:17,643 --> 00:07:22,040
是其相关两项权重的乘积

92
00:07:23,720 --> 00:07:27,998
其实也就是 热平衡方程的放入数据过程

93
00:07:27,998 --> 00:07:31,597
也就是积累权重信息的过程

94
00:07:31,597 --> 00:07:34,925
我们不需要一个明确的【反向传播】阶段

95
00:07:34,925 --> 00:07:38,593
我们需要两个阶段 需要放入数据

96
00:07:38,593 --> 00:07:43,595
并且我们需要放入【非数据】

97
00:07:43,595 --> 00:07:46,477
但需要注意的是【网络】的行为与这两个阶段相似

98
00:07:46,477 --> 00:07:51,408
网络中的【单元】也在做同样的事情 只是边界条件不同而已

99
00:07:51,408 --> 00:07:55,250
在【反向传播】的情况下 【正向通过】和【反向通过】是截然不同的

100
00:07:55,250 --> 00:08:02,666
另一个你们可能问的问题会是【negative阶段】的作用

101
00:08:02,666 --> 00:08:06,794
正如我前面提到的这是为了像是【霍普菲尔德网络】中

102
00:08:06,794 --> 00:08:10,121
为摆脱【虚假极小值】而进行的【非学习】过程

103
00:08:10,121 --> 00:08:15,792
现在我们看一下具体情况 

104
00:08:15,792 --> 00:08:21,638
一个【可见】向量 的概率公式 也就是它的【E的总和总体隐藏向量】

105
00:08:21,638 --> 00:08:25,338
减去【可见】和【隐藏向量】的能量和 

106
00:08:25,338 --> 00:08:30,000
【由总的【可见向量】的和 对其进行归一化处理】

107
00:08:30,480 --> 00:08:36,758
因此对于分子部分 

108
00:08:36,758 --> 00:08:42,608
是用来减少求和式中的本就足够大的能量项

109
00:08:42,608 --> 00:08:48,315
并且它通过给带有v下标的向量放入数据 来寻求合适的分子分母部分

110
00:08:48,315 --> 00:08:53,880
这样以便能够找到匹配v的h项 也就是能够给予v较小的能量值

111
00:08:53,880 --> 00:08:57,663
【在对H向量取样后】

112
00:08:57,663 --> 00:09:01,460
此项将进一步减小【能量值】的权重

113
00:09:03,000 --> 00:09:08,413
分母部分是一个学习过程 与【negative阶段】做法相同

114
00:09:08,413 --> 00:09:13,185
但它是针对【分区功能】也就是对【底线】进行归一化

115
00:09:13,185 --> 00:09:16,802
它会寻找由【可见项】与【隐藏项】组合的

116
00:09:16,802 --> 00:09:21,198
低能量值的整体组合

117
00:09:21,198 --> 00:09:25,593
也正是对于【分区功能】有着较大贡献的组合

118
00:09:25,593 --> 00:09:30,338
找到这些整体的组合后

119
00:09:30,338 --> 00:09:36,591
它会试着提高它们的能量值 以降低它们的贡献值

120
00:09:36,591 --> 00:09:39,940
也就是分子式为了保证上限大 分母是为了保证下线小

121
00:09:43,300 --> 00:09:47,292
为了运行该学习法则 你需要对统计进行收集

122
00:09:47,292 --> 00:09:51,506
你需要收集【正统计】 也就是你有【可见项】下标的数据

123
00:09:51,506 --> 00:09:55,997
和【负统计】 也就是你没有下标的数据

124
00:09:55,997 --> 00:10:00,488
和你将要把它们用作【非学习过程】的数据

125
00:10:00,488 --> 00:10:05,194
一个有效的获得这些统计的方法

126
00:10:05,194 --> 00:10:09,740
由我和【Terry sejnowski】在1983年提出

127
00:10:10,140 --> 00:10:15,987
这个观点是 在【正统计】阶段 你给【可见】项一个下标

128
00:10:15,987 --> 00:10:19,440
同时设定【隐藏项】为【随机二项状态】

129
00:10:20,100 --> 00:10:24,632
然后你持续更新网络中的【隐藏单元】 每次更新一个单元

130
00:10:24,632 --> 00:10:28,700
【直到网络在某一温度下达到热平衡】

131
00:10:29,400 --> 00:10:34,227
我们事实上是从高温开始 然后逐渐【降温】

132
00:10:34,227 --> 00:10:39,592
但这个并不重要

133
00:10:39,592 --> 00:10:43,633
一旦你达到了热平衡 你开始两个单元同时出现进行取样

134
00:10:43,633 --> 00:10:48,840
此时则是对【可见】项有【下标】的情况下 进行i和j的相关性的估量

135
00:10:49,120 --> 00:10:54,175
然后对所有【可见】项进行重复

136
00:10:54,175 --> 00:11:00,876
所以你估量的相关性的取样是平均了所有数据的

137
00:11:00,876 --> 00:11:04,314
然后是对于【负阶段】 你不需要加任何下标 整个网络是免于【外界干预的】

138
00:11:04,314 --> 00:11:08,103
所以你设定所有的单元为【随机二项状态】 包括【可见】的和【隐藏】的

139
00:11:08,103 --> 00:11:13,624
然后你更新这些单元 一次一个

140
00:11:13,624 --> 00:11:18,017
知道网络达到某一温度下的热平衡

141
00:11:18,017 --> 00:11:24,142
正如你在【正阶段】所做的 

142
00:11:24,142 --> 00:11:29,762
你需要取样每对i,j的相关性 并且重复多次

143
00:11:29,762 --> 00:11:35,071
很难确定你需要重复它多少次

144
00:11:35,071 --> 00:11:39,996
但在【负阶段】 我们期待【energy landscape】有多个不同的极小值

145
00:11:39,996 --> 00:11:43,770
但他们有相同能量是合理的

146
00:11:43,770 --> 00:11:48,153
我们有这种期待 是因为我们将要用到波尔兹莫机来做一系列的事情

147
00:11:48,153 --> 00:11:52,199
比如对一组图像进行建模 

148
00:11:52,199 --> 00:11:54,841
我们期待一些诸如有着相同能量值的 合理的图片

149
00:11:54,841 --> 00:11:58,437
还有一些有着较高能量值的 不合理的图片

150
00:11:58,437 --> 00:12:02,821
因此你会期待这其中的一小部分是低能量态

151
00:12:02,821 --> 00:12:06,980
并且较大一部分是较高能量态

152
00:12:07,360 --> 00:12:12,787
如果你有多个【modes】 将很难确定你需要重复这个过程多少次

153
00:12:12,787 --> 00:12:15,398
才能从这些【modes】中取样