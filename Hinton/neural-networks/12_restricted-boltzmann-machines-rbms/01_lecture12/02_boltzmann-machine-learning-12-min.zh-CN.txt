在前面视频中 我讲解了玻尔兹曼机 是如何对二值数据向量的集合 在本次课中 我们将走进玻尔兹曼机算法 这是一个有着优雅理论证明的简单学习算法 但是在实际应用中 它却速度慢 噪音大 并不那么实用 在多年以来 人们认为 波尔曼兹机并不能被实际应用 后来我们发现 能够提高学习算法的速度 现在波尔曼兹机算法很实用 并且实际上 也被用作为一个百万美元的机器学习竞争获胜的一部分 博尔顿机学习算法是一个无监督学习算法 并不像典型的用户反向传播 有着输入矩阵 我们提供给它理想的输出 在波尔兹莫机器学习中 我们只是给它一个输入矩阵 有q个标签 而算法要做的则是通过输入矩阵集建立一个模型 也可以把他们当成输出矩阵 我们想要做的就是最大化概率的乘积 也就是波尔兹莫机派出的 训练集的二项矩阵集 这等同于最大化对数概率的玻尔兹曼机 分配给训练矢量的总和 如果我们通过以下的方法运行波尔兹莫机 这也等同于最大化 我们获得精确训练结果的概率 首先我们使它多次平稳分布 没有外界输入 我们从【可见】向量取样 并且我们再次放入数据 并且再次从中取样 诸如此多次 现在来讲一下 为什么这种学习会比较难 这可能是最重要的原因 想象一下有一组元素 这组单元除两端可见外 中间各项不可见 如果我们用包括0 1 1 0 的训练集 也就是说 我们希望两个可见元素处于相反的状态 那么为达到这一目标 我们需要保证这组元素的乘积为负数 所以 比如说 如果总乘积为正数  W1将会倾向于转向第一个不可见元素 它又将转向第二个不可见元素 等等 第四个不可见元素将会转向另一个可见元素 如果这其中有权重为负数 那么我们将会得到两个可见元素间的反校正 这意味着 如果我们考虑学习W1的权重 我们需要知道其他的权重 然后得出W1的 想知道如何改变这个权重 我们需要知道W3 我们需要知道W3的信息 因为如果W3是负值 此时我们要对W1做的 与 W3为正值时 我们要对W1做的相反 所以鉴于此权重 需要了解其他的权重 以便能够即使在正确的方向来改变 令人惊讶的是 这是一个很简单的学习算法 该算法只需要本地信息 所以事实证明 一个权重需要知道的 所有其他权重和有关数据的一切 都包含在两个相关的差之中 另一种描述方法是 你取波尔兹曼机分配可视向量V的对数概率 并去了解赋予一定权重的对数概率导数 Wij 它是i j状态乘积的期望值的差值 当网络通过在可见单元加以v下标 以解决热平衡方程 这就是当可见单元有v下标时 i和j同时出现的频次 并且整个网络处于热平衡方程 减去当v不处于可见单元的下标位置时的量 ... 因为可见单元的对数概率的导数是一个简单的相关性间的差值 我们可以根据 平均所有训练集中的可见单元之中的 积的期望值 成一定的比例 也就是我们所说的数据 减去 不加下标的两项的乘积 在没有外界干预的情况下 网络达到热平衡 这是一个非常有趣的学习法则 学习法则的第一项表示 在你呈现数据时 按比例提高各项乘积的权重 这是赫布学习法则的最简单的表述方式 唐纳德·赫布 在很久以前 20世纪40或50年代( 1940~ 1950) 曾指出 大脑中的突触的工作原理的规则正是如此 但如若只考虑这一规则 突触会变得越来越强大 所有权重都会被增强 整个系统会变得膨胀 你必须让所有事情处于掌控之中 并且这种算法就是通过使用第二项条例使事情处于控制之中 当通过模型的分布采样时 它以两项单元同时出现的频次为根据 按照一定的比例降低了他们的权重 你同样可以把它当成如储存条例对于霍普费尔德网一样 把它当成第一条例 第二条例是为了摆脱虚假极小值 事实上这种理解方式是正确的 这个规则是为了告诉你确切还有多少的未学习 一个明显的问题是为什么它的导数如此简单 在热平衡方程下的一个配比的概率 也就是你放入一组数的概率 是它能量值的指数方程 这个概率和e 能量的负数相关 因此在方程中 我们寻求的是对数概率和能量方程的线性关系 现在能量方程是跟【权重】成线性关系 也就是说权重和对数概率成线性关系 那么我们通过控制权重来控制对数概率 是一个正确的想法 它是一个对数线性模型 事实上关系就是这么简单 也就是关于特定权重Wij的能量的导数 是其相关两项权重的乘积 其实也就是 热平衡方程的放入数据过程 也就是积累权重信息的过程 我们不需要一个明确的【反向传播】阶段 我们需要两个阶段 需要放入数据 并且我们需要放入【非数据】 但需要注意的是【网络】的行为与这两个阶段相似 网络中的【单元】也在做同样的事情 只是边界条件不同而已 在【反向传播】的情况下 【正向通过】和【反向通过】是截然不同的 另一个你们可能问的问题会是【negative阶段】的作用 正如我前面提到的这是为了像是【霍普菲尔德网络】中 为摆脱【虚假极小值】而进行的【非学习】过程 现在我们看一下具体情况 一个【可见】向量 的概率公式 也就是它的【E的总和总体隐藏向量】 减去【可见】和【隐藏向量】的能量和 【由总的【可见向量】的和 对其进行归一化处理】 因此对于分子部分 是用来减少求和式中的本就足够大的能量项 并且它通过给带有v下标的向量放入数据 来寻求合适的分子分母部分 这样以便能够找到匹配v的h项 也就是能够给予v较小的能量值 【在对H向量取样后】 此项将进一步减小【能量值】的权重 分母部分是一个学习过程 与【negative阶段】做法相同 但它是针对【分区功能】也就是对【底线】进行归一化 它会寻找由【可见项】与【隐藏项】组合的 低能量值的整体组合 也正是对于【分区功能】有着较大贡献的组合 找到这些整体的组合后 它会试着提高它们的能量值 以降低它们的贡献值 也就是分子式为了保证上限大 分母是为了保证下线小 为了运行该学习法则 你需要对统计进行收集 你需要收集【正统计】 也就是你有【可见项】下标的数据 和【负统计】 也就是你没有下标的数据 和你将要把它们用作【非学习过程】的数据 一个有效的获得这些统计的方法 由我和【Terry sejnowski】在1983年提出 这个观点是 在【正统计】阶段 你给【可见】项一个下标 同时设定【隐藏项】为【随机二项状态】 然后你持续更新网络中的【隐藏单元】 每次更新一个单元 【直到网络在某一温度下达到热平衡】 我们事实上是从高温开始 然后逐渐【降温】 但这个并不重要 一旦你达到了热平衡 你开始两个单元同时出现进行取样 此时则是对【可见】项有【下标】的情况下 进行i和j的相关性的估量 然后对所有【可见】项进行重复 所以你估量的相关性的取样是平均了所有数据的 然后是对于【负阶段】 你不需要加任何下标 整个网络是免于【外界干预的】 所以你设定所有的单元为【随机二项状态】 包括【可见】的和【隐藏】的 然后你更新这些单元 一次一个 知道网络达到某一温度下的热平衡 正如你在【正阶段】所做的 你需要取样每对i,j的相关性 并且重复多次 很难确定你需要重复它多少次 但在【负阶段】 我们期待【energy landscape】有多个不同的极小值 但他们有相同能量是合理的 我们有这种期待 是因为我们将要用到波尔兹莫机来做一系列的事情 比如对一组图像进行建模 我们期待一些诸如有着相同能量值的 合理的图片 还有一些有着较高能量值的 不合理的图片 因此你会期待这其中的一小部分是低能量态 并且较大一部分是较高能量态 如果你有多个【modes】 将很难确定你需要重复这个过程多少次 才能从这些【modes】中取样