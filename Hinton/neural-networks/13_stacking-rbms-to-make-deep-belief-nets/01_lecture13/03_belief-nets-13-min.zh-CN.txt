本讲中 我将介绍置信网络 我在20世纪90年代一度放弃了反向传播
原因之一就是 它需要太多标签了
那时 没有一个数据集 包含了足够多的标签
同时 人类在学习的时候并没有 用很多显式的标签 这也影响了我 然而 我并不想放弃梯度下降法 因为其优势在于可以同时学习很多权重 这样 问题来了
是否能找到另外的目标函数来做 梯度下降呢?
很显然 我们应该考虑生成模型 就是说 目标函数要拟合
输入数据的概率分布 而不是预测输出的标签
这与统计学和人工智能 的主流— (概率) 图模型正好相符 图模型是要把离散的图结构组合起来 来表示变量之间的依赖关系 在给定其他变量的值的情况下 可以用实值运算来推算某个变量的值 玻尔兹曼机 (Boltzmann Machine) 
事实上是早期图模型的一个例子 但它是无向图模型
1992年, 拉德福德·尼尔 (Radford Neal) 指出 我们可以用与玻尔兹曼机相同的单元 来搭建有向图模型
他称这个模型为Sigmoid置信网络 现在 问题就变成了
我们该如何用Sigmoid置信网络进行学习? 另一个问题则是 深度网络的学习时间的可扩展性并不好 当网络有很多隐含层的时候 训练十分缓慢
你可能想问 这是为什么 现在我们知道原因之一是
权值没有很合理地初始化 还有一个问题 就是反向传播会被困在局部最优值 大部分时候这些极小点是很好的
因此反向传播是有用的 但现在我们可以说对于深度网络
如果以较小的随机权重来开始训练 那通常最后收敛到的局部极小值
与全局极小值相去甚远 当然 退回到简单一些的
允许凸优化的模型也是可能的 但我不认为这是个好主意 数学家喜欢做这样的事
因为他们可以证明一些东西 但在实际应用中 这种简单化是在
与实际数据的复杂度背道而驰 无监督学习是一条解决反向传播
的受限之处的途径 想法就是 我们想保持 用梯度方法和随机分批梯度下降
来调整权值的有效性和简单性 但是我们将运用这一方法来 拟合感官的输入 而不是拟合 输入与输出的关系
因此 我们想要调整权值而最大化的 是生成模型能够正确生成感官输入的概率 在玻尔兹曼机中 我们已经看到过这一点 打个比方 如果你想 做计算机视觉 那你应该
先学习计算机图形学 首先 图形学是好用的 而计算机视觉目前不是 生成模型的学习目标 就像玻尔兹曼机 是最大化观测到的数据的概率
而不是最大化 标签对输入的条件概率
那么问题来了 我们应该 学习什么样的生成模型呢?
我们可以学习基于能量的模型 就像玻尔兹曼机
或者也可以学习基于理想化的神经元 的因果模型
这就是我们首先要研究的 最终 我们可能会学习这两种模型的某种
混合体 我们将讲到这种模型为止 在我讲解由神经元构成的因果置信网络之前 我想讲一点关于人工智能和概率的背景知识 二十世纪七八十年代
人工智能研究者 难以置信地反对使用概率 当我还是个研究生的时候
如果你提到了概率 这就意味着 你很笨 你不懂人工智能 电脑就是用于离散的单一处理逻辑的
如果你在里面混入任何概率 那似乎会感染一切 很难想象 人们是有多讨厌概率
以下是一条当时的引文 也许有所帮助 我会把它读出来 很多古希腊人支持苏格拉底的观点：
深刻的 无法解释的思想来自于神 当今的神就是那些捉摸不定的 甚至基于概率的神经元 看起来神经行为增加的随机性 导致的是癫痫和酗酒之类的问题 而不是才华的进步
这条引文出现在Patrick Henry第一本AI教材第一版中 这是当时的普遍观点 Winston后来成为了MIT AI实验室的负责人 这里是另一种观点： 所有这些将会使计算理论与过去和现在的形式逻辑相比 其非零即一的性质将越来越不严格 有种种迹象让我们相信形式逻辑的新体系 将越发接近那些在过去与逻辑学无关的学科 其主要形式就是从玻尔兹曼那里接收到的热力学 这个引文来自John von Neumann 写于1957年 是他留下的未完成手稿的一部分 而这份手稿后来成了他的鼎冠之作
-- 《计算机与人脑（The Computer and the Brain）》 我认为如果von Neumann还活着 
人工智能的历史可能会有点不同 总之 概率最终找到了融入AI的途径 途径的名字叫作图模型 图模型是图理论和概率论的结合 上世纪80年代 大量的AI领域专家系统的工作 将成堆的规则应用于如医疗诊断或矿产勘探的任务上 现在 这些任务已经成为了实际问题
所以他们必须处理非确定性 他们不能只用那些确定世界中的
玩具般的例子了 人们如此不喜欢概率 甚至在处理不确定性时 他们仍然不想使用概率 所以他们自己搞出了一些
不包含概率的处理不确定性的方法 你可以证明 这实际上是个烂赌注 图模型由Pearl, Heckman, Lauritz 和其他许多人共同引入的 这些人认为概率实际上比 做专家系统的那帮人搞出的临时方案更好用 离散图用于展示变量间的依赖关系时是好用的 但是一旦你有了那些图 你就需要做遵循概率规则的实值计算 以算出图中某些点
在给定其他点的观察状态下的期望值 置信网络是搞图模型的人用于命名 有向无环图的某个特定子集的 而且通常他们使用的是稀疏连接图 针对稀疏连接图 他们能用聪明的推理算法 有效计算未被观察到的点的概率 但是这些算法的聪明程度
随相互影响的节点的数量 呈指数性增长 所以他们不能用于密集图 所以 置信网络是由随机变量构成的有向无环图 这里是一个置信网络图
总的来说  你可能观察到任意变量 我将为这个图加上只能观察到叶节点的限制 所以我们只能想象这些不可观测的隐藏节点
他们可能是原因或者也可能被引导(led) 最终他们会产生某些可被观察的效果 当我们观察某些变量时 我们想要解决2个问题 其一 我称之为推理问题 即推断不可观测变量的状态的问题 当然 我们不能推导出确定的结果 所以我们寻求的是不可观测变量的概率分布 而且 如果不可观测变量彼此并不独立 给定可观测变量 我们寻求的概率分布很可能 是个有指数极数目的项的庞然大物 第二个问题是学习问题 即 给定由所有叶节点的
被观察到的状态矢量构成的训练集 我们如何调整变量间的相互作用 以使这个网络产生这些训练数据的可能性更大？ 所以 调整相互作用包括判断节点间被影响的关系 以及判断影响效果的强度 所以 让我来稍微阐述一下 图模型和神经网络的关系 早期的图模型让专家来定义图结构和条件概率 通常 他们请来一位医学专家 向他咨询这个导致那个的可能性 然后他们生成一个图 图中每个节点都有意义
而且他们通常有一个条件概率表 该表描述了父节点的一组数值如何决定 子节点的值分布 他们的图是稀疏连接图
最开始他们关注的问题是 如何进行正确的推导
当时他们对学习并不感兴趣 因为知识来自于专家 相比之下 对于神经网络来说
学习始终是核心问题 手工连线显然很逊
虽然 连线以某些基本性质来说是明智的 --比如在卷积网络中 但是基本上 网络中的知识来自于对训练数据的学习 而不是专家
神经网络的目标并不是引入解释或稀疏连接 以使推断变得容易 无论如何 这里有神经网络版本的置信网络 所以 如果我们思考如何用理想神经元构建生成模型 基本上有2类生成模型是你可以构建的 一种是基于能量的模型 这种模型用对称链接连接二元随机神经元 然后你就得到了波尔兹曼机
波尔兹曼机 我们已经知道 它很难学习
但是如果我们对连接进行限制 受限波尔兹曼机就容易学习了 然而 这种方法只能使我们学习一个隐藏层 所以 我们放弃了很多多层神经网络的能力 只为了使学习容易一些
另一类模型是因果模型 这是由二元随机神经元组成的有向无环图 这种方法得到是sigmoid置信网络 1992年 Neal引入了类似模型并于波尔兹曼机比较 结果显示Sigmoid置信网络的学习要稍微容易点 所以 Sigmoid置信网络就是 所有变量都是二元随机神经元的置信网络 为了从这个模型生成数据 你要从顶层取得神经元 根据他们的偏差 你决定他们的值应该是0还是1 即你随机做这些决定
然后根据顶层神经元的状态 你随机的决定隐含层的神经元应该怎么做 然后根据他们的二元状态 你再来决定可见状态 通过这一系列操作 这种从一层到另一层的因果序列 你将得到你的神经网络相信的
可见数值矢量的无偏差样本 所以因果模型 不像波尔兹曼机 它是易于生成样本的