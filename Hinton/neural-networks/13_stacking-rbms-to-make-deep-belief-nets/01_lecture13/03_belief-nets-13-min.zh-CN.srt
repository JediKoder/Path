1
00:00:00,000 --> 00:00:02,725
本讲中 我将介绍置信网络

2
00:00:02,725 --> 00:00:07,592
我在20世纪90年代一度放弃了反向传播
原因之一就是

3
00:00:07,592 --> 00:00:11,810
它需要太多标签了
那时 没有一个数据集

4
00:00:11,810 --> 00:00:16,547
包含了足够多的标签
同时 人类在学习的时候并没有

5
00:00:16,547 --> 00:00:20,052
用很多显式的标签 这也影响了我

6
00:00:20,052 --> 00:00:24,854
然而 我并不想放弃梯度下降法

7
00:00:24,854 --> 00:00:27,644
因为其优势在于可以同时学习很多权重

8
00:00:27,644 --> 00:00:32,251
这样 问题来了
是否能找到另外的目标函数来做

9
00:00:32,251 --> 00:00:36,252
梯度下降呢?
很显然 我们应该考虑生成模型

10
00:00:36,252 --> 00:00:41,434
就是说 目标函数要拟合
输入数据的概率分布

11
00:00:41,434 --> 00:00:45,580
而不是预测输出的标签
这与统计学和人工智能

12
00:00:45,580 --> 00:00:50,140
的主流— (概率) 图模型正好相符

13
00:00:50,140 --> 00:00:55,348
图模型是要把离散的图结构组合起来

14
00:00:55,348 --> 00:00:58,938
来表示变量之间的依赖关系

15
00:00:58,938 --> 00:01:04,287
在给定其他变量的值的情况下

16
00:01:04,287 --> 00:01:07,454
可以用实值运算来推算某个变量的值

17
00:01:07,454 --> 00:01:12,733
玻尔兹曼机 (Boltzmann Machine) 
事实上是早期图模型的一个例子

18
00:01:12,733 --> 00:01:18,463
但它是无向图模型
1992年, 拉德福德·尼尔 (Radford Neal) 指出

19
00:01:18,463 --> 00:01:23,864
我们可以用与玻尔兹曼机相同的单元

20
00:01:23,864 --> 00:01:28,390
来搭建有向图模型
他称这个模型为Sigmoid置信网络

21
00:01:28,390 --> 00:01:33,820
现在 问题就变成了
我们该如何用Sigmoid置信网络进行学习?

22
00:01:34,100 --> 00:01:38,894
另一个问题则是 深度网络的学习时间的可扩展性并不好

23
00:01:38,894 --> 00:01:41,784
当网络有很多隐含层的时候

24
00:01:41,784 --> 00:01:44,857
训练十分缓慢
你可能想问 这是为什么

25
00:01:44,857 --> 00:01:49,959
现在我们知道原因之一是
权值没有很合理地初始化

26
00:01:49,959 --> 00:01:52,726
还有一个问题

27
00:01:52,726 --> 00:01:55,615
就是反向传播会被困在局部最优值

28
00:01:55,615 --> 00:01:59,119
大部分时候这些极小点是很好的
因此反向传播是有用的

29
00:01:59,119 --> 00:02:04,037
但现在我们可以说对于深度网络
如果以较小的随机权重来开始训练

30
00:02:04,037 --> 00:02:07,910
那通常最后收敛到的局部极小值
与全局极小值相去甚远

31
00:02:07,910 --> 00:02:12,420
当然 退回到简单一些的
允许凸优化的模型也是可能的

32
00:02:12,420 --> 00:02:15,488
但我不认为这是个好主意

33
00:02:15,488 --> 00:02:19,156
数学家喜欢做这样的事
因为他们可以证明一些东西

34
00:02:19,156 --> 00:02:23,607
但在实际应用中 这种简单化是在
与实际数据的复杂度背道而驰

35
00:02:23,607 --> 00:02:28,358
无监督学习是一条解决反向传播
的受限之处的途径

36
00:02:28,358 --> 00:02:31,275
想法就是 我们想保持

37
00:02:31,275 --> 00:02:36,284
用梯度方法和随机分批梯度下降
来调整权值的有效性和简单性

38
00:02:36,284 --> 00:02:40,522
但是我们将运用这一方法来

39
00:02:40,522 --> 00:02:45,274
拟合感官的输入 而不是拟合

40
00:02:45,274 --> 00:02:50,395
输入与输出的关系
因此 我们想要调整权值而最大化的

41
00:02:50,395 --> 00:02:55,225
是生成模型能够正确生成感官输入的概率

42
00:02:55,225 --> 00:02:59,540
在玻尔兹曼机中 我们已经看到过这一点

43
00:02:59,540 --> 00:03:04,036
打个比方 如果你想

44
00:03:04,036 --> 00:03:08,764
做计算机视觉 那你应该
先学习计算机图形学

45
00:03:08,764 --> 00:03:13,046
首先 图形学是好用的 而计算机视觉目前不是

46
00:03:13,046 --> 00:03:17,902
生成模型的学习目标 就像玻尔兹曼机

47
00:03:17,902 --> 00:03:22,503
是最大化观测到的数据的概率
而不是最大化

48
00:03:22,503 --> 00:03:27,517
标签对输入的条件概率
那么问题来了 我们应该

49
00:03:27,517 --> 00:03:32,799
学习什么样的生成模型呢?
我们可以学习基于能量的模型

50
00:03:32,799 --> 00:03:37,598
就像玻尔兹曼机
或者也可以学习基于理想化的神经元

51
00:03:37,598 --> 00:03:41,120
的因果模型
这就是我们首先要研究的

52
00:03:41,620 --> 00:03:46,318
最终 我们可能会学习这两种模型的某种
混合体 我们将讲到这种模型为止

53
00:03:46,318 --> 00:03:49,643
在我讲解由神经元构成的因果置信网络之前

54
00:03:49,643 --> 00:03:54,673
我想讲一点关于人工智能和概率的背景知识

55
00:03:54,673 --> 00:03:59,436
二十世纪七八十年代
人工智能研究者

56
00:03:59,436 --> 00:04:03,085
难以置信地反对使用概率

57
00:04:03,085 --> 00:04:07,725
当我还是个研究生的时候
如果你提到了概率 这就意味着

58
00:04:07,725 --> 00:04:11,003
你很笨 你不懂人工智能

59
00:04:11,003 --> 00:04:15,890
电脑就是用于离散的单一处理逻辑的
如果你在里面混入任何概率

60
00:04:15,890 --> 00:04:18,860
那似乎会感染一切

61
00:04:19,140 --> 00:04:24,958
很难想象 人们是有多讨厌概率
以下是一条当时的引文 也许有所帮助

62
00:04:24,958 --> 00:04:28,255
我会把它读出来

63
00:04:28,255 --> 00:04:33,251
很多古希腊人支持苏格拉底的观点：
深刻的 无法解释的思想来自于神

64
00:04:33,251 --> 00:04:36,982
当今的神就是那些捉摸不定的 甚至基于概率的神经元

65
00:04:36,982 --> 00:04:41,220
看起来神经行为增加的随机性

66
00:04:41,220 --> 00:04:46,026
导致的是癫痫和酗酒之类的问题

67
00:04:46,026 --> 00:04:52,281
而不是才华的进步
这条引文出现在Patrick Henry第一本AI教材第一版中

68
00:04:52,281 --> 00:04:57,456
这是当时的普遍观点

69
00:04:57,456 --> 00:05:00,736
Winston后来成为了MIT AI实验室的负责人

70
00:05:00,736 --> 00:05:03,506
这里是另一种观点：

71
00:05:03,506 --> 00:05:09,265
所有这些将会使计算理论与过去和现在的形式逻辑相比

72
00:05:09,265 --> 00:05:13,420
其非零即一的性质将越来越不严格

73
00:05:14,020 --> 00:05:18,860
有种种迹象让我们相信形式逻辑的新体系

74
00:05:18,860 --> 00:05:23,462
将越发接近那些在过去与逻辑学无关的学科

75
00:05:23,462 --> 00:05:27,048
其主要形式就是从玻尔兹曼那里接收到的热力学

76
00:05:27,048 --> 00:05:32,282
这个引文来自John von Neumann

77
00:05:32,282 --> 00:05:36,810
写于1957年 是他留下的未完成手稿的一部分

78
00:05:36,810 --> 00:05:40,700
而这份手稿后来成了他的鼎冠之作
-- 《计算机与人脑（The Computer and the Brain）》

79
00:05:41,540 --> 00:05:45,432
我认为如果von Neumann还活着 
人工智能的历史可能会有点不同

80
00:05:45,432 --> 00:05:50,262
总之 概率最终找到了融入AI的途径

81
00:05:50,262 --> 00:05:53,816
途径的名字叫作图模型

82
00:05:53,816 --> 00:05:58,081
图模型是图理论和概率论的结合

83
00:05:58,081 --> 00:06:03,766
上世纪80年代 大量的AI领域专家系统的工作

84
00:06:03,766 --> 00:06:08,600
将成堆的规则应用于如医疗诊断或矿产勘探的任务上

85
00:06:08,900 --> 00:06:12,700
现在 这些任务已经成为了实际问题
所以他们必须处理非确定性

86
00:06:12,700 --> 00:06:16,180
他们不能只用那些确定世界中的
玩具般的例子了

87
00:06:16,560 --> 00:06:21,935
人们如此不喜欢概率 甚至在处理不确定性时

88
00:06:21,935 --> 00:06:25,337
他们仍然不想使用概率

89
00:06:25,337 --> 00:06:30,917
所以他们自己搞出了一些
不包含概率的处理不确定性的方法

90
00:06:30,917 --> 00:06:34,727
你可以证明 这实际上是个烂赌注

91
00:06:34,727 --> 00:06:38,191
图模型由Pearl, Heckman, Lauritz 和其他许多人共同引入的

92
00:06:38,191 --> 00:06:43,374
这些人认为概率实际上比

93
00:06:43,374 --> 00:06:48,159
做专家系统的那帮人搞出的临时方案更好用

94
00:06:48,159 --> 00:06:53,475
离散图用于展示变量间的依赖关系时是好用的

95
00:06:53,475 --> 00:06:57,189
但是一旦你有了那些图

96
00:06:57,189 --> 00:07:02,565
你就需要做遵循概率规则的实值计算

97
00:07:02,565 --> 00:07:07,309
以算出图中某些点
在给定其他点的观察状态下的期望值

98
00:07:07,309 --> 00:07:12,210
置信网络是搞图模型的人用于命名

99
00:07:12,210 --> 00:07:17,788
有向无环图的某个特定子集的

100
00:07:17,788 --> 00:07:22,085
而且通常他们使用的是稀疏连接图

101
00:07:22,085 --> 00:07:24,950
针对稀疏连接图

102
00:07:24,950 --> 00:07:30,152
他们能用聪明的推理算法

103
00:07:30,152 --> 00:07:33,620
有效计算未被观察到的点的概率

104
00:07:33,900 --> 00:07:38,769
但是这些算法的聪明程度
随相互影响的节点的数量

105
00:07:38,769 --> 00:07:43,120
呈指数性增长 所以他们不能用于密集图

106
00:07:43,520 --> 00:07:48,612
所以 置信网络是由随机变量构成的有向无环图

107
00:07:48,612 --> 00:07:54,120
这里是一个置信网络图
总的来说  你可能观察到任意变量

108
00:07:54,120 --> 00:07:57,692
我将为这个图加上只能观察到叶节点的限制

109
00:07:57,692 --> 00:08:04,971
所以我们只能想象这些不可观测的隐藏节点
他们可能是原因或者也可能被引导(led)

110
00:08:04,971 --> 00:08:10,726
最终他们会产生某些可被观察的效果

111
00:08:10,726 --> 00:08:15,182
当我们观察某些变量时 我们想要解决2个问题

112
00:08:15,182 --> 00:08:20,269
其一 我称之为推理问题

113
00:08:20,269 --> 00:08:24,064
即推断不可观测变量的状态的问题

114
00:08:24,064 --> 00:08:28,160
当然 我们不能推导出确定的结果

115
00:08:28,160 --> 00:08:31,171
所以我们寻求的是不可观测变量的概率分布

116
00:08:31,171 --> 00:08:35,568
而且 如果不可观测变量彼此并不独立

117
00:08:35,568 --> 00:08:40,086
给定可观测变量 我们寻求的概率分布很可能

118
00:08:40,086 --> 00:08:43,520
是个有指数极数目的项的庞然大物

119
00:08:45,220 --> 00:08:48,080
第二个问题是学习问题

120
00:08:48,440 --> 00:08:54,333
即 给定由所有叶节点的
被观察到的状态矢量构成的训练集

121
00:08:54,333 --> 00:08:58,143
我们如何调整变量间的相互作用

122
00:08:58,143 --> 00:09:03,390
以使这个网络产生这些训练数据的可能性更大？

123
00:09:03,390 --> 00:09:08,565
所以 调整相互作用包括判断节点间被影响的关系

124
00:09:08,565 --> 00:09:13,669
以及判断影响效果的强度

125
00:09:13,669 --> 00:09:17,191
所以 让我来稍微阐述一下

126
00:09:17,191 --> 00:09:21,360
图模型和神经网络的关系

127
00:09:22,760 --> 00:09:28,460
早期的图模型让专家来定义图结构和条件概率

128
00:09:28,460 --> 00:09:33,199
通常 他们请来一位医学专家

129
00:09:33,199 --> 00:09:38,763
向他咨询这个导致那个的可能性 然后他们生成一个图

130
00:09:38,763 --> 00:09:43,159
图中每个节点都有意义
而且他们通常有一个条件概率表

131
00:09:43,159 --> 00:09:48,653
该表描述了父节点的一组数值如何决定

132
00:09:48,653 --> 00:09:52,500
子节点的值分布

133
00:09:54,360 --> 00:10:00,159
他们的图是稀疏连接图
最开始他们关注的问题是

134
00:10:00,159 --> 00:10:04,435
如何进行正确的推导
当时他们对学习并不感兴趣

135
00:10:04,435 --> 00:10:07,720
因为知识来自于专家

136
00:10:07,720 --> 00:10:12,863
相比之下 对于神经网络来说
学习始终是核心问题

137
00:10:12,863 --> 00:10:17,456
手工连线显然很逊
虽然 连线以某些基本性质来说是明智的

138
00:10:17,456 --> 00:10:21,620
--比如在卷积网络中

139
00:10:21,620 --> 00:10:27,094
但是基本上 网络中的知识来自于对训练数据的学习

140
00:10:27,094 --> 00:10:30,378
而不是专家
神经网络的目标并不是引入解释或稀疏连接

141
00:10:30,378 --> 00:10:34,963
以使推断变得容易

142
00:10:34,963 --> 00:10:39,205
无论如何 这里有神经网络版本的置信网络

143
00:10:39,205 --> 00:10:43,858
所以 如果我们思考如何用理想神经元构建生成模型

144
00:10:43,858 --> 00:10:47,279
基本上有2类生成模型是你可以构建的

145
00:10:47,279 --> 00:10:51,466
一种是基于能量的模型

146
00:10:51,466 --> 00:10:56,650
这种模型用对称链接连接二元随机神经元

147
00:10:56,650 --> 00:11:01,484
然后你就得到了波尔兹曼机
波尔兹曼机 我们已经知道

148
00:11:01,484 --> 00:11:04,969
它很难学习
但是如果我们对连接进行限制

149
00:11:04,969 --> 00:11:08,137
受限波尔兹曼机就容易学习了

150
00:11:08,137 --> 00:11:11,939
然而 这种方法只能使我们学习一个隐藏层

151
00:11:11,939 --> 00:11:17,072
所以 我们放弃了很多多层神经网络的能力

152
00:11:17,072 --> 00:11:23,708
只为了使学习容易一些
另一类模型是因果模型

153
00:11:23,708 --> 00:11:27,464
这是由二元随机神经元组成的有向无环图

154
00:11:27,464 --> 00:11:32,108
这种方法得到是sigmoid置信网络

155
00:11:32,108 --> 00:11:36,435
1992年 Neal引入了类似模型并于波尔兹曼机比较

156
00:11:36,435 --> 00:11:42,297
结果显示Sigmoid置信网络的学习要稍微容易点

157
00:11:42,297 --> 00:11:46,958
所以 Sigmoid置信网络就是

158
00:11:46,958 --> 00:11:51,265
所有变量都是二元随机神经元的置信网络

159
00:11:51,265 --> 00:11:56,280
为了从这个模型生成数据 你要从顶层取得神经元

160
00:11:56,640 --> 00:12:00,673
根据他们的偏差 你决定他们的值应该是0还是1

161
00:12:00,673 --> 00:12:05,110
即你随机做这些决定
然后根据顶层神经元的状态

162
00:12:05,110 --> 00:12:09,662
你随机的决定隐含层的神经元应该怎么做

163
00:12:09,662 --> 00:12:13,580
然后根据他们的二元状态

164
00:12:13,580 --> 00:12:16,750
你再来决定可见状态

165
00:12:16,750 --> 00:12:22,298
通过这一系列操作 这种从一层到另一层的因果序列

166
00:12:22,298 --> 00:12:27,917
你将得到你的神经网络相信的
可见数值矢量的无偏差样本

167
00:12:27,917 --> 00:12:32,974
所以因果模型 不像波尔兹曼机

168
00:12:32,974 --> 00:12:35,643
它是易于生成样本的