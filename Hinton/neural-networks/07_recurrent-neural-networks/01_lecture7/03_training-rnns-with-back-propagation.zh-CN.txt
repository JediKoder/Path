在这一讲的视频中 我将讲解随着时间的反向传播算法 这是训练或者循环你的神经网络的标准做法 这个算法非常的简单 一旦你看到 循环神经网络和一个每个时间步长是一层的前馈神经网络 是等效的 我也会讲解给循环神经网络提供 输入和理想输出的方法 这张图展示了一个简单的
有着三个互相连接的神经元的循环网络 我们假设在那些连接之间的
时间延迟为1 并且这个网络以离散时间运行
所以时钟是整数刻度 理解如何训练循环神经网络的关键是
要明白 循环神经网络真的只是和前馈网络一模一样 而前馈神经网络只是把循环网络扩展而已
所以循环神经网络从初始化 一些状态开始
这张图的底部是 0时刻 然后用这些连接来得到一个新的状态 如时刻1所示
你接着再用相同的权重来得到 有一个新的状态
接着再用相同的权重来得到另一个新的 状态 以此类推
所以它只是一个不断前进的前馈网络 而权重有一个约束
每一层都要是相同的 当有了权重约束后
反向传播就适合用于学习 我们在卷积网络中已经看到过
提醒一下 事实上我们可以 很容易地在反向传播中加入线性约束
所以我们像以往一样 计算梯度 就好像权重没有被约束 接着我们就修改梯度
这样我们就保持了约束 所以如果我们想让W1等于W2
我们就从相同权重出发 接着需要确保 改变W1和改变W2是相等的 然后我们只要简单地对W1求导 对W2求导 然后
对导数相加或者取平均 然后 用所得的相同结果对W1和W2更新 所以如果权重一开始就满足约束
他们会继续 满足约束
随着时间的反向传播算法 正如其名 你可以把一个循环网络想成
是一个共享权重的前进的前馈网络 前馈网络
用反向传播来训练 所以我们可以考虑一个
在时间域内的算法 前向的每次传播在每个时间片
建立一堆的活动 而反向的每次传播把这堆活动一片片地取出
然后计算误差导数 这就是为什么它叫做随着时间的反向传播 在反向传播后 我们可以 把每个在不同时间段的权重导数相加 然后改变所有的相同权重 通过用与所有导数和或者均值成比例的值
用来更新 有一个令人麻烦的额外问题 如果我们没有指定所有单元的初始状态
比如 如果一些但是是隐藏或者输出层 那么我们需要把他们初始化成
一些特定的状态 我们可以就把这些初始化状态 设置成默认值0.5 但是
这也许不会让系统工作的那么好 相比于把它初始化成一些
更明智的初始值 事实上我们可以学习到
初始状态 我们可以把它们当做参数
而不是活动 然后就可以像学习权重 一样学习它们 我们用一些随机值来初始化 这些初始状态 这包括所有的非 输入单元的所有其他单元 接着再
在每次一系列的训练后 我们反向 沿着时间序列一直传播到
初始状态 这就给了我们初始状态的误差函数的导数 我们接着就只需要用这些导数 来调整初始状态
我们用梯度下降法 这就 给了我们不太相同的初始化状态 有许多方法可以让我们为
循环神经网络提供输入 我们可以 比如 指定好所有单元的初始状态 这是我们能够最直接就想到的
当我们考虑一个像是有着权重约束的前馈网络 一样的循环网络
我们可以指定一部分单元的初始化状态 或者我们可以指定
在每个时刻的一部分单元的状态 这也许是对输入序列数据的最
直接的方法 相似地 我们有许多方法来指定 循环网络的输出
当我们把它想成是有权重约束的 前馈神经网络时
最自然的事就是指定所有单元的 理想的最后状态 
如果我们是要训练它来 适用于一些吸引子 我们也许想要指定
不仅仅是最后时刻的 理想状态也包括一些时刻的状态
事实上这就会使它在那些状态下 沉淀 而不是传播某些状态
然后在某些其它地方停止 所以通过指定最后的一些状态
我们可以迫使它来学习吸引子 并且这非常的简单 因为我们可以
反向传播在每个时刻得到的 导数 所以反向传播从顶开始 带着最后时刻的导数 接着当我们沿着顶端之后的这条线
向后 我们为它加入导数 以此类推 所以在许多不同层
求导数 是几乎不费力的 或者我们
可以指定我们认为是输出单元的一部分单元的 设计活动 这是很自然的方法 训练一个
可以得到连续输出值的 循环神经网络
翻译 Slyne