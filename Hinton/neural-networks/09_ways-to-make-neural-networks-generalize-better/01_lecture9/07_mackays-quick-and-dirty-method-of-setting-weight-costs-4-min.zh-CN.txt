在这段视频中，我将要描述一种在20世纪90年代，由大卫麦基开发的方法。 这种方法用作决定神经网络所用的权值衰减 而不需要使用验证集。它是基于一种想法，我们能够 边进行估值预算，边理解全职衰减，由此 权值衰减的量值能关联至前权重分布的紧密程度。 麦基展示了我们能经验性的同时 将权值衰减和已假设噪音两者同时代入神经网络的输出中，以此而得到一种 不需验证集的拟合权重损失的方法，而且 同时，也允许我们对不同的子集有不同的权重损失， 因为神经网络中的链接，要付出非常大的代价而 使用验证集来进行。麦基进而使用这种方法而赢取了一些竞赛。 现在，我将要描述一种简单而 有实际应用的方法，它由大卫麦基所开发，基于这样一种事实，我们能够 将权值损失理解为两种方差之比。 在我们学习了一种能最小化方差的模型之后，我们能够找到针对输出方差的最佳值 而这最佳值则是通过简单应残留误差的方差而找到的。 我们也可以在获得权值之前，而估计高斯方差。 我们必须由猜测方差应该为多少而出发， 然后，我们进行一些学习，继而我们能使用 一种非常简易的名叫实用贝叶斯的方法。我们将前方差设定为 学习后模型权重的方差，因为那个方差 最有可能对应生成权重。这很大程度上违反了 许多贝叶斯方法的假定预设。我们是通过使用数据来决定我们的前设猜想。 因此，一旦我们知道了权重，我们将 一个平均值为0的高斯分布拟合至学习到的权重的一维分布。 然后，我们使用高斯方差， 而将它应为我们的前设猜想。现在，有一件非常好的事情，就是 不同子集的权重，譬如在不同层，我们 能许熙道不同层的方差。 我们不需要一个验证集，因此我们能使用所有非测试数据而用于训练。 以为我们不需要验证集就能决定不同层面的权重损失， 我能实际上能有不同的权重损失。 通过验证集，这将会非常困难。 综上，这是麦基方法。你首先猜测噪音方差和 前权重方差。实际上，你只需要猜测它们的比值。 继而，你使用梯度下降学习 而提高权重质量。然后，你将噪音方差重设至 残留误差的方差，然后你将前权重方差设置为 实际学习权重的分布。 然后，你需要重新循环这段过程。 其实这种方法在应用中相当有效。 麦基也因此而赢取了许多竞赛。