1
00:00:00,000 --> 00:00:05,938
在这个视频中
我们将要讨论权重惩罚的贝叶斯解释

2
00:00:05,938 --> 00:00:10,085
在完整的贝叶斯方法中

3
00:00:10,085 --> 00:00:15,657
我们尝试计算模型参数每一个
可能取值点的后验概率

4
00:00:15,657 --> 00:00:18,992
但是这里还有一个简化版的贝叶斯方法

5
00:00:18,992 --> 00:00:24,032
就是仅仅去寻找一个参数集

6
00:00:24,032 --> 00:00:29,138
能够最好地在匹配我们关于参数的先验信息

7
00:00:29,138 --> 00:00:33,720
和匹配观测到的数据之间平衡

8
00:00:34,040 --> 00:00:38,868
这叫做最大alpha后验学习

9
00:00:38,868 --> 00:00:45,195
当我们使用权重衰减来控制模型相容性时
这很好地解释了真正发生了什么

10
00:00:45,195 --> 00:00:49,999
我接下来将要解释一下

11
00:00:49,999 --> 00:00:55,064
当我们最小化
监督最大似然学习的平方误差时

12
00:00:55,064 --> 00:01:00,398
都发生了什么
寻找权重向量来最小化

13
00:01:00,398 --> 00:01:05,005
真实值与模型预测值之间的平方误差

14
00:01:05,005 --> 00:01:09,427
与寻找一个权重向量

15
00:01:09,427 --> 00:01:13,420
来最大化正确答案的对数概率密度
是等价的

16
00:01:14,020 --> 00:01:19,141
为了证明这个等价性
我们要先假设正确答案

17
00:01:19,141 --> 00:01:23,672
是由神经网络的输出值
加上一个高斯噪声来产生的

18
00:01:23,672 --> 00:01:28,990
所以想法就是我们从输入用神经网络得到输出

19
00:01:28,990 --> 00:01:32,733
然后加一些高斯噪声

20
00:01:32,733 --> 00:01:37,920
然后我们问
当我们这样做的时候

21
00:01:37,920 --> 00:01:43,344
所以模型的输出值是某个高斯分布的中心

22
00:01:43,344 --> 00:01:49,050
并且我们对这个高斯分布中
概率最高的取值很感兴趣

23
00:01:49,050 --> 00:01:55,146
因为在申请网络输出y的条件下
产生值t的概率

24
00:01:55,146 --> 00:02:01,477
就是在以y为中心的高斯分布中

25
00:02:01,477 --> 00:02:11,688
t点的概率
所以事情就是这样的

26
00:02:11,688 --> 00:02:17,350
我们假设神经网络在训练集c上
的输出为yc

27
00:02:17,350 --> 00:02:25,780
并且这个输出是通过对输入c应用了
权重W后得出的

28
00:02:25,780 --> 00:02:32,509
我们添加高斯噪声后得到正确目标值
的概率

29
00:02:32,509 --> 00:02:38,892
由一个以yc为中心的高斯分布决定
所以我们对目标值的分布密度感兴趣

30
00:02:38,892 --> 00:02:44,261
这个分布密度是给定以神经网络的输出
为中心的高斯密度的条件下的条件密度

31
00:02:44,261 --> 00:02:47,554
在右边我们有一个以yc为均值
的一个高斯分布

32
00:02:47,554 --> 00:02:53,786
我们同时假设了某个方差的取值

33
00:02:53,786 --> 00:03:01,927
这个方差在之后会很重要
如果我们对式子取负对数

34
00:03:01,927 --> 00:03:05,805
我们会发现在给定神经网络输出yc的条件下

35
00:03:05,805 --> 00:03:12,887
目标值tc的条件分布密度的负对数

36
00:03:12,887 --> 00:03:19,715
是一个来自正态分布归一化系数的常数加上

37
00:03:19,715 --> 00:03:25,364
负的e的(tc-yc)^2除以两倍方差次幂

38
00:03:25,364 --> 00:03:30,145
你可以看见

39
00:03:30,145 --> 00:03:36,291
如果我们的损失函数是
得到正确答案概率的负对数

40
00:03:36,291 --> 00:03:44,069
这就变成了最小化一个平方距离
了解以下一点是很有帮助的

41
00:03:44,069 --> 00:03:48,788
当你看到一个平方误差被最小化时
你可以用概率的观点来解释这一切

42
00:03:48,788 --> 00:03:53,688
并且在这个概率的解释中
你是在最大化一个

43
00:03:53,688 --> 00:04:01,197
高斯密度的对数
所以这种漂亮的贝叶斯方法

44
00:04:01,197 --> 00:04:05,400
就是寻找所有可能取值的权重向量
的整个后验分布

45
00:04:06,060 --> 00:04:11,565
如果权重很多的话
那么当你处理非线性网络的时候就很绝望

46
00:04:11,565 --> 00:04:15,705
贝叶斯学者有很多方法

47
00:04:15,705 --> 00:04:19,899
来近似这个分布
通常使用的手段是蒙特卡洛方法

48
00:04:19,899 --> 00:04:23,700
从时间角度考虑
让我们做一些更简单的

49
00:04:23,980 --> 00:04:27,889
我们仅仅去寻找最可能的
权重向量

50
00:04:27,889 --> 00:04:33,149
那么关于权重的单一设定就是
给定数据和先验信息是最可能的那个

51
00:04:33,149 --> 00:04:40,268
那么我们要做的就是寻找

52
00:04:40,268 --> 00:04:44,618
一个最优的W
这个过程从一个随机的向量开始

53
00:04:44,618 --> 00:04:49,026
然后寻找给定数据下
能够提高取该权重可能性的方向

54
00:04:49,026 --> 00:04:53,280
这样我们只能得到一个局部的极小值

55
00:04:55,620 --> 00:05:01,143
这时候取对数之后问题会变得更好处理

56
00:05:01,143 --> 00:05:04,310
如果我们想最小化一个损失函数

57
00:05:04,310 --> 00:05:10,745
我们最好是对他取负对数
这也正是我们为何最大化对数概率的和

58
00:05:10,745 --> 00:05:14,940
或者最小化负对数概率的和

59
00:05:15,400 --> 00:05:20,327
我们最后把问题归结到最大化数据的概率

60
00:05:20,327 --> 00:05:25,057
也就是最大化我们对不同训练情境下

61
00:05:25,057 --> 00:05:29,000
得到的观测产生目标值的概率的乘积

62
00:05:30,140 --> 00:05:35,475
如果我们假设不同情形下输出误差是独立的

63
00:05:35,475 --> 00:05:41,016
那么这个就能表示为所有不同情况下

64
00:05:41,016 --> 00:05:44,300
给定权重条件下产生目标值概率的乘积

65
00:05:44,640 --> 00:05:50,477
这也就是在给定网络输出值的条件下
得到tc的概率的乘积

66
00:05:50,477 --> 00:05:55,400
如果我们输入c和权重W的话

67
00:05:58,300 --> 00:06:03,143
对数函数是单调的
所以取对数不会改变最大值点

68
00:06:03,143 --> 00:06:08,458
所以我们不去最大化概率的乘积
而是去最大化对数概率的和

69
00:06:08,458 --> 00:06:13,100
这通常在计算机上处理起来更方便

70
00:06:13,100 --> 00:06:18,950
而且会更稳定
所以我们最大化给定权重

71
00:06:18,950 --> 00:06:24,048
产生数据的概率的对数
也就是对所有训练情形来说

72
00:06:24,048 --> 00:06:29,021
给定输入和权重下产生输出的概率
取对数之后的和式

73
00:06:29,021 --> 00:06:33,625
在最大后验学习中

74
00:06:33,625 --> 00:06:39,835
我们总是尝试寻找一些权重
能够最优化拟合先验和拟合数据之间的权衡

75
00:06:39,835 --> 00:06:44,080
所以这是最基本的定理

76
00:06:44,900 --> 00:06:50,270
如果我们取了负对数来得到一个损失函数
那么我们得到的是给定数据后

77
00:06:50,270 --> 00:06:56,284
权重向量如此取值的概率之负对数
可以表达成一个先验项对数的相反数

78
00:06:56,284 --> 00:07:00,007
加上数据项对数的相反数
再加上一个额外的项

79
00:07:00,007 --> 00:07:04,733
这就是这个额外项
是对所有可能的权重向量积分后的结果

80
00:07:04,733 --> 00:07:09,816
这一项不影响W
所以当对W做优化时 我们可以丢掉这一项

81
00:07:09,816 --> 00:07:15,116
依赖于数据的那一项

82
00:07:15,116 --> 00:07:20,160
是给定W下条件概率的负对数
并且这是我们的正态误差项

83
00:07:21,460 --> 00:07:27,451
只依赖于W的那一项是先验信息下
W概率的负对数

84
00:07:27,451 --> 00:07:32,710
最大化权重概率的对数

85
00:07:32,710 --> 00:07:37,120
与最小化一个平方距离有关

86
00:07:37,120 --> 00:07:42,122
这和最大化产生正确的目标值概率的对数与
最小化平方距离有关一样

87
00:07:42,122 --> 00:07:48,124
所以最小化平方权重与

88
00:07:48,124 --> 00:07:52,800
最小化权重概率的对数是等价的

89
00:07:52,800 --> 00:07:56,682
这个等价性在零均值高斯先验
这样一个条件下成立

90
00:07:56,682 --> 00:08:02,411
这就是一个高斯分布其均值为零
并且我们想要最大化权重向量的概率

91
00:08:02,411 --> 00:08:08,366
或者说权重概率的对数
为了这么做 我们显然要

92
00:08:08,366 --> 00:08:14,784
让w接近于均值零
这个高斯分布的表达式就是这样的

93
00:08:14,784 --> 00:08:19,000
他的均值是零
所以我们不需要把他放进去

94
00:08:19,320 --> 00:08:25,375
w概率的对数正是被两倍的方差除

95
00:08:25,375 --> 00:08:31,509
的权重平方加上一个来自高斯分布归一化系数的项

96
00:08:31,509 --> 00:08:38,853
当我们改变w的时候不会受到影响
所以我们最终得到了

97
00:08:38,853 --> 00:08:42,160
权值衰减或者说权重惩罚的解释

98
00:08:42,440 --> 00:08:47,321
我们尝试最小化给定数据后权重向量概率的负对数

99
00:08:47,321 --> 00:08:52,143
这涉及到最小化与权重有关的项

100
00:08:52,143 --> 00:08:57,143
也就是说目标值如何改变并且确定下来
仅仅依赖于权重

101
00:08:57,143 --> 00:09:03,919
这来源于给定权重下数据概率的对数

102
00:09:03,919 --> 00:09:09,401
如果我们假设模型输出有
一个加性的高斯噪声

103
00:09:09,401 --> 00:09:14,289
那么对数概率是

104
00:09:14,289 --> 00:09:19,506
输出值y和t之间距离的平方
除以两倍的方差

105
00:09:19,506 --> 00:09:25,414
类似的 如果我们假设权重的
先验分布服从高斯分布

106
00:09:25,414 --> 00:09:31,929
那么在这个先验下的对数概率就是

107
00:09:31,929 --> 00:09:37,640
权重值的平方除以两倍的高斯分布之方差

108
00:09:40,220 --> 00:09:45,688
让我们写下这个方程
并且等式两边同乘以sigma D的平方

109
00:09:45,688 --> 00:09:51,373
这样我们就得到了一个新的损失函数
并且当我们乘完以后

110
00:09:51,373 --> 00:09:57,058
第一项变成了所有输出值与目标值差的平方和

111
00:09:57,058 --> 00:10:02,527
也就是我们一般在神经网络上
最小化的平方误差

112
00:10:02,527 --> 00:10:07,952
第二项就变成了两个方差的比

113
00:10:07,952 --> 00:10:12,454
乘以权重值的平方

114
00:10:12,454 --> 00:10:19,080
这样你就能看到
两个方差的比值就是权重惩罚

115
00:10:19,080 --> 00:10:22,658
所以我们一开始

116
00:10:22,658 --> 00:10:27,137
把权重惩罚看作一个你用来
让事情变得更好的一个数

117
00:10:27,137 --> 00:10:31,496
通过使用验证集
我们可以固定权重惩罚的数值

118
00:10:31,496 --> 00:10:36,096
但是我们现在可以看到
如果我们使用了高斯分布的解释角度

119
00:10:36,096 --> 00:10:40,999
也就是说对先验分布以及输出-目标值关系模型
两处的高斯分布假设

120
00:10:40,999 --> 00:10:44,571
那么权重惩罚就是由分布的方差决定的

121
00:10:44,571 --> 00:10:48,929
也就是这两个分布方差的比值

122
00:10:48,929 --> 00:10:52,380
在这个框架下 其取值并不是随意的
翻译: iChen | 审核:
Coursera Global Translator Community