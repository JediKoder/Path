在这个视频中
我们将要讨论权重惩罚的贝叶斯解释 在完整的贝叶斯方法中 我们尝试计算模型参数每一个
可能取值点的后验概率 但是这里还有一个简化版的贝叶斯方法 就是仅仅去寻找一个参数集 能够最好地在匹配我们关于参数的先验信息 和匹配观测到的数据之间平衡 这叫做最大alpha后验学习 当我们使用权重衰减来控制模型相容性时
这很好地解释了真正发生了什么 我接下来将要解释一下 当我们最小化
监督最大似然学习的平方误差时 都发生了什么
寻找权重向量来最小化 真实值与模型预测值之间的平方误差 与寻找一个权重向量 来最大化正确答案的对数概率密度
是等价的 为了证明这个等价性
我们要先假设正确答案 是由神经网络的输出值
加上一个高斯噪声来产生的 所以想法就是我们从输入用神经网络得到输出 然后加一些高斯噪声 然后我们问
当我们这样做的时候 所以模型的输出值是某个高斯分布的中心 并且我们对这个高斯分布中
概率最高的取值很感兴趣 因为在申请网络输出y的条件下
产生值t的概率 就是在以y为中心的高斯分布中 t点的概率
所以事情就是这样的 我们假设神经网络在训练集c上
的输出为yc 并且这个输出是通过对输入c应用了
权重W后得出的 我们添加高斯噪声后得到正确目标值
的概率 由一个以yc为中心的高斯分布决定
所以我们对目标值的分布密度感兴趣 这个分布密度是给定以神经网络的输出
为中心的高斯密度的条件下的条件密度 在右边我们有一个以yc为均值
的一个高斯分布 我们同时假设了某个方差的取值 这个方差在之后会很重要
如果我们对式子取负对数 我们会发现在给定神经网络输出yc的条件下 目标值tc的条件分布密度的负对数 是一个来自正态分布归一化系数的常数加上 负的e的(tc-yc)^2除以两倍方差次幂 你可以看见 如果我们的损失函数是
得到正确答案概率的负对数 这就变成了最小化一个平方距离
了解以下一点是很有帮助的 当你看到一个平方误差被最小化时
你可以用概率的观点来解释这一切 并且在这个概率的解释中
你是在最大化一个 高斯密度的对数
所以这种漂亮的贝叶斯方法 就是寻找所有可能取值的权重向量
的整个后验分布 如果权重很多的话
那么当你处理非线性网络的时候就很绝望 贝叶斯学者有很多方法 来近似这个分布
通常使用的手段是蒙特卡洛方法 从时间角度考虑
让我们做一些更简单的 我们仅仅去寻找最可能的
权重向量 那么关于权重的单一设定就是
给定数据和先验信息是最可能的那个 那么我们要做的就是寻找 一个最优的W
这个过程从一个随机的向量开始 然后寻找给定数据下
能够提高取该权重可能性的方向 这样我们只能得到一个局部的极小值 这时候取对数之后问题会变得更好处理 如果我们想最小化一个损失函数 我们最好是对他取负对数
这也正是我们为何最大化对数概率的和 或者最小化负对数概率的和 我们最后把问题归结到最大化数据的概率 也就是最大化我们对不同训练情境下 得到的观测产生目标值的概率的乘积 如果我们假设不同情形下输出误差是独立的 那么这个就能表示为所有不同情况下 给定权重条件下产生目标值概率的乘积 这也就是在给定网络输出值的条件下
得到tc的概率的乘积 如果我们输入c和权重W的话 对数函数是单调的
所以取对数不会改变最大值点 所以我们不去最大化概率的乘积
而是去最大化对数概率的和 这通常在计算机上处理起来更方便 而且会更稳定
所以我们最大化给定权重 产生数据的概率的对数
也就是对所有训练情形来说 给定输入和权重下产生输出的概率
取对数之后的和式 在最大后验学习中 我们总是尝试寻找一些权重
能够最优化拟合先验和拟合数据之间的权衡 所以这是最基本的定理 如果我们取了负对数来得到一个损失函数
那么我们得到的是给定数据后 权重向量如此取值的概率之负对数
可以表达成一个先验项对数的相反数 加上数据项对数的相反数
再加上一个额外的项 这就是这个额外项
是对所有可能的权重向量积分后的结果 这一项不影响W
所以当对W做优化时 我们可以丢掉这一项 依赖于数据的那一项 是给定W下条件概率的负对数
并且这是我们的正态误差项 只依赖于W的那一项是先验信息下
W概率的负对数 最大化权重概率的对数 与最小化一个平方距离有关 这和最大化产生正确的目标值概率的对数与
最小化平方距离有关一样 所以最小化平方权重与 最小化权重概率的对数是等价的 这个等价性在零均值高斯先验
这样一个条件下成立 这就是一个高斯分布其均值为零
并且我们想要最大化权重向量的概率 或者说权重概率的对数
为了这么做 我们显然要 让w接近于均值零
这个高斯分布的表达式就是这样的 他的均值是零
所以我们不需要把他放进去 w概率的对数正是被两倍的方差除 的权重平方加上一个来自高斯分布归一化系数的项 当我们改变w的时候不会受到影响
所以我们最终得到了 权值衰减或者说权重惩罚的解释 我们尝试最小化给定数据后权重向量概率的负对数 这涉及到最小化与权重有关的项 也就是说目标值如何改变并且确定下来
仅仅依赖于权重 这来源于给定权重下数据概率的对数 如果我们假设模型输出有
一个加性的高斯噪声 那么对数概率是 输出值y和t之间距离的平方
除以两倍的方差 类似的 如果我们假设权重的
先验分布服从高斯分布 那么在这个先验下的对数概率就是 权重值的平方除以两倍的高斯分布之方差 让我们写下这个方程
并且等式两边同乘以sigma D的平方 这样我们就得到了一个新的损失函数
并且当我们乘完以后 第一项变成了所有输出值与目标值差的平方和 也就是我们一般在神经网络上
最小化的平方误差 第二项就变成了两个方差的比 乘以权重值的平方 这样你就能看到
两个方差的比值就是权重惩罚 所以我们一开始 把权重惩罚看作一个你用来
让事情变得更好的一个数 通过使用验证集
我们可以固定权重惩罚的数值 但是我们现在可以看到
如果我们使用了高斯分布的解释角度 也就是说对先验分布以及输出-目标值关系模型
两处的高斯分布假设 那么权重惩罚就是由分布的方差决定的 也就是这两个分布方差的比值 在这个框架下 其取值并不是随意的
翻译: iChen | 审核:
Coursera Global Translator Community