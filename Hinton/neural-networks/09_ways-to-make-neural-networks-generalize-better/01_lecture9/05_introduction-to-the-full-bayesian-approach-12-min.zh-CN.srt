1
00:00:00,000 --> 00:00:05,921
本视频将通过一个简单的抛硬币的例子
介绍用来拟合模型的贝叶斯方法

2
00:00:05,921 --> 00:00:10,398
如果你已经了解过贝叶斯方法

3
00:00:10,398 --> 00:00:14,915
你可以跳过这个视频
贝叶斯方法的主要思想是

4
00:00:14,915 --> 00:00:20,317
不局限于考虑模型参数最可能的取值

5
00:00:20,317 --> 00:00:25,591
我们考虑参数所有可能的取值

6
00:00:25,591 --> 00:00:30,028
并找出每个取值在给定观测数据的条件下
有多大概率取到

7
00:00:30,028 --> 00:00:33,115
贝叶斯框架假设我们对于任何事情

8
00:00:33,115 --> 00:00:36,202
都有一个先验分布

9
00:00:36,202 --> 00:00:41,283
也就是对于你任意感兴趣的事情
我都有某个先验的概率

10
00:00:41,283 --> 00:00:47,780
这个概率刻画了这个事情发生的可能性
这个先验可能比较模糊

11
00:00:49,320 --> 00:00:53,907
我们已有的数据给了我们一个似然项

12
00:00:53,907 --> 00:00:58,340
我们把它和先验分布结合起来
就可以得到一个后验分布

13
00:00:59,120 --> 00:01:05,660
似然项更青睐使得观测数据取得的概率
更高的参数

14
00:01:06,940 --> 00:01:11,602
如果我们能够得到足够的数据
这个参数可能与先验信息不太一致

15
00:01:11,602 --> 00:01:15,332
无论先验信息多么不靠谱
足够的数据都能在一定意义上纠正过来

16
00:01:15,332 --> 00:01:18,730
并且最后 有了足够的数据
真实的参数取值就会被找到

17
00:01:18,730 --> 00:01:23,725
也就说即使你的先验信息是错误的
你都有可能最终获得一个正确的假设

18
00:01:23,725 --> 00:01:29,387
但是这可能会需要海量的数据
你可能会认为这不太可能

19
00:01:29,387 --> 00:01:34,887
那么我们从一个抛硬币的例子开始

20
00:01:34,887 --> 00:01:37,973
如果你只知道硬币可以被抛

21
00:01:37,973 --> 00:01:43,609
然后抛完要么正面朝上要么反面朝上

22
00:01:43,609 --> 00:01:46,963
如果我们还假设每次抛硬币

23
00:01:46,963 --> 00:01:50,720
这些事件之间都是相互独立的

24
00:01:53,180 --> 00:01:57,100
那么我们抛硬币的模型将有一个参数P

25
00:01:57,100 --> 00:02:02,210
P刻画了硬币正面朝上的概率

26
00:02:02,210 --> 00:02:08,280
如果100次抛掷

27
00:02:08,280 --> 00:02:12,640
有53次正面朝上
那么P的取值应该是多少呢

28
00:02:13,180 --> 00:02:19,220
当然你要说P取值是0.53
那么你这么说的理由是什么呢

29
00:02:20,800 --> 00:02:26,917
从频率学派的角度讲
这叫做最大似然估计

30
00:02:26,917 --> 00:02:31,074
也就是找出使得能够得到观测数据最可能的p

31
00:02:31,074 --> 00:02:36,407
然后这个p取值就是0.53
这个结论并不显然

32
00:02:36,407 --> 00:02:40,662
我们推导一下

33
00:02:40,662 --> 00:02:47,368
得到53次正面47次反面的概率可以这样表示
每次正面朝上就乘p

34
00:02:47,368 --> 00:02:52,602
每次反面朝上就乘1-p

35
00:02:52,602 --> 00:02:58,655
如果我们把所有p的项放在一起
把所有1-p的项也放在一起

36
00:02:58,655 --> 00:03:04,320
那么就得到了p的53次方和1-p的47次方

37
00:03:04,320 --> 00:03:10,674
如果问得到观测数据的概率是怎样依赖于p的
那么我们可以对p求导

38
00:03:10,674 --> 00:03:16,620
然后得到这样一个表达式
然后我们另导函数为0

39
00:03:17,020 --> 00:03:24,256
我们发现可以发现如果想最大化这个概率
就要令P取值为0.53

40
00:03:24,256 --> 00:03:31,202
这就是极大似然方法
但是这里有个问题

41
00:03:31,202 --> 00:03:34,780
就在我们使用极大似然
来决定一个模型的参数的时候

42
00:03:35,640 --> 00:03:39,743
例如我们只抛掷一次硬币
然后我们得到了正面朝上

43
00:03:39,743 --> 00:03:44,643
但是我们认为硬币正面朝上的概率

44
00:03:44,643 --> 00:03:48,746
取值为1这显然是不合理的
不然这就意味着我们也认可了

45
00:03:48,746 --> 00:03:51,747
硬币抛掷无数次都不可能出现反面朝上的情况

46
00:03:51,747 --> 00:03:58,603
这看起来也太荒谬了
而且很直观的一个更好的猜测

47
00:03:58,603 --> 00:04:02,920
就是p取值为0.5
但是我们要如何验证这一点呢

48
00:04:02,920 --> 00:04:08,240
更重要的是 我们可以问
只求得一个答案这件事情本身是合理的吗

49
00:04:10,280 --> 00:04:14,147
我们知道的不多
我们没有足够多的数据

50
00:04:14,147 --> 00:04:19,604
所以我们也不能知道P的取值是多少
所以我们真正应该做的是

51
00:04:19,604 --> 00:04:24,991
拒绝得到一个单一的答案
而是应该去得到一个概率分布

52
00:04:24,991 --> 00:04:28,651
这个分布包含了所有可能的答案
一个像0.5这样的答案是很有可能的

53
00:04:28,651 --> 00:04:34,453
一个像1这样的答案是不太可能的

54
00:04:34,453 --> 00:04:43,722
如果我们先验地认为有一半的次数会正面朝上
现在我们要开始一个例子

55
00:04:43,722 --> 00:04:47,992
我们从对参数的所有取值有一个先验分布出发

56
00:04:47,992 --> 00:04:51,469
这里我们选择一个好处理的先验分布

57
00:04:51,469 --> 00:04:55,556
不一定是一个完全符合我们先验认知的分布

58
00:04:55,556 --> 00:05:00,131
然后我们会看到这个先验分布
将会被数据修正

59
00:05:00,131 --> 00:05:04,929
如果我们采取贝叶斯方法的话
那么我们将从一个

60
00:05:04,929 --> 00:05:09,016
所有p的取值概率都相同的分布开始

61
00:05:09,016 --> 00:05:12,024
我们相信抛掷硬币存在偏差

62
00:05:12,024 --> 00:05:16,621
或多或少的偏差都有可能存在
所以一些硬币一半的次数正面朝上

63
00:05:16,621 --> 00:05:19,515
另一些硬币则是一直正面朝上

64
00:05:19,515 --> 00:05:22,240
这两种硬币同等可能存在

65
00:05:22,520 --> 00:05:29,058
我们现在观测到硬币正面朝上
那么我们要做的就是

66
00:05:29,058 --> 00:05:36,010
对任意可能的取值p 找到其先验概率

67
00:05:36,010 --> 00:05:41,970
然后乘以给定该取值时观测到正面朝上的概率

68
00:05:41,970 --> 00:05:47,464
例如我们取P为1 则每次硬币都正面朝上

69
00:05:47,464 --> 00:05:51,984
也就是观测到正面的概率是1

70
00:05:51,984 --> 00:05:56,922
这时就不会出现其他可能
并且如果我们取P为0

71
00:05:56,922 --> 00:06:00,400
那么观测到硬币正面朝上的
概率就是0

72
00:06:00,400 --> 00:06:05,048
如果我们取P为0.5
那么观测到正面朝上的概率就是0.5

73
00:06:05,048 --> 00:06:10,311
所以我们取一条红线 当作我们的先验分布
然后给每个点

74
00:06:10,311 --> 00:06:14,959
乘以基于这个假设下观测到正面朝上的概率

75
00:06:14,959 --> 00:06:19,334
然后我们就得到了一个倾斜的
未经归一化的后验分布

76
00:06:19,334 --> 00:06:24,324
称其未归一化是因为直线下方面积不是1

77
00:06:24,324 --> 00:06:29,382
然而对任意的概率分布
其所有可能事件的概率加起来应该是1

78
00:06:29,382 --> 00:06:35,065
所以最后一步就是对后验分布归一化

79
00:06:35,065 --> 00:06:38,458
我们按比例整理所有概率之后

80
00:06:38,458 --> 00:06:43,066
曲线下方的面积就是1了
如果我们从均匀分布开始

81
00:06:43,066 --> 00:06:49,583
就会得到一个三角形的分布

82
00:06:49,583 --> 00:06:54,201
这是经过观测一边得到的

83
00:06:54,201 --> 00:07:01,483
让我们再来一次
这次我们假设得到反面朝上

84
00:07:01,483 --> 00:07:07,189
那么这时候我们的先验分布
就是刚才观测到正面朝上之后的后验分布

85
00:07:07,189 --> 00:07:12,301
图中的绿色直线表示了

86
00:07:12,301 --> 00:07:17,488
和P的取值相对应的假设下
我们得到硬币反面朝上的概率

87
00:07:17,488 --> 00:07:22,749
例如如果P是1
那么我们能观测到反面朝上的概率就是0

88
00:07:22,749 --> 00:07:27,294
我们再次用先验乘以似然

89
00:07:27,294 --> 00:07:31,797
然后就得到了这样一个曲线

90
00:07:31,797 --> 00:07:35,744
此外我们还要再次归一化后验分布
使得曲线下方的面积是1

91
00:07:35,744 --> 00:07:41,627
那么这就是我们目前为止的后验分布了
这是在观测到一次正面一次反面后得到的

92
00:07:41,627 --> 00:07:44,501
注意到这个分布是比较敏感的

93
00:07:44,501 --> 00:07:48,182
在一次正面一次反面的观测后

94
00:07:48,182 --> 00:07:53,839
P的取值既不是0也不是1
而且最可能的取值是在中间

95
00:07:53,839 --> 00:08:00,499
如果我们持续做完剩下98次

96
00:08:00,499 --> 00:08:06,427
并且持续使用同样的公式
也就是用上次试验后得到的后验分布作为

97
00:08:06,427 --> 00:08:12,007
本次的先验分布
然后乘以P取不同值确定的似然函数

98
00:08:12,007 --> 00:08:17,608
如果我们最后得到53次正面朝上
47次反面朝上

99
00:08:17,608 --> 00:08:21,509
那最后得到的曲线就是这样的

100
00:08:21,509 --> 00:08:24,488
这个分布的均值就是0.53

101
00:08:24,488 --> 00:08:30,802
因为我们是从均匀分布开始的
所以最后曲线在0.53处有一个尖锐的峰值

102
00:08:30,802 --> 00:08:36,902
但是同时也允许像0.49这样的值
也有一个比较合理的取值可能性

103
00:08:36,902 --> 00:08:40,307
虽然不像0.53这样概率高
但也比较合理

104
00:08:40,307 --> 00:08:43,995
但是像0.25这样的取值

105
00:08:43,995 --> 00:08:51,093
在这个去曲线下就十分不可能了
我们可以用贝叶斯定理总结

106
00:08:51,093 --> 00:08:56,177
等式中间这一项

107
00:08:56,177 --> 00:09:00,844
是关于参数W和数据D的联合分布

108
00:09:00,844 --> 00:09:06,400
对于监督式学习
数据将包括模型的输出值

109
00:09:06,400 --> 00:09:12,475
所以我们这里假设
输入值和输入值分别对应的输出值

110
00:09:12,475 --> 00:09:16,920
都是给定的
这也是我们观测到的数据

111
00:09:17,580 --> 00:09:22,804
这个联合分布还可以重写为

112
00:09:22,804 --> 00:09:28,744
关于单一变量的概率和条件概率的乘积
在右边写为W的概率乘以给定W下D的概率

113
00:09:28,744 --> 00:09:34,327
左边写为W的概率乘以给定W下D的概率

114
00:09:34,327 --> 00:09:39,960
在左右两边同时除以D的概率

115
00:09:40,220 --> 00:09:43,640
这就得到了贝叶斯公式的一般形式

116
00:09:44,200 --> 00:09:50,431
贝叶斯公式说明了特定取值的W
其给定D的后验概率

117
00:09:50,431 --> 00:09:56,745
就是取值W的概率再乘以

118
00:09:56,745 --> 00:10:02,894
给定W产生D的概率

119
00:10:02,894 --> 00:10:08,716
并且这个概率要被D的概率归一化处理

120
00:10:08,716 --> 00:10:14,866
数据D的概率可以通过对所有可能的W积分求得

121
00:10:14,866 --> 00:10:21,083
被积函数是W的概率乘以给定W下D的概率

122
00:10:21,083 --> 00:10:25,563
线下项的数值应该是
线上项所有W决定的数值的和

123
00:10:25,563 --> 00:10:30,764
因为所有的概率加起来要是1
因为D的概率计算的时候对所有W进行了积分

124
00:10:30,764 --> 00:10:36,168
所以这一项就与W具体的取值无关

125
00:10:36,168 --> 00:10:40,790
所以当我们想求一个W使得后验概率最大

126
00:10:40,790 --> 00:10:45,127
我们就可以忽略D的概率这一项
因为这一项和W无关

127
00:10:45,127 --> 00:10:49,821
然而等式右边的其他两项都是依赖于W的
翻译: iChen | 审核:
Coursera Global Translator Community