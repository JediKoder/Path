在这个视频中 我会介绍机器学习的三种类别 监督学习 强化学习和无监督学习 大体而言 课程前半部分将会讨论监督学习 后一半主要讨论非监督学习 这门课将不会涉及强化学习 因为我们不可能面面俱到 机器学习可以被粗略地分为三类 在监督学习中 你试图依据给定的输入向量来预测输出结果 因此监督学习的特点很明显 强化学习中 你试图 选择一系列行为或行为序列来使得获得奖励最大化 而且这个奖励可能不定时产生的 
在无监督学习中 你试图 找到对于输入数据的好的内在表达
我们将在后面对此详细解释 监督学习又可以被分为两类 回归和分类 回归问题中 目标输出是一个实数或者一个实数向量 比如六个月以后的股价 或者明天中午的气温 总之回归问题的目标是 让输出
尽可能地接近正确的实数或实数向量 分类问题中 目标输出是一个分类标签 最简单的例子就是在零和一之间二选一 也就是在阳性和阴性之间给出一个选择 当然 我们也可以有多个备选分类 比如手写数字识别问题 在监督学习中 我们必须在一开始找到一个模型集合 里边是一系列的备选模型 你也可以认为它是一个函数 根据输入向量 x 和相关的参数 W  能够得到相应的输出 y 所以模型集合可以被认为是一种映射 从输入到输出的映射，使用了数值参数W， 我们对参数进行调整，使这种映射关系能拟合
相应的训练数据。 拟合，在这里指最小化每一条训练数据的目标输出和 机器学习算法实际输出的差异。 回归问题中，差异的度量， 常使用的是我们系统的输出y和正确输出t的差异的平方
再乘以二分之一 这个系数（二分之一）可以在求导的时候被消掉。 对分类问题， 我们也可以使用这个度量，
但是存在更加敏感的度量方式， 其更有效，我们课程后面会详细描述。 强化学习，输出是一系列动作 你需要根据不定时的奖励来决定应该如何选择。 所以目标是选择每个动作，使得将来的奖励最大化。 一般我们会使用一个衰减因子，
这样我们就不用计算到太远的将来。 换言之，很久以后才获得的奖励 计算时的权重要低于马上就要获得的奖励 强化学习很难，这是因为一般而言奖励是滞后的 所以很难确定一系列动作到底那个是错误的。 强化学习很难，因为奖励是数值， 是不定时给出的标量，起没有没有提供很多信息 用以构建相应的参数。 一般而言，强化学习中你没法学习出上百万的参数， 但是在监督学习和非监督学习中，是可以的。 典型的强化学习，你试图学习 数十个或者一千个参数，但是不会是一百万个。 这个课程不会覆盖一切， 所以虽然强化学习很重要，但是我们不会涉及。 课程后半部分我们会讨论非监督学习。 长达40年的时间里 机器学习学界
基本上忽视了非监督学习 除了聚类问题这一种非常有限的形式 实际上 他们对机器学习的定义
就排除了非监督学习 在某些教科书上，
机器学习被定义为一种从输入到输出的映射 很多机器学习研究人员认为 聚类是唯一的一种非监督学习 原因之一是因为很难定义非监督学习的目的 其中一个主要目的 是得到输入数据的内在表达，从而在接下来的 监督学习或者强化学习中排上用场。 分成两个阶段，以强化学习为例，
是因为我们不想使用奖励来 得到我们视觉系统的参数。 你可以通过计算双眼看到的图像的差异来计算 到某个表面距离。
你肯定不会通过 不停的调整你自己视觉系统的相关参数， 从而计算出相应的距离。 真这样做，
会挪很多次， 通过学习融合输入的两张图片提供的信息
无疑是更好的方式。 非监督学习的另一个目标 是为输入数据提供一个紧凑，低维的表达。 高维数据，比如图片，
往往存在或者近似与 一个低维流形。
或者多个这样的流形，比如手写数字这个例子。 具体的说，
你有一张一百万像素的图片， 但是并不存在一百万维的自由度。 可能仅仅只有几百个维度的自由度。 所以我们要做的是 从百万像素 依据我们在流形相关信息， 我们还需要知道我们在那个流形上。 一个很基础的方式就是主成分分析，其是线性的。 其假设存在一个流形，
其是高维空间的一个平面。 非监督学习的另一个定义， 或者另一个目标， 就是把输入转换为的表达。 比如，我们可以将输入表达为binary特征， 这样很经济，
因为每个变量只需要一个比特来说明其状态。 或者我们可以是用大量的实数变量， 但是限制输入，使得几乎所有的变量都是0。 这种情况下，对每个输入， 我们仅仅要表达少数实数，这也是经济划算的。 前面提到过 非监督学习的另一个定义 或者说它的另一个目标 是对输入进行聚类 聚类也可以被认为是非常稀疏的编码，
对于每一个聚类我们有一个变量， 那么对于所有变量来说，
除了一个，其他取值都为0，这一个取值为1。 所以聚类是稀疏编码的 一种极端形式。