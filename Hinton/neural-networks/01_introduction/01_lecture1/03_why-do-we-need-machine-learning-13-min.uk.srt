1
00:00:00,000 --> 00:00:03,075
Привіт! Ласкаво прошу до Курсерівського курсу з Нейронних

2
00:00:03,075 --> 00:00:09,006
мереж для машинного навчання. Перед тим як ми зануримось в деталі алгоритмів

3
00:00:09,006 --> 00:00:14,004
навчання нейронних мереж, я хочу ще трохи поговорити про машинне навчання

4
00:00:14,004 --> 00:00:19,015
чому ми потребуємо машинне навчання, типи задач, для яких ми можемо їх застосувати, і покажемо деякі

5
00:00:19,015 --> 00:00:23,087
приклади того, що вони можуть робити. Причина, через яку ми потребуємо машинне навчання

6
00:00:23,087 --> 00:00:29,010
це комплекс проблем, для яких дужеважко писати програми, розпізнавання трьох

7
00:00:29,010 --> 00:00:33,059
вимірних об'єктів наприклад. Коли це відбувається з додаванням  нових кутів зору і

8
00:00:33,059 --> 00:00:37,026
освітлення в захаращенному зображенні, це дуже важко зробити

9
00:00:37,026 --> 00:00:42,018
Ми не знаємо яку писати програму,  тому що ми не знаємо як це відбувається в нашому

10
00:00:42,018 --> 00:00:45,005
мозку. И навіть якщо б ми знали яку програму

11
00:00:45,005 --> 00:00:49,010
писати, це могла б бути страшенно складна програма

12
00:00:50,029 --> 00:00:55,083
Інший приклад це визначення шахрайських транзакцій з кредитних карток, де не може

13
00:00:55,083 --> 00:01:00,014
бути гарних, простих правил які б вказували на те, що вона шахрайська

14
00:01:00,014 --> 00:01:05,014
Насправді, потрібно комбінувати значну кількість не дуже надійних правил

15
00:01:05,014 --> 00:01:10,060
І також, ці правила кожен раз змінюються, бо люди змінюють трюки, які вони використовують

16
00:01:10,060 --> 00:01:13,084
для шахрайства. Таким чином, ми потребуємо складну програму, що

17
00:01:13,084 --> 00:01:17,062
комбінує ненадійні правила, які и можемо легко міняти.

18
00:01:18,087 --> 00:01:24,027
Підхід машинного навчання полягає в тому, що скажімо, замість написання кожної програми вручну

19
00:01:24,027 --> 00:01:29,040
для кожної специфічної задачі, ми збираємо багато зразкві і

20
00:01:29,040 --> 00:01:32,029
визначаємо правильний результат для вхідних даних

21
00:01:32,062 --> 00:01:37,080
Алгоритм машинного навчання потім бере ці приклади і створює програму, яка

22
00:01:37,080 --> 00:01:41,029
виконує роботу. Програма створена

23
00:01:41,029 --> 00:01:45,035
алгоритмом що навчається,  може дуже відрізнятися від типової програми написаної вручну.

24
00:01:45,035 --> 00:01:49,093
Наприклад, вона може містити мілліони значень про те як ви оцінюєте різні

25
00:01:49,093 --> 00:01:54,014
види ознак. Якщо ми робимо це правильно, програма буде працювати

26
00:01:54,014 --> 00:01:57,004
для нових випадкві так само добре як і для тих на яких вона натренована

27
00:01:57,051 --> 00:02:03,047
І якщо дані зміняться, ми будемо мати можливість змінити хід виконання програми дуже просто

28
00:02:03,047 --> 00:02:09,627
перетренувавши її на нових даних. І зараз великі обсяги обчислень

29
00:02:09,627 --> 00:02:14,084
дешевше ніж плата кому-небудь за написання прграми для визначеної задачі, таким чином ми можемо

30
00:02:14,084 --> 00:02:20,000
дозволити собі значно складніші программи машинного навчання для виконання цих важких (абсолютних?) задач

31
00:02:20,000 --> 00:02:26,023
специфічних систем для нас. Деякі приклади таких задач, які краще

32
00:02:26,023 --> 00:02:32,050
розв'язуються використанням алгоритмів навчання, це розпізнавання зразків, так, наприклад

33
00:02:32,050 --> 00:02:38,095
об'єкти в реальних сценах або ідентичність чи вираз обличчя людини,

34
00:02:38,095 --> 00:02:42,053
або усна мова. Також це визначення аномалій

35
00:02:42,053 --> 00:02:46,084
Так, незвичайна послідовність транзацій кредитної картки може бути аномальною.

36
00:02:47,002 --> 00:02:51,098
Іншим прикладом аномальності можна назвати незвичайний зразок зчитування(даних) сенсора

37
00:02:51,098 --> 00:02:55,062
атомної станції. І ви насправді нехочете

38
00:02:55,062 --> 00:02:58,034
мати справу робити це з навчанням з учителем(навчання під керівництвом на зразках)

39
00:02:58,034 --> 00:03:03,025
Коли ви дивитесь на те що вибухнуло, и бачите яка причина викликала цей вибух

40
00:03:03,025 --> 00:03:07,067
Ви дійсно бажали б розпізнати, що щось дивне з'явилося без

41
00:03:07,067 --> 00:03:11,097
отримання підтверджуючого сигналу. Цього просто не станеться в нормальному випадку

42
00:03:12,059 --> 00:03:16,047
І ще  - це прогнозування. Так, типово прогнозують ціни фьючерсів

43
00:03:16,047 --> 00:03:21,333
або обмінного курсу валют або прогнозують який фільм сподобається особі

44
00:03:21,333 --> 00:03:25,812
знаючи які інші фільми він полюбляє. І які фільми багато інших людей

45
00:03:25,812 --> 00:03:31,226
полюбляє. Так, в цьому курсі я вважатиму  стандартним

46
00:03:31,226 --> 00:03:36,306
зразком для пояснення багатьох алгоритмів машинного навчання.

47
00:03:36,306 --> 00:03:41,669
Це зроблено в багатьох науках. В генетиці наприклад, багато генетичних (досліджень)

48
00:03:41,669 --> 00:03:45,809
зроблено на фруктових мухах(дрозофілах?) І причина в тому, що вони зручні.

49
00:03:45,809 --> 00:03:51,760
Вони швидко розмножуються і багато вже відомо про генетику плодових мушок.

50
00:03:51,760 --> 00:03:58,840
База даних MNIST рукописних символів це машиний еквівалент фруктових мух.

51
00:03:58,840 --> 00:04:04,573
Вони публічно доступні. Ми можемо застосувати машинні алгоритми навчання

52
00:04:04,573 --> 00:04:09,769
для тренування розпізнавання цих рукописних символів досатньо швидко, так що легко спробувати

53
00:04:09,769 --> 00:04:13,500
багато варіацій. І ми знаємо значну кількість про те як добре

54
00:04:13,500 --> 00:04:16,425
різні машинні алгоритми працюють на MNIST.

55
00:04:16,425 --> 00:04:21,036
І  зокрема, різні методи машинного навчання були реалізовані

56
00:04:21,036 --> 00:04:24,492
людьми що вірили в них, тому ми можемо покладатися на цы результати.

57
00:04:24,492 --> 00:04:29,395
Так, з усіх цих причин ми збираємся використовувати MNIST як нашу стандартну задачу

58
00:04:29,395 --> 00:04:33,499
Ось приклад деяких цифр MNIST

59
00:04:33,499 --> 00:04:38,566
Це ті, що коррекно були розпізнані нейронною мережею, яка перший раз

60
00:04:38,566 --> 00:04:42,958
побачила їх. Але це ті, щодо яких нейронна мережа не була

61
00:04:42,958 --> 00:04:45,819
дуже впевнена. І ви зможете побачити чому.

62
00:04:45,819 --> 00:04:50,205
Я розташував ці цифри в стандартному відсканованому порядку

63
00:04:50,205 --> 00:04:57,163
Так нулі, потім одноці, потім двійки і так далі. Якщо ви поглянете на кілька елементів як

64
00:04:57,163 --> 00:05:02,025
ті що в зеденому квадраті. Ви можете побачити, що якщо ви знаєте що було 100

65
00:05:02,025 --> 00:05:04,086
в цифрах, ви ймовірно здогадаєтесь, це були двійки.

66
00:05:04,086 --> 00:05:08,038
Але це дуже важко сказати що робить їх двійками.

67
00:05:08,038 --> 00:05:11,046
Тут нема нічого простого, що вони всі б мали взагалі.

68
00:05:11,046 --> 00:05:16,019
Зокрема, якщо ви спробуєте накласти одну на іншу, ви побачите, що вони не співпадають.

69
00:05:16,019 --> 00:05:21,021
І навіть якщо ви їх трохи повернете, дуже важко зробити їх перекриваючими.

70
00:05:21,021 --> 00:05:25,087
Так, визначення зразка не допоможе. Зокрема зразок який

71
00:05:25,087 --> 00:05:30,090
дуже важко знайти покриє двійки в зеленому прямокутнику і також покривав би

72
00:05:30,090 --> 00:05:35,074
зразки в червоному прямокутнику. Так, це та річ, що робить розпізнавання

73
00:05:35,074 --> 00:05:38,075
рукописних исел гарною задачею для машинного навчання.

74
00:05:39,062 --> 00:05:43,076
Зараз, я не хочу щоб ви думали, що це лише те що мим можемо робити.

75
00:05:43,096 --> 00:05:48,043
Це відносно просто для нашої системи машинного навчання зараз.

76
00:05:48,043 --> 00:05:53,078
І щоб  мотивувати іншу частину курсу, я хочу показати деякі зразки

77
00:05:53,078 --> 00:05:57,039
більш складних речей. Так, зараз ми маємо нейронні мережі що

78
00:05:57,059 --> 00:06:02,087
пропонують сотні мільїонів параметрів в них, що можуть розпізнавати тисячі

79
00:06:02,087 --> 00:06:08,028
різних класів об'єктів з 1.3 мілліона тренувальних зображень високої якості, що були отримані з

80
00:06:08,028 --> 00:06:12,006
вебу. Так, в 2010 році проводилося змагання

81
00:06:12,006 --> 00:06:17,001
найкраща система отримала 47 відсотків рівня помилки якщо дивитися на перший вибір і 25

82
00:06:17,001 --> 00:06:21,089
відсотків помилки якщо вважати що вона була права, якщо правильна відповідь була серед перших 5 запропонованих,

83
00:06:21,089 --> 00:06:24,087
що не так погано для 1000 різних об'єктів.

84
00:06:25,008 --> 00:06:30,070
Жітендра Малік,  відомий скептик нейронних мереж і лідуючий дослідник комп'ютерного зору

85
00:06:30,070 --> 00:06:36,046
сказав, що ці змагання  - гарний тест чи зможуть глибокі нейронні

86
00:06:36,046 --> 00:06:39,066
мережі працювати добре для розпізнавання образів.

87
00:06:39,066 --> 00:06:44,068
І дуже глибока нейрона мережа може зараз зробити значно більше ніж

88
00:06:44,068 --> 00:06:48,000
виграти змагання. Вона може взяти менш ніж 40 відстоків помилки, для

89
00:06:48,000 --> 00:06:52,023
першого вибору, і менш ніж 20 відсотків для перших п'яти

90
00:06:52,023 --> 00:06:55,060
Я опишу значно детальніше в лекції п'ять

91
00:06:55,060 --> 00:06:59,065
Ось деякі приклади видів зображень які необхідно розпізнати.

92
00:06:59,065 --> 00:07:03,026
Ці зображення з тестового набору які вона(Н.М.) раніше не бачила

93
00:07:03,026 --> 00:07:08,062
І під зразком, я показую що нейронна мережа вважає правильною

94
00:07:08,062 --> 00:07:12,030
відповіддю. Де довжина горизонтальної полоси це

95
00:07:12,030 --> 00:07:16,006
наскільки впевненою вона була, а правильна відповідь - червона.

96
00:07:16,006 --> 00:07:20,061
Так, якщо ви поглянете, посередині, воно корректно ідентифіковано як снігоочисна машина

97
00:07:20,061 --> 00:07:23,086
Але ти можеш бачити, що їх інші вибори достатньо чутливі

98
00:07:23,086 --> 00:07:26,067
Воно дійсно виглядає трохи схожою на бурову платформу

99
00:07:26,067 --> 00:07:30,091
І якщо ви подивитесь на третій вибір, рятувальну лодку, він дійсно виглядає дуже схоже на

100
00:07:30,091 --> 00:07:33,067
рятувальну лодку. Ви можете бачити прапор носі

101
00:07:33,067 --> 00:07:38,018
лодки і (капітанський) місток і флаг на кормі, і високу хвилю

102
00:07:38,018 --> 00:07:41,011
на фоні. Таким чином, її помилки говорять багато про

103
00:07:41,011 --> 00:07:43,097
те як все відбувається і це дуже правдоподібні помилки

104
00:07:43,097 --> 00:07:48,049
На малюнку лівооруч - результат неправильний. Він неправильний, ймовірно тому, що клюв птаха

105
00:07:48,049 --> 00:07:52,475
пропущений і тому що пір'я птаха виглядають дуже схожими на моке хутро видри

106
00:07:52,475 --> 00:07:56,027
Але вона знайшло її в перших п'яти і вона зробила це краще за мене

107
00:07:56,027 --> 00:07:59,853
Я б не знав, чи це був перепел, або куріпка, або грівчаста куріпка

108
00:07:59,853 --> 00:08:03,214
Якщо ви подивитесь на правий малюнок - відповідь повністю неправильна

109
00:08:03,214 --> 00:08:07,827
Це гільйотина. Ви можете сказати чому вона так відповіла. Ви можливо бачите чому она сказала що це

110
00:08:07,827 --> 00:08:12,430
орангутанг, томущо аздній фон виглядає як джунглі і щось помаранчеве

111
00:08:12,430 --> 00:08:15,449
в середині. Але вона не змогла дати правильну відповідь.

112
00:08:15,449 --> 00:08:19,286
Але вона може, наразі, мати справу з широким переліком різних об'єктів

113
00:08:19,286 --> 00:08:23,888
Якщо ви подивитесь ліворуч, я б сказав що це мікрохвильова піч на перший погляд

114
00:08:23,888 --> 00:08:28,225
Мітки не дуже систематичні. Але насправді, правильна відповідь

115
00:08:28,225 --> 00:08:30,955
електрична плита. І вона повернула це в перших п'яти

116
00:08:30,955 --> 00:08:34,822
Посередині, вона отримала турнікет, який є розподіленим об'єктом

117
00:08:34,822 --> 00:08:38,661
Вона змогла, вона може більше ніж розпізнавати компактні об'єкти

118
00:08:38,661 --> 00:08:43,699
Також, вона може мати справу з малюнками так само гарно як і з реальними сценами, як наприклад бронежилет

119
00:08:43,699 --> 00:08:46,959
І вона робить деякі цікаві(класні?) помилки.

120
00:08:46,959 --> 00:08:49,976
Якщо ви подивитесь ліворуч - то це навушники

121
00:08:49,976 --> 00:08:54,101
Вона не видала нічого схожого на навушники. Але якщо ви подивитесь четверту відповідь,

122
00:08:54,101 --> 00:08:57,316
вона думає що це мураха. І для вас це виглядає божевільним.

123
00:08:57,316 --> 00:09:01,581
Але потім, якщо ви подивитесь на неї акуратно, ви зможете побачити, це вигляд мурахи з

124
00:09:01,581 --> 00:09:04,350
низу. Очі дивляться знизу знизу на вас і ви

125
00:09:04,350 --> 00:09:08,698
можете бачити антени позаду. Це не той вигляд мурахи, який би ви хотіли

126
00:09:08,698 --> 00:09:12,777
мати якби були зеленою мухою. Якщо ви подивитесь на правий малюнок

127
00:09:12,777 --> 00:09:16,547
там не має правильної відповіді. Але всі відповіді циліндричні

128
00:09:16,547 --> 00:09:22,002
об'єкти. Інше завдання в якому нейронні мережі зараз дуже

129
00:09:22,002 --> 00:09:27,441
добрі, це розпізнавання мови. Або як мінімум частина системи розпізнавання мови.

130
00:09:27,441 --> 00:09:30,643
Так, системи розпізнавання мови мають декілька

131
00:09:30,643 --> 00:09:34,051
стадій. Поперше  - попередня обробка звукової хвилі,

132
00:09:34,051 --> 00:09:39,916
щоб отримати вектор акустичних коефіцієнтів, для кожних 10 мілісекунд звукової хвилі.

133
00:09:39,916 --> 00:09:43,638
І так вони отримують 100 цих векторів за секунду

134
00:09:43,638 --> 00:09:49,418
Потім вони беруть декілька сусідніх векторів акусичних коефіцієнтів і вони потребують

135
00:09:49,418 --> 00:09:52,965
визначити ваги для яких частин яких фонем вони були вимовленію

136
00:09:52,965 --> 00:09:57,894
Так, вони дивляться на це маленьке віконечко і вони кажуть всередині цього вікна

137
00:09:57,894 --> 00:10:01,889
що я думаю, яка це фонема є, і яка це частина цієї фонеми

138
00:10:01,889 --> 00:10:06,507
І гарна система розпізнавання мови буде мати значну кількість альтернативних моделей для

139
00:10:06,507 --> 00:10:09,131
фонеми і кожна модель повинна мати три

140
00:10:09,131 --> 00:10:12,341
різних частини. Так, вона може мати тисячі

141
00:10:12,341 --> 00:10:15,609
альтернативних фрагментів, що вона думає можуть бути.

142
00:10:15,609 --> 00:10:20,075
І ви маєте поставити ваги на всі ці тисячі альтернатив

143
00:10:20,075 --> 00:10:26,171
І потім, як тільки ви виставите ці ваги, ви отримуєту стадію декодування, що робить кращу

144
00:10:26,171 --> 00:10:32,211
роботу, вона може використовуючи правдоподібні ваги, але складаючи їх раом в послідовність

145
00:10:32,211 --> 00:10:37,641
ваг, що відповідають тому набору, що люди вимовляють

146
00:10:37,641 --> 00:10:44,094
Наразі, глибока нейрона мережа, відкрита джорджом Дахлом і Абель-рахманом Мохаммедом

147
00:10:44,094 --> 00:10:48,410
з університету Торонто, виконує це кращі ніж попередні методи машинного

148
00:10:48,410 --> 00:10:52,783
навчання для акустичних моделей, і вони зараз починають впроваджуватися  в

149
00:10:52,783 --> 00:10:58,529
практичні системи. Так, Дахл і Мохаммед розробили систему

150
00:10:58,529 --> 00:11:05,214
що використовує багато шарів бінарних нейронів, для того щоб зробити акустичні фрейми і проставити

151
00:11:05,214 --> 00:11:09,986
ваги для міток. вони зробили достатньо малу

152
00:11:09,986 --> 00:11:13,656
базу даних і потім використовували 183 альтернативні мітки

153
00:11:13,656 --> 00:11:20,094
І для того, щоб їх система працювала добре, вони зробили деяке попереднє тренування, яке буде

154
00:11:20,094 --> 00:11:23,825
описано в другій частині нашого курсу.

155
00:11:23,825 --> 00:11:30,471
Після стандартного постпроцессінгу, вони отримали 20.7 відсотковий рівень помилок на дуже стандартному

156
00:11:30,471 --> 00:11:34,154
тесті, який є чимось накшталт MNIST для мови.

157
00:11:34,154 --> 00:11:39,704
Найкращий попередній результат на цьому тесті був

158
00:11:39,704 --> 00:11:43,467
24.4%. І дуже досвідчені мовні дослідники

159
00:11:43,467 --> 00:11:49,369
в Майкрософт ресерч зрозуміли це, це було значне вдосконалення, що

160
00:11:49,369 --> 00:11:54,698
ймовірно це може змінити шлях яким системи розпізнавання речі будуть створені.

161
00:11:54,698 --> 00:11:58,951
І дійсног, так і є. Так, якщо ви переглянете свіжі результати з

162
00:11:58,951 --> 00:12:04,811
декількох різних розповсюджених мовних груп, Майкросоіт показало, що варіант глибоких

163
00:12:04,811 --> 00:12:09,651
нейронних мереж, коли використовується як акустична модель в ситемі розпізнавання мови

164
00:12:09,651 --> 00:12:14,927
зменшує рівень помилок з 27.4 відсотків до 18.5% або льтернативно, ви можете побачити

165
00:12:14,927 --> 00:12:21,018
вона зменшує кількість тренувальних даних, які ви потребуєте з 2000 годин до 309

166
00:12:21,018 --> 00:12:26,814
годин для отримання порівнюваного результату. АйБіЕм, що мав найкращу систему для одного з

167
00:12:26,814 --> 00:12:33,058
стандартного завдання розпізнавання мови для великої системи відновлювального розпізнавання мови показало

168
00:12:33,058 --> 00:12:38,297
що навіть дуже сильно підлаштовані системи, які давали 18.8 відстоків можуть бути

169
00:12:38,297 --> 00:12:41,613
переможені однією з цих глибоких нейронних мереж.

170
00:12:41,613 --> 00:12:46,768
І Гугль, достатньо свіжий, натренував глибоку нейронну мережу на великій кількості

171
00:12:46,768 --> 00:12:51,301
мови 5800 годин. Це все одно було менше ніж вони тренували

172
00:12:51,301 --> 00:12:55,769
їх змішану модель. Але навіть із значно меншим обсягом даних вона працює

173
00:12:55,769 --> 00:12:58,708
значно краще ніж технологія, яку вони мали дотепер.

174
00:12:58,708 --> 00:13:03,291
так воно зменшиє рівень помилки з 16% до 12.3% і рівень помилки

175
00:13:03,291 --> 00:13:07,284
досі падає. І в останньому Андроїді, якщо ви виберете голосовий

176
00:13:07,284 --> 00:13:12,770
пошук, він скористається однією з глибоких нейронних мереж для того щоб зробити дуже

177
00:13:12,770 --> 00:13:14,017
гарне розпізнавання мовлення.