1
00:00:00,176 --> 00:00:03,949
В цьому відео я збираюсь показати вам

2
00:00:03,949 --> 00:00:06,663
приклад машинного навчання

3
00:00:06,663 --> 00:00:08,756
Це дуже простий вид нейронних мереж

4
00:00:08,756 --> 00:00:12,672
і він буде навчений розпізнаванню цифр

5
00:00:12,672 --> 00:00:15,029
І ви зможете побачити як ваги еволіюціонують

6
00:00:15,082 --> 00:00:18,135
так як ми запустили дуже простий алгоритм навчання

7
00:00:18,711 --> 00:00:20,326
Так, ми збираємся подивитися дуже

8
00:00:20,326 --> 00:00:24,250
простий алгоритм навчання дуже простої мережі

9
00:00:24,250 --> 00:00:29,273
для розпізнавання рукописних знаків. Мережа  має два шари нейронів

10
00:00:29,588 --> 00:00:31,646
Вона має вхідні нейрони

11
00:00:31,646 --> 00:00:38,033
чия активність показує інтенсивність пікселів і вихідних нейронів, чия активність репрезетує

12
00:00:38,033 --> 00:00:39,010
класи

13
00:00:41,857 --> 00:00:43,587
Це буде схоже на те, коли ми показуємо

14
00:00:43,587 --> 00:00:49,611
конкретний символ, вихідний нейрон для цього символу активується

15
00:00:51,287 --> 00:00:53,863
Якщо піксел активний,

16
00:00:53,863 --> 00:00:57,096
він голосує за конкретний символ

17
00:00:57,096 --> 00:01:00,889
Формально, символи складаються з символів

18
00:01:00,889 --> 00:01:04,031
Кожен піксел може голосувати за декілька символів

19
00:01:04,031 --> 00:01:09,803
і голоси будуть мати різну інтенсивність, символ який отримає більше голосів - переможе

20
00:01:09,803 --> 00:01:10,794
Так, ми вважаємо

21
00:01:10,794 --> 00:01:12,960
що це буде змагання між вхідними елементами

22
00:01:12,960 --> 00:01:14,772
і ще дещо, що я ще досі не пояснив.

23
00:01:14,772 --> 00:01:17,951
буду пояснювати пізніше в лекціях

24
00:01:20,028 --> 00:01:24,297
Перш за все, нам необхідно вирішити як відображати ваги. І

25
00:01:24,297 --> 00:01:29,385
природно виглядає писати ваги на з'єднаннях між

26
00:01:29,385 --> 00:01:34,809
вхідними елементами та вихідними. Але ми не будемо мати можливості що відбувається, якщо ми це отримаємо.

27
00:01:35,255 --> 00:01:39,309
Нам необхідно відображати так, щоб ми могли бачити значення тисяч ваг.

28
00:01:39,447 --> 00:01:45,196
Ідея для кожного виходу ми зробимо маленьку карту. І на цій карті ми покажемо

29
00:01:46,180 --> 00:01:53,032
силу з'єднання, що приходить з кожного вхідного піксела з місця розташування вхідного піксела.

30
00:01:53,324 --> 00:01:59,375
І ми покажемо силу з'єднання використовуючи чорно-білі ділянки, чия площа буде показувати магнітуду.

31
00:01:59,375 --> 00:02:09,822
і чий знак буде представлятися кольором. Так, початкові ваги що ви бачите це просто малі випадкові ваги

32
00:02:10,083 --> 00:02:18,643
Зараз, що ми збираємось робити, це показати мережі певну кількість даних і відправити її вивчати ваги, що є кращим за випадкові ваги.

33
00:02:19,550 --> 00:02:27,240
шлях, який ми збираємся розглянути, це коли ми показуємо зображення ми збираємося збільшити ваги

34
00:02:27,240 --> 00:02:33,274
активних пікселів зображення, для коригуваня классу.

35
00:02:33,274 --> 00:02:37,942
як тільки ми  це зробимо, ваги можуть

36
00:02:37,942 --> 00:02:40,782
збільшуватися яке б ми зображення не показали.

37
00:02:40,782 --> 00:02:44,124
Таким чином, мипотребуємо способу тримання ваг під контролем

38
00:02:44,124 --> 00:02:50,311
Що ми збираємось робити  - ми також будемо зменшувати ваги активних пікселів, классу до якого

39
00:02:50,311 --> 00:02:53,040
віднесе нейронна мережа.

40
00:02:53,040 --> 00:03:00,113
Таким чиномми тренуємо робити правильний вибір, замість того, до яких вона має схильність.

41
00:03:00,113 --> 00:03:04,909
Якщо, звичайно вона робить правильний вибір, тоді збільшення, що миробимо

42
00:03:04,909 --> 00:03:11,358
на першому кроці навчального правила будуть повністю скасовувати зменшення, так нічого не зміниться, цете що ми хотіли.

43
00:03:12,896 --> 00:03:18,569
Ось  це  початкові ваги. Зараз мизбираємось показати декілька сот тренувальних зразків і потім

44
00:03:18,569 --> 00:03:20,706
подивимся на ваги знову.

45
00:03:20,706 --> 00:03:25,893
Зараз ваги змінилися, вони починають

46
00:03:25,893 --> 00:03:31,204
формувати звичайні символи. Покажемо ще декілька сот зразків

47
00:03:31,204 --> 00:03:34,871
Ваги трохи змігились, і ще декілька сот зразків.

48
00:03:34,871 --> 00:03:39,648
і ще декілька сот зразків. Ще трохи сотень.

49
00:03:39,648 --> 00:03:43,016
І зараз ваги значно( кращі ) в їх фінальних значеннях

50
00:03:43,186 --> 00:03:50,720
Я розкажу більше в майбутніх лекціях про обчислювальні деталі навчального алгоритму Але що ви можете побачити це

51
00:03:50,720 --> 00:03:54,082
те, що ваги зараз виглядають як маленькі шаблони символів.

52
00:03:54,082 --> 00:04:02,128
Якщо ви поглянете на ваги елемента один, наприклад, це не геть шаблон для ідентифікації одиниці. Це не просто зразок.

53
00:04:02,128 --> 00:04:04,786
Якщо ви подивитесь на ваги елемента дев'ять

54
00:04:04,786 --> 00:04:08,875
вони не мають жодної позитивної ваги нижче середньої лінії

55
00:04:08,875 --> 00:04:16,137
це для того щоб позначити різницю між 9ою і 7ою, ваги нижче середине не дуже використовувалися.

56
00:04:16,137 --> 00:04:21,710
Ви повинні сказати різницю вирішуючи чи є петля зверху, чи горизонтальноа полоса зверху.

57
00:04:21,710 --> 00:04:26,421
Таким чином, ці вихідні елементи сфокусовані на дискримінації

58
00:04:29,928 --> 00:04:38,703
Одна річ про цей алгоритм навчання пов'язана з тим, що мережа дуже проста, немає можливості вивчити дуже добрий спосіб відділення символів.

59
00:04:40,995 --> 00:04:45,707
Все що вона вчить еквівалентно маленкому шаблону для кожного з символів.

60
00:04:45,707 --> 00:04:53,533
І потім, вибір переможця базужться на тому, який з символів має шаблон, шо покриває більшу частину з написаного.

61
00:04:54,425 --> 00:05:03,043
Проблема в тому, що ваги в яких рукописні символи розрізняються занадто складні для розпізнавання простим шаблоном всього символа.

62
00:05:03,043 --> 00:05:13,250
Ви маєте модель що дозволяє варіацію цифр. Визначіть спочатку властивості і потім знайдіть розташування цих властивостей

63
00:05:14,774 --> 00:05:18,090
Цеприклади, якіми вже бачили.

64
00:05:18,090 --> 00:05:29,790
Якщо ви подивитеся на цю двійку в зеленому квадраті, ви зможете побачити що тут немає шаблону, що добре все покриває і дає збій покриваючи трійку в червоному квадраті.

65
00:05:29,790 --> 00:05:33,721
Тобто задача просто не може бути розв'язана такою простою мережею як ця.

66
00:05:35,025 --> 00:05:38,800
Нейронна мережа робить найкраще з того, що може, але не може розв'язати проблему.