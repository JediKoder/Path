Привіт! Ласкаво прошу до Курсерівського курсу з Нейронних мереж для машинного навчання. Перед тим як ми зануримось в деталі алгоритмів навчання нейронних мереж, я хочу ще трохи поговорити про машинне навчання чому ми потребуємо машинне навчання, типи задач, для яких ми можемо їх застосувати, і покажемо деякі приклади того, що вони можуть робити. Причина, через яку ми потребуємо машинне навчання це комплекс проблем, для яких дужеважко писати програми, розпізнавання трьох вимірних об'єктів наприклад. Коли це відбувається з додаванням  нових кутів зору і освітлення в захаращенному зображенні, це дуже важко зробити Ми не знаємо яку писати програму,  тому що ми не знаємо як це відбувається в нашому мозку. И навіть якщо б ми знали яку програму писати, це могла б бути страшенно складна програма Інший приклад це визначення шахрайських транзакцій з кредитних карток, де не може бути гарних, простих правил які б вказували на те, що вона шахрайська Насправді, потрібно комбінувати значну кількість не дуже надійних правил І також, ці правила кожен раз змінюються, бо люди змінюють трюки, які вони використовують для шахрайства. Таким чином, ми потребуємо складну програму, що комбінує ненадійні правила, які и можемо легко міняти. Підхід машинного навчання полягає в тому, що скажімо, замість написання кожної програми вручну для кожної специфічної задачі, ми збираємо багато зразкві і визначаємо правильний результат для вхідних даних Алгоритм машинного навчання потім бере ці приклади і створює програму, яка виконує роботу. Програма створена алгоритмом що навчається,  може дуже відрізнятися від типової програми написаної вручну. Наприклад, вона може містити мілліони значень про те як ви оцінюєте різні види ознак. Якщо ми робимо це правильно, програма буде працювати для нових випадкві так само добре як і для тих на яких вона натренована І якщо дані зміняться, ми будемо мати можливість змінити хід виконання програми дуже просто перетренувавши її на нових даних. І зараз великі обсяги обчислень дешевше ніж плата кому-небудь за написання прграми для визначеної задачі, таким чином ми можемо дозволити собі значно складніші программи машинного навчання для виконання цих важких (абсолютних?) задач специфічних систем для нас. Деякі приклади таких задач, які краще розв'язуються використанням алгоритмів навчання, це розпізнавання зразків, так, наприклад об'єкти в реальних сценах або ідентичність чи вираз обличчя людини, або усна мова. Також це визначення аномалій Так, незвичайна послідовність транзацій кредитної картки може бути аномальною. Іншим прикладом аномальності можна назвати незвичайний зразок зчитування(даних) сенсора атомної станції. І ви насправді нехочете мати справу робити це з навчанням з учителем(навчання під керівництвом на зразках) Коли ви дивитесь на те що вибухнуло, и бачите яка причина викликала цей вибух Ви дійсно бажали б розпізнати, що щось дивне з'явилося без отримання підтверджуючого сигналу. Цього просто не станеться в нормальному випадку І ще  - це прогнозування. Так, типово прогнозують ціни фьючерсів або обмінного курсу валют або прогнозують який фільм сподобається особі знаючи які інші фільми він полюбляє. І які фільми багато інших людей полюбляє. Так, в цьому курсі я вважатиму  стандартним зразком для пояснення багатьох алгоритмів машинного навчання. Це зроблено в багатьох науках. В генетиці наприклад, багато генетичних (досліджень) зроблено на фруктових мухах(дрозофілах?) І причина в тому, що вони зручні. Вони швидко розмножуються і багато вже відомо про генетику плодових мушок. База даних MNIST рукописних символів це машиний еквівалент фруктових мух. Вони публічно доступні. Ми можемо застосувати машинні алгоритми навчання для тренування розпізнавання цих рукописних символів досатньо швидко, так що легко спробувати багато варіацій. І ми знаємо значну кількість про те як добре різні машинні алгоритми працюють на MNIST. І  зокрема, різні методи машинного навчання були реалізовані людьми що вірили в них, тому ми можемо покладатися на цы результати. Так, з усіх цих причин ми збираємся використовувати MNIST як нашу стандартну задачу Ось приклад деяких цифр MNIST Це ті, що коррекно були розпізнані нейронною мережею, яка перший раз побачила їх. Але це ті, щодо яких нейронна мережа не була дуже впевнена. І ви зможете побачити чому. Я розташував ці цифри в стандартному відсканованому порядку Так нулі, потім одноці, потім двійки і так далі. Якщо ви поглянете на кілька елементів як ті що в зеденому квадраті. Ви можете побачити, що якщо ви знаєте що було 100 в цифрах, ви ймовірно здогадаєтесь, це були двійки. Але це дуже важко сказати що робить їх двійками. Тут нема нічого простого, що вони всі б мали взагалі. Зокрема, якщо ви спробуєте накласти одну на іншу, ви побачите, що вони не співпадають. І навіть якщо ви їх трохи повернете, дуже важко зробити їх перекриваючими. Так, визначення зразка не допоможе. Зокрема зразок який дуже важко знайти покриє двійки в зеленому прямокутнику і також покривав би зразки в червоному прямокутнику. Так, це та річ, що робить розпізнавання рукописних исел гарною задачею для машинного навчання. Зараз, я не хочу щоб ви думали, що це лише те що мим можемо робити. Це відносно просто для нашої системи машинного навчання зараз. І щоб  мотивувати іншу частину курсу, я хочу показати деякі зразки більш складних речей. Так, зараз ми маємо нейронні мережі що пропонують сотні мільїонів параметрів в них, що можуть розпізнавати тисячі різних класів об'єктів з 1.3 мілліона тренувальних зображень високої якості, що були отримані з вебу. Так, в 2010 році проводилося змагання найкраща система отримала 47 відсотків рівня помилки якщо дивитися на перший вибір і 25 відсотків помилки якщо вважати що вона була права, якщо правильна відповідь була серед перших 5 запропонованих, що не так погано для 1000 різних об'єктів. Жітендра Малік,  відомий скептик нейронних мереж і лідуючий дослідник комп'ютерного зору сказав, що ці змагання  - гарний тест чи зможуть глибокі нейронні мережі працювати добре для розпізнавання образів. І дуже глибока нейрона мережа може зараз зробити значно більше ніж виграти змагання. Вона може взяти менш ніж 40 відстоків помилки, для першого вибору, і менш ніж 20 відсотків для перших п'яти Я опишу значно детальніше в лекції п'ять Ось деякі приклади видів зображень які необхідно розпізнати. Ці зображення з тестового набору які вона(Н.М.) раніше не бачила І під зразком, я показую що нейронна мережа вважає правильною відповіддю. Де довжина горизонтальної полоси це наскільки впевненою вона була, а правильна відповідь - червона. Так, якщо ви поглянете, посередині, воно корректно ідентифіковано як снігоочисна машина Але ти можеш бачити, що їх інші вибори достатньо чутливі Воно дійсно виглядає трохи схожою на бурову платформу І якщо ви подивитесь на третій вибір, рятувальну лодку, він дійсно виглядає дуже схоже на рятувальну лодку. Ви можете бачити прапор носі лодки і (капітанський) місток і флаг на кормі, і високу хвилю на фоні. Таким чином, її помилки говорять багато про те як все відбувається і це дуже правдоподібні помилки На малюнку лівооруч - результат неправильний. Він неправильний, ймовірно тому, що клюв птаха пропущений і тому що пір'я птаха виглядають дуже схожими на моке хутро видри Але вона знайшло її в перших п'яти і вона зробила це краще за мене Я б не знав, чи це був перепел, або куріпка, або грівчаста куріпка Якщо ви подивитесь на правий малюнок - відповідь повністю неправильна Це гільйотина. Ви можете сказати чому вона так відповіла. Ви можливо бачите чому она сказала що це орангутанг, томущо аздній фон виглядає як джунглі і щось помаранчеве в середині. Але вона не змогла дати правильну відповідь. Але вона може, наразі, мати справу з широким переліком різних об'єктів Якщо ви подивитесь ліворуч, я б сказав що це мікрохвильова піч на перший погляд Мітки не дуже систематичні. Але насправді, правильна відповідь електрична плита. І вона повернула це в перших п'яти Посередині, вона отримала турнікет, який є розподіленим об'єктом Вона змогла, вона може більше ніж розпізнавати компактні об'єкти Також, вона може мати справу з малюнками так само гарно як і з реальними сценами, як наприклад бронежилет І вона робить деякі цікаві(класні?) помилки. Якщо ви подивитесь ліворуч - то це навушники Вона не видала нічого схожого на навушники. Але якщо ви подивитесь четверту відповідь, вона думає що це мураха. І для вас це виглядає божевільним. Але потім, якщо ви подивитесь на неї акуратно, ви зможете побачити, це вигляд мурахи з низу. Очі дивляться знизу знизу на вас і ви можете бачити антени позаду. Це не той вигляд мурахи, який би ви хотіли мати якби були зеленою мухою. Якщо ви подивитесь на правий малюнок там не має правильної відповіді. Але всі відповіді циліндричні об'єкти. Інше завдання в якому нейронні мережі зараз дуже добрі, це розпізнавання мови. Або як мінімум частина системи розпізнавання мови. Так, системи розпізнавання мови мають декілька стадій. Поперше  - попередня обробка звукової хвилі, щоб отримати вектор акустичних коефіцієнтів, для кожних 10 мілісекунд звукової хвилі. І так вони отримують 100 цих векторів за секунду Потім вони беруть декілька сусідніх векторів акусичних коефіцієнтів і вони потребують визначити ваги для яких частин яких фонем вони були вимовленію Так, вони дивляться на це маленьке віконечко і вони кажуть всередині цього вікна що я думаю, яка це фонема є, і яка це частина цієї фонеми І гарна система розпізнавання мови буде мати значну кількість альтернативних моделей для фонеми і кожна модель повинна мати три різних частини. Так, вона може мати тисячі альтернативних фрагментів, що вона думає можуть бути. І ви маєте поставити ваги на всі ці тисячі альтернатив І потім, як тільки ви виставите ці ваги, ви отримуєту стадію декодування, що робить кращу роботу, вона може використовуючи правдоподібні ваги, але складаючи їх раом в послідовність ваг, що відповідають тому набору, що люди вимовляють Наразі, глибока нейрона мережа, відкрита джорджом Дахлом і Абель-рахманом Мохаммедом з університету Торонто, виконує це кращі ніж попередні методи машинного навчання для акустичних моделей, і вони зараз починають впроваджуватися  в практичні системи. Так, Дахл і Мохаммед розробили систему що використовує багато шарів бінарних нейронів, для того щоб зробити акустичні фрейми і проставити ваги для міток. вони зробили достатньо малу базу даних і потім використовували 183 альтернативні мітки І для того, щоб їх система працювала добре, вони зробили деяке попереднє тренування, яке буде описано в другій частині нашого курсу. Після стандартного постпроцессінгу, вони отримали 20.7 відсотковий рівень помилок на дуже стандартному тесті, який є чимось накшталт MNIST для мови. Найкращий попередній результат на цьому тесті був 24.4%. І дуже досвідчені мовні дослідники в Майкрософт ресерч зрозуміли це, це було значне вдосконалення, що ймовірно це може змінити шлях яким системи розпізнавання речі будуть створені. І дійсног, так і є. Так, якщо ви переглянете свіжі результати з декількох різних розповсюджених мовних груп, Майкросоіт показало, що варіант глибоких нейронних мереж, коли використовується як акустична модель в ситемі розпізнавання мови зменшує рівень помилок з 27.4 відсотків до 18.5% або льтернативно, ви можете побачити вона зменшує кількість тренувальних даних, які ви потребуєте з 2000 годин до 309 годин для отримання порівнюваного результату. АйБіЕм, що мав найкращу систему для одного з стандартного завдання розпізнавання мови для великої системи відновлювального розпізнавання мови показало що навіть дуже сильно підлаштовані системи, які давали 18.8 відстоків можуть бути переможені однією з цих глибоких нейронних мереж. І Гугль, достатньо свіжий, натренував глибоку нейронну мережу на великій кількості мови 5800 годин. Це все одно було менше ніж вони тренували їх змішану модель. Але навіть із значно меншим обсягом даних вона працює значно краще ніж технологія, яку вони мали дотепер. так воно зменшиє рівень помилки з 16% до 12.3% і рівень помилки досі падає. І в останньому Андроїді, якщо ви виберете голосовий пошук, він скористається однією з глибоких нейронних мереж для того щоб зробити дуже гарне розпізнавання мовлення.