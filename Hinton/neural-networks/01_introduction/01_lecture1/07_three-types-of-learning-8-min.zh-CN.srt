1
00:00:00,000 --> 00:00:04,052
在这个视频中 我会介绍机器学习的三种类别

2
00:00:04,052 --> 00:00:08,057
监督学习 强化学习和无监督学习

3
00:00:08,057 --> 00:00:13,027
大体而言 课程前半部分将会讨论监督学习

4
00:00:13,027 --> 00:00:17,079
后一半主要讨论非监督学习

5
00:00:17,079 --> 00:00:22,049
这门课将不会涉及强化学习 因为我们不可能面面俱到

6
00:00:22,049 --> 00:00:26,060
机器学习可以被粗略地分为三类

7
00:00:26,060 --> 00:00:30,067
在监督学习中 你试图依据给定的输入向量来预测输出结果

8
00:00:30,067 --> 00:00:35,092
因此监督学习的特点很明显

9
00:00:35,092 --> 00:00:41,017
强化学习中 你试图

10
00:00:41,017 --> 00:00:46,607
选择一系列行为或行为序列来使得获得奖励最大化

11
00:00:46,607 --> 00:00:53,030
而且这个奖励可能不定时产生的 
在无监督学习中 你试图

12
00:00:53,030 --> 00:00:59,577
找到对于输入数据的好的内在表达
我们将在后面对此详细解释

13
00:00:59,577 --> 00:01:03,795
监督学习又可以被分为两类 回归和分类

14
00:01:03,795 --> 00:01:08,121
回归问题中 目标输出是一个实数或者一个实数向量

15
00:01:08,121 --> 00:01:14,135
比如六个月以后的股价 或者明天中午的气温

16
00:01:14,135 --> 00:01:21,059
总之回归问题的目标是 让输出
尽可能地接近正确的实数或实数向量

17
00:01:21,059 --> 00:01:25,399
分类问题中 目标输出是一个分类标签

18
00:01:25,399 --> 00:01:29,364
最简单的例子就是在零和一之间二选一

19
00:01:29,364 --> 00:01:32,606
也就是在阳性和阴性之间给出一个选择

20
00:01:32,606 --> 00:01:37,636
当然 我们也可以有多个备选分类 比如手写数字识别问题

21
00:01:37,636 --> 00:01:44,492
在监督学习中 我们必须在一开始找到一个模型集合

22
00:01:44,492 --> 00:01:49,512
里边是一系列的备选模型

23
00:01:49,512 --> 00:01:53,422
你也可以认为它是一个函数

24
00:01:53,422 --> 00:01:59,825
根据输入向量 x 和相关的参数 W  能够得到相应的输出 y

25
00:01:59,825 --> 00:02:03,836
所以模型集合可以被认为是一种映射

26
00:02:03,836 --> 00:02:10,939
从输入到输出的映射，使用了数值参数W，

27
00:02:10,939 --> 00:02:16,394
我们对参数进行调整，使这种映射关系能拟合
相应的训练数据。

28
00:02:16,394 --> 00:02:22,046
拟合，在这里指最小化每一条训练数据的目标输出和

29
00:02:22,046 --> 00:02:27,255
机器学习算法实际输出的差异。

30
00:02:27,255 --> 00:02:32,591
回归问题中，差异的度量，

31
00:02:32,591 --> 00:02:38,746
常使用的是我们系统的输出y和正确输出t的差异的平方
再乘以二分之一

32
00:02:38,746 --> 00:02:44,057
这个系数（二分之一）可以在求导的时候被消掉。

33
00:02:44,057 --> 00:02:47,453
对分类问题，

34
00:02:47,453 --> 00:02:51,994
我们也可以使用这个度量，
但是存在更加敏感的度量方式，

35
00:02:51,994 --> 00:02:56,203
其更有效，我们课程后面会详细描述。

36
00:02:56,203 --> 00:03:03,055
强化学习，输出是一系列动作

37
00:03:03,055 --> 00:03:07,080
你需要根据不定时的奖励来决定应该如何选择。

38
00:03:07,080 --> 00:03:12,516
所以目标是选择每个动作，使得将来的奖励最大化。

39
00:03:12,516 --> 00:03:17,139
一般我们会使用一个衰减因子，
这样我们就不用计算到太远的将来。

40
00:03:17,139 --> 00:03:20,472
换言之，很久以后才获得的奖励

41
00:03:20,472 --> 00:03:24,592
计算时的权重要低于马上就要获得的奖励

42
00:03:24,592 --> 00:03:29,538
强化学习很难，这是因为一般而言奖励是滞后的

43
00:03:29,538 --> 00:03:34,451
所以很难确定一系列动作到底那个是错误的。

44
00:03:34,451 --> 00:03:38,007
强化学习很难，因为奖励是数值，

45
00:03:38,007 --> 00:03:41,879
是不定时给出的标量，起没有没有提供很多信息

46
00:03:41,879 --> 00:03:45,082
用以构建相应的参数。

47
00:03:45,082 --> 00:03:50,235
一般而言，强化学习中你没法学习出上百万的参数，

48
00:03:50,235 --> 00:03:53,830
但是在监督学习和非监督学习中，是可以的。

49
00:03:53,830 --> 00:03:57,798
典型的强化学习，你试图学习

50
00:03:57,798 --> 00:04:00,755
数十个或者一千个参数，但是不会是一百万个。

51
00:04:00,755 --> 00:04:04,827
这个课程不会覆盖一切，

52
00:04:04,827 --> 00:04:08,552
所以虽然强化学习很重要，但是我们不会涉及。

53
00:04:08,552 --> 00:04:14,350
课程后半部分我们会讨论非监督学习。

54
00:04:14,350 --> 00:04:20,040
长达40年的时间里 机器学习学界
基本上忽视了非监督学习

55
00:04:20,040 --> 00:04:24,282
除了聚类问题这一种非常有限的形式

56
00:04:24,282 --> 00:04:28,990
实际上 他们对机器学习的定义
就排除了非监督学习

57
00:04:28,990 --> 00:04:34,481
在某些教科书上，
机器学习被定义为一种从输入到输出的映射

58
00:04:34,481 --> 00:04:37,589
很多机器学习研究人员认为

59
00:04:37,589 --> 00:04:40,822
聚类是唯一的一种非监督学习

60
00:04:40,822 --> 00:04:46,870
原因之一是因为很难定义非监督学习的目的

61
00:04:46,870 --> 00:04:50,518
其中一个主要目的

62
00:04:50,518 --> 00:04:54,879
是得到输入数据的内在表达，从而在接下来的

63
00:04:54,879 --> 00:04:59,188
监督学习或者强化学习中排上用场。

64
00:04:59,188 --> 00:05:04,481
分成两个阶段，以强化学习为例，
是因为我们不想使用奖励来

65
00:05:04,481 --> 00:05:08,503
得到我们视觉系统的参数。

66
00:05:08,503 --> 00:05:13,310
 你可以通过计算双眼看到的图像的差异来计算

67
00:05:13,310 --> 00:05:17,076
到某个表面距离。
你肯定不会通过

68
00:05:17,076 --> 00:05:21,003
不停的调整你自己视觉系统的相关参数，

69
00:05:21,003 --> 00:05:24,566
从而计算出相应的距离。

70
00:05:24,566 --> 00:05:29,100
真这样做，
会挪很多次，

71
00:05:29,100 --> 00:05:33,474
通过学习融合输入的两张图片提供的信息
无疑是更好的方式。

72
00:05:33,474 --> 00:05:37,799
非监督学习的另一个目标

73
00:05:37,799 --> 00:05:42,194
是为输入数据提供一个紧凑，低维的表达。

74
00:05:42,194 --> 00:05:47,149
高维数据，比如图片，
往往存在或者近似与

75
00:05:47,149 --> 00:05:51,599
一个低维流形。
或者多个这样的流形，比如手写数字这个例子。

76
00:05:51,599 --> 00:05:55,584
具体的说，
你有一张一百万像素的图片，

77
00:05:55,584 --> 00:06:00,605
但是并不存在一百万维的自由度。

78
00:06:00,605 --> 00:06:04,118
可能仅仅只有几百个维度的自由度。

79
00:06:04,118 --> 00:06:08,025
所以我们要做的是

80
00:06:08,025 --> 00:06:12,617
从百万像素

81
00:06:12,617 --> 00:06:15,804
依据我们在流形相关信息，

82
00:06:15,804 --> 00:06:18,342
我们还需要知道我们在那个流形上。

83
00:06:18,342 --> 00:06:24,321
一个很基础的方式就是主成分分析，其是线性的。

84
00:06:24,321 --> 00:06:29,064
其假设存在一个流形，
其是高维空间的一个平面。

85
00:06:29,064 --> 00:06:33,323
非监督学习的另一个定义，

86
00:06:33,323 --> 00:06:37,846
或者另一个目标，

87
00:06:37,846 --> 00:06:41,746
就是把输入转换为的表达。

88
00:06:41,746 --> 00:06:46,605
比如，我们可以将输入表达为binary特征，

89
00:06:46,605 --> 00:06:51,552
这样很经济，
因为每个变量只需要一个比特来说明其状态。

90
00:06:51,552 --> 00:06:54,600
或者我们可以是用大量的实数变量，

91
00:06:54,600 --> 00:06:59,330
但是限制输入，使得几乎所有的变量都是0。

92
00:06:59,330 --> 00:07:03,481
这种情况下，对每个输入，

93
00:07:03,481 --> 00:07:07,107
我们仅仅要表达少数实数，这也是经济划算的。

94
00:07:07,107 --> 00:07:13,711
前面提到过 非监督学习的另一个定义

95
00:07:13,711 --> 00:07:18,543
或者说它的另一个目标 是对输入进行聚类

96
00:07:18,543 --> 00:07:23,969
聚类也可以被认为是非常稀疏的编码，
对于每一个聚类我们有一个变量，

97
00:07:23,969 --> 00:07:30,062
那么对于所有变量来说，
除了一个，其他取值都为0，这一个取值为1。

98
00:07:30,062 --> 00:07:33,814
所以聚类是稀疏编码的

99
00:07:33,814 --> 00:07:36,037
一种极端形式。