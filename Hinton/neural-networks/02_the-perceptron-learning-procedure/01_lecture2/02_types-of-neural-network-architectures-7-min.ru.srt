1
00:00:00,090 --> 00:00:05,060
В этом видео я хочу описать различные типы архитектур

2
00:00:05,060 --> 00:00:08,067
нейронных сетей. Под архитектурой я подразумеваю способ

3
00:00:08,067 --> 00:00:11,033
связи нейронов между собой.

4
00:00:11,033 --> 00:00:16,003
На сегодняшний день наиболее распространенным типом архитектуры, применяемым на практике, является

5
00:00:16,003 --> 00:00:20,061
нейронная сеть прямого распространения, в которой вся информация приходит во входные элементы и

6
00:00:20,061 --> 00:00:25,013
движется в одном направлении через скрытые слои до тех пор пока не достигнет выходных элементов.

7
00:00:25,013 --> 00:00:28,042
Гораздо более интересной представляется архитектура

8
00:00:28,042 --> 00:00:33,013
рекуррентных нейронных сетей, в которых информация может двигаться циклично по кругу.

9
00:00:33,013 --> 00:00:36,060
Эти нейронные сети могу надолго запоминать информацию.

10
00:00:36,060 --> 00:00:41,044
Они способны выявлять самые разнообразные колебания, но они

11
00:00:41,044 --> 00:00:46,052
гораздо сложнее поддаются тренировке отчасти из-за того, что они могут решать гораздо более сложные задачи.

12
00:00:46,052 --> 00:00:50,040
Хотя, в последнее время человечество достигло

13
00:00:50,040 --> 00:00:55,045
значительных результатов в области тренировки рекуррентных нейронных сетей и сегодня такие сети способны делать

14
00:00:55,045 --> 00:00:59,002
действительно впечатляющие вещи.
Последний тип архитектуры, которую я

15
00:00:59,002 --> 00:01:03,088
опишу, это сеть с симметричной матрицей связей, такая, в которой весовые коэффициенты связи между нейронами

16
00:01:03,088 --> 00:01:08,093
равнозначны в обоих направлениях для двух связанных нейронов.
Наиболее часто применяются на практике

17
00:01:08,093 --> 00:01:12,037
нейронные сети прямого распространения.

18
00:01:12,037 --> 00:01:17,011
У такой сети есть несколько входных рецепторов — в первом слое снизу.

19
00:01:17,011 --> 00:01:22,049
Несколько выходных нейронов наверху и один или несколько скрытых слоёв.

20
00:01:22,049 --> 00:01:27,087
Если у сети более одного скрытого слоя, мы называем такую сеть глубокой нейронной сетью.

21
00:01:28,054 --> 00:01:32,090
Эти сети производят ряд изменений сигнала между входным

22
00:01:32,090 --> 00:01:35,046
и выходным слоями. Таким образом на каждом слое, вы получаете новое

23
00:01:35,046 --> 00:01:40,000
представление входных данных, в котором то, что было схожим в предыдущем

24
00:01:40,000 --> 00:01:44,025
слое, может стать менее схожим, либо данные, которые были различными

25
00:01:44,025 --> 00:01:46,081
в предыдущем слое, могут стать более схожими.

26
00:01:46,081 --> 00:01:51,047
Например, в распознавании речи мы хотим, чтобы одна и та же фраза произнесённая разными

27
00:01:51,047 --> 00:01:56,001
людьми, становилась всё более схожей, а разные фразы, произнесённые одним человеком,

28
00:01:56,001 --> 00:01:59,079
, становились бы всё менее схожими по мере прохождения слоёв в нейронной сети.

29
00:02:00,045 --> 00:02:04,097
Чтобы достичь этого, нам нужно, чтобы активность нейронов каждого слоя

30
00:02:04,097 --> 00:02:08,047
была нелинейной функцией активности предыдущего слоя.

31
00:02:10,034 --> 00:02:15,053
Рекуррентные нейронные сети - намного более мощный инструмент, чем нейросети прямого распространения.

32
00:02:15,053 --> 00:02:18,096
У них есть направленные циклы

33
00:02:18,096 --> 00:02:23,073
в графе их связей. Это значит, что если вы начинаете движение

34
00:02:23,073 --> 00:02:29,013
с вершины или нейрона и следуете по стрелкам, вы можете иногда возвращаться 

35
00:02:29,013 --> 00:02:34,063
к нейрону, с которого вы начинали. У этих сетей может быть очень сложная динамика,

36
00:02:34,063 --> 00:02:37,074
и это может сделать их очень труднообучаемыми.

37
00:02:38,078 --> 00:02:42,097
В настоящее время есть много интересного в нахождении эффективных путей обучения

38
00:02:42,097 --> 00:02:46,069
данных [сетей], так как они очень производительны, если мы сможем их обучить.

39
00:02:47,050 --> 00:02:54,086
К тому же, они более реалистичны с бтологической точки зрения. Рекуррентные нейросети с множественными

40
00:02:54,086 --> 00:02:59,026
скрытыми слоями, в действительности, только частный случай рекуррентных нейросетей в целом,

41
00:02:59,026 --> 00:03:02,034
у которых потеряна часть связей между скрытыми слоями.

42
00:03:05,012 --> 00:03:09,069
Рекурентные нейронные сети - очень натуралистичный способ моделирования последовательных данных.

43
00:03:10,080 --> 00:03:15,052
Для этого мы устанавливаем связи между скрытыми элементами,

44
00:03:15,078 --> 00:03:20,036
и скрытые элементы со временем действуют как очень глубокая сеть.

45
00:03:20,036 --> 00:03:26,008
Так, в каждый момент времени состояния скрытых элементов определяют состояния

46
00:03:26,008 --> 00:03:32,073
скрытых элементов в следующий момент времени. Отличие от

47
00:03:32,073 --> 00:03:36,049
сетей прямого распространения в том, что здесь мы используем одни и те же веса связей в каждый момент времени.

48
00:03:36,049 --> 00:03:41,001
Таким образом, если вы посмотрите на эти красные стрелки, которые показывают как скрытые элементы определяют

49
00:03:41,001 --> 00:03:45,026
состояния следующих скрытых элементов, то матрица весовых коэффициентов, изображенная каждой из красных стрелок, 

50
00:03:45,026 --> 00:03:50,023
одна и та же в каждый момент времени. Эти сети также принимают входные данные в каждый момент времени

51
00:03:50,023 --> 00:03:54,086
и часто выдают результат в каждый момент времени, и там тоже будут использованы одни и теже

52
00:03:54,086 --> 00:03:59,171
матрицы весов. Рекуррентные сети имеют возможность

53
00:03:59,171 --> 00:04:02,030
запоминать информацию в скрытом слое на долгое время.

54
00:04:02,062 --> 00:04:07,097
К сожалению, достаточно сложно обучить их использовать эту способность.

55
00:04:07,097 --> 00:04:12,031
Однако, последние алгоритмы стали способны это сделать.

56
00:04:13,052 --> 00:04:19,032
Итак, просто чтобы показать вам, что сейчас могут делать рекуррентные нейронные сети, я собираюсь показать вам сеть,

57
00:04:19,032 --> 00:04:24,001
разработанную Ильей Суцкевером. Это особый тип рекуррентной

58
00:04:24,001 --> 00:04:29,048
нейросети, немного отличающийся от типа, изображенного на диаграмме на предыдущем слайде, и

59
00:04:29,048 --> 00:04:33,025
она используется, чтобы предсказать следующую букву в последовательности.

60
00:04:33,080 --> 00:04:38,042
Итак, Илья обучал её на множестве и множестве строк из англоязычной википедии.

61
00:04:38,042 --> 00:04:43,070
Сеть видит английские буквы и пытается предугадать следующую английскую букву.

62
00:04:43,070 --> 00:04:49,006
На самом деле он использовал 86 различных символов, для учета пунктуации, и цифр, и

63
00:04:49,006 --> 00:04:54,000
заглавных букв и так далее. После обучения, один из способов увидеть,

64
00:04:54,000 --> 00:04:59,069
насколько хорошо сеть может работать, это посмотреть, присвоит ли она высокую вероятность следующему

65
00:04:59,069 --> 00:05:05,060
символу, который имеет место в действительности. Другой способ увидеть, что сеть может делать, это

66
00:05:05,060 --> 00:05:10,073
сгенерировать с её помощью текст. Что нужно сделать, это дать сети строку

67
00:05:10,073 --> 00:05:14,093
символов и позволить ей угадывать вероятности следующего символа.

68
00:05:14,093 --> 00:05:19,000
Затем вы выбираете следующий символ из этого вероятностного распределения.

69
00:05:19,000 --> 00:05:21,076
Не следует выбирать наиболее вероятный символ.

70
00:05:21,076 --> 00:05:26,044
Если вы будете так делать, то через некоторое время сеть начнет говорить: Соединенные Штаты Соединенных

71
00:05:26,044 --> 00:05:29,038
Штатов Соединенных Штатов Соединенных Штатов.

72
00:05:29,038 --> 00:05:34,065
Это говорит вам что-то о википедии. Но если вы выбираете из вероятностного

73
00:05:34,065 --> 00:05:40,097
распределения, то если сеть говорит, что у Z шанс 1 из 100, вы выбитаете Z один раз

74
00:05:40,097 --> 00:05:45,045
из 100, так вы узнаете гораздо больше о том, что сеть изучила.

75
00:05:45,095 --> 00:05:50,002
Следующий слайд содержит пример текста, который сгенерировала сеть, 

76
00:05:50,002 --> 00:05:54,071
и интересно отметить, как много выучено только путем чтения википедии и в попытке 

77
00:05:54,071 --> 00:06:00,011
предсказать следующий символ. Итак, помните, что этот текст был сгенерирован 

78
00:06:00,011 --> 00:06:06,000
по одной букве за раз. Обратите внимание, что сеть составила разумные осмысленные

79
00:06:06,000 --> 00:06:11,000
предложения, и они все составлены полностью из реальных английских слов.

80
00:06:11,000 --> 00:06:15,099
Местами у неё получились неверно образованные слова, но они обычно понятны.

81
00:06:15,099 --> 00:06:20,084
И обратите внимание, что внутри предложения выдержана какая-либо тематика.

82
00:06:20,084 --> 00:06:25,053
Так, фраза: "Несколько ирландских спецслужб в

83
00:06:25,053 --> 00:06:30,031
Средиземноморском регионе", имеет проблемы, но написана на почти хорошем английском.

84
00:06:30,031 --> 00:06:35,095
Ещё заметьте, что говорится в конце о нечеткости изображения

85
00:06:35,095 --> 00:06:41,026
на любом дорогостоящем типе коробки принтера. Прослеживается определенная тематика, 

86
00:06:41,026 --> 00:06:45,096
такая как изображения и принтеры, и синтаксис весьма хорош.

87
00:06:45,096 --> 00:06:48,095
И помните, что это - один символ за прогон.

88
00:06:52,062 --> 00:06:57,043
Весьма отличными от этих сетей являются нейросети с симметричной матрицей связей.

89
00:06:58,050 --> 00:07:03,030
В них связи между элементами имеют одинаковый вес в обоих направлениях.

90
00:07:03,056 --> 00:07:09,077
Джон Хопфилд и другие обнаружили, что сети с симметричной матрицей связей намного легче

91
00:07:09,077 --> 00:07:15,032
анализировать, чем рекуррентные сети. Так происходит главным образом потому, что они более

92
00:07:15,032 --> 00:07:19,086
ограничены в том, что они могут делать, так как они зависят от функции энергии сети.

93
00:07:20,069 --> 00:07:25,054
Так они не могут, к примеру, моделировать циклы. Вы не можете вернутося туда, откуда в начали,

94
00:07:25,054 --> 00:07:27,045
в одной из этих симметричных сетей.