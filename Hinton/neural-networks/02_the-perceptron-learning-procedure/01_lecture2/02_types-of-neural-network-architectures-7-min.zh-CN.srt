1
00:00:00,090 --> 00:00:05,060
在这个视频中 我将讲解不同种类的神经网络结构

2
00:00:05,060 --> 00:00:08,067
所谓结构指的是

3
00:00:08,067 --> 00:00:11,033
将神经元连接到一起的方式

4
00:00:11,033 --> 00:00:16,003
迄今为止 实际应用中最常见的一种结构
是前馈神经网络

5
00:00:16,003 --> 00:00:20,061
在该结构中 信息从输入层流入
沿着一个方向通过隐含层

6
00:00:20,061 --> 00:00:25,013
直到输出层

7
00:00:25,013 --> 00:00:28,042
另一种更有趣的结构是循环神经网络 RNN

8
00:00:28,042 --> 00:00:33,013
信息在其中循环流动

9
00:00:33,013 --> 00:00:36,060
这类网络可以长时间记忆信息

10
00:00:36,060 --> 00:00:41,044
表现出各种有趣的振荡
但相对更难训练

11
00:00:41,044 --> 00:00:46,052
因为比起它们的能力 其结构要更加复杂

12
00:00:46,052 --> 00:00:50,040
不过最近人们在如何训练 RNN 上
取得了很大的进步

13
00:00:50,040 --> 00:00:55,045
它们已经能够完成许多出色的任务了

14
00:00:55,045 --> 00:00:59,002
最后一种结构是对称连接网络

15
00:00:59,002 --> 00:01:03,088
该网络中的两个神经元之间

16
00:01:03,088 --> 00:01:08,093
两个方向的权重是相同的

17
00:01:08,093 --> 00:01:12,037
实际应用中 前馈神经网络是最常见的神经网络

18
00:01:12,037 --> 00:01:17,011
如右图所示 第一层是输入单元

19
00:01:17,011 --> 00:01:22,049
输出单元在上方最后一层
中间是一层或更多层的隐层单元

20
00:01:22,049 --> 00:01:27,087
如果有多个隐层
就称为深度神经网络

21
00:01:28,054 --> 00:01:32,090
这些网络计算输入和输出间的一系列变换

22
00:01:32,090 --> 00:01:35,046
因此在每一层都能得到输入的新表示

23
00:01:35,046 --> 00:01:40,000
前一层原本相似的地方

24
00:01:40,000 --> 00:01:44,025
在后一层可能变得不同

25
00:01:44,025 --> 00:01:46,081
而原本不同的地方也可能变得相似

26
00:01:46,081 --> 00:01:51,047
比如在语音识别的例子中

27
00:01:51,047 --> 00:01:56,001
我们希望通过网络后
不同的人说的同样的内容变得相似

28
00:01:56,001 --> 00:01:59,079
而同一个人说的不同的内容差异变大

29
00:02:00,045 --> 00:02:04,097
为了实现这一点 我们需要让每一层神经元信号的非线性输出

30
00:02:04,097 --> 00:02:08,047
成为下一层神经元的输入

31
00:02:10,034 --> 00:02:15,053
RNN 比前馈网络更强大

32
00:02:15,053 --> 00:02:18,096
 RNN 的连接图中包含了有向环路

33
00:02:18,096 --> 00:02:23,073
这意味着如果从一个神经元开始
沿着箭头移动

34
00:02:23,073 --> 00:02:29,013
有时候可能又回到了开始的神经元

35
00:02:29,013 --> 00:02:34,063
RNN 参数的动态变化非常复杂

36
00:02:34,063 --> 00:02:37,074
这让它们很难训练

37
00:02:38,078 --> 00:02:42,097
现在有很多人在努力发现训练 RNN 的有效方法

38
00:02:42,097 --> 00:02:46,069
因为如果可以有效训练 RNN 它们将发挥强大的性能

39
00:02:47,050 --> 00:02:54,086
RNN 也更切合生物学

40
00:02:54,086 --> 00:02:59,026
多隐层的 RNN
只是常规 RNN 的特例

41
00:02:59,026 --> 00:03:02,034
只是少了隐层单元之间的相互连接

42
00:03:05,012 --> 00:03:09,069
RNN 是对序列数据非常自然的建模方式

43
00:03:10,080 --> 00:03:15,052
我们要做的是在隐层单元之间建立连接

44
00:03:15,078 --> 00:03:20,036
因此这些隐层单元表现为在时序上很深的网络

45
00:03:20,036 --> 00:03:26,008
在每个时间步 隐层单元的当前状态
决定了它下个时序的状态

46
00:03:26,008 --> 00:03:32,073
不同于前馈网络

47
00:03:32,073 --> 00:03:36,049
RNN 在每个时间步都使用相同的权重

48
00:03:36,049 --> 00:03:41,001
看看那些红色箭头 隐层单元决定了它的下一个状态

49
00:03:41,001 --> 00:03:45,026
红色箭头指示的权重矩阵
在每一步都是一样的

50
00:03:45,026 --> 00:03:50,023
它们在每个时间戳获取输入
同时产生输出

51
00:03:50,023 --> 00:03:54,086
而且使用相同的权重矩阵

52
00:03:54,086 --> 00:03:59,171
RNN 具有在隐藏单元中

53
00:03:59,171 --> 00:04:02,030
长时间记忆信息的能力

54
00:04:02,062 --> 00:04:07,097
但训练其使用这种能力却很难

55
00:04:07,097 --> 00:04:12,031
不过最近的算法已经能够实现训练了

56
00:04:13,052 --> 00:04:19,032
现在要给你展现一下 RNN 的能力

57
00:04:19,032 --> 00:04:24,001
这是一个 Ilya Sutskever 设计的 RNN

58
00:04:24,001 --> 00:04:29,048
和上一张幻灯片中的 RNN 略有不同

59
00:04:29,048 --> 00:04:33,025
它被用来预测一个序列中的下一个字符

60
00:04:33,080 --> 00:04:38,042
Ilya 用了很多英文维基中的字符串来训练

61
00:04:38,042 --> 00:04:43,070
它会看到一串英文字符 并试图预测下一个英文字符

62
00:04:43,070 --> 00:04:49,006
Ilya 实际使用了86个不同的字符
包括标点符号 数字 大写字母等等

63
00:04:49,006 --> 00:04:54,000
训练完成后 一种评估的方法是
看它是否会给实际出现的字符

64
00:04:54,000 --> 00:04:59,069
分配较高的概率值

65
00:04:59,069 --> 00:05:05,060
另一方法是看它能产生出什么样的文本

66
00:05:05,060 --> 00:05:10,073
所以 你要做的就是给它一个字符串

67
00:05:10,073 --> 00:05:14,093
让它预测下一个字符出现的概率

68
00:05:14,093 --> 00:05:19,000
然后从它给处的概率分布中挑选出下一个字符

69
00:05:19,000 --> 00:05:21,076
直接挑选概率最高的是没用的

70
00:05:21,076 --> 00:05:26,044
如果这样做 过一会它就会开始说

71
00:05:26,044 --> 00:05:29,038
the United States of the United States of the United
States

72
00:05:29,038 --> 00:05:34,065
但是如果从概率分布中挑的话

73
00:05:34,065 --> 00:05:40,097
比如 它说有1/100的机率是个Z
那么在100次中 只挑1次Z

74
00:05:40,097 --> 00:05:45,045
这样才能看出它学到了多少

75
00:05:45,095 --> 00:05:50,002
下一张幻灯片展示了它生成的一个文本的例子

76
00:05:50,002 --> 00:05:54,071
只让它阅读维基百科来预测字符
看它学到了多少

77
00:05:54,071 --> 00:06:00,011
请记住这段文字是逐字符生成的

78
00:06:00,011 --> 00:06:06,000
它产生出了合理有意义的句子

79
00:06:06,000 --> 00:06:11,000
而且都是由真正的英语单词组合而成

80
00:06:11,000 --> 00:06:15,099
有时它会产生不正确的单词
但通常是有意义的

81
00:06:15,099 --> 00:06:20,084
同时注意到 在句子中还包含有一些主题句

82
00:06:20,084 --> 00:06:25,053
比如这句话
Severa Irish intelligence agents is in the Mediterranean region

83
00:06:25,053 --> 00:06:30,031
虽然有些问题 但几乎是不错的英语了

84
00:06:30,031 --> 00:06:35,095
还要注意最后这部分
such that it is the blurring of appearing

85
00:06:35,095 --> 00:06:41,026
on any well-paid type of box printer

86
00:06:41,026 --> 00:06:45,096
有些关于外观 印刷的主题
而且语法相当不错

87
00:06:45,096 --> 00:06:48,095
请记住 这都是逐个字符产生的

88
00:06:52,062 --> 00:06:57,043
对称连接网络和 RNN 完全不同

89
00:06:58,050 --> 00:07:03,030
在对称连接网络中
单元间的连接在两个方向权重相同

90
00:07:03,056 --> 00:07:09,077
John Hopfield 和其他学者认识到

91
00:07:09,077 --> 00:07:15,032
对称网络比 RNN 容易分析得多

92
00:07:15,032 --> 00:07:19,086
原因在于对称网络服从的能量函数
制约了他们可以做的事

93
00:07:20,069 --> 00:07:25,054
比如 不能建模环路 即在对称网络中

94
00:07:25,054 --> 00:07:27,045
你无法回到起点