1
00:00:00,000 --> 00:00:04,108
这段视频中 我们来看如何先用 

2
00:00:04,108 --> 00:00:08,969
堆叠受限玻尔兹曼机的方式
学习一个深度信念网络 

3
00:00:08,969 --> 00:00:12,164
然后将其视为一个 

4
00:00:12,164 --> 00:00:16,140
可以进行判别性微调的深度神经网络 

5
00:00:16,140 --> 00:00:21,331
并非如上一个视频中的
调优使其更好地生成数据 

6
00:00:21,331 --> 00:00:26,880
而是更好地区分不同类别 

7
00:00:26,880 --> 00:00:32,320
这个方法效果非常好
它引发了神经网络的一次巨大复兴 

8
00:00:33,440 --> 00:00:39,489
在语音识别中它具有相当的影响力

9
00:00:39,489 --> 00:00:45,463
许多主要的研究组现在转而
使用深度神经网络来降低错误率 

10
00:00:45,463 --> 00:00:48,895
我现在想探讨一下让深度网络
更擅长判别的调优过程 

11
00:00:48,895 --> 00:00:52,390
更擅长判别的调优过程 

12
00:00:52,390 --> 00:00:57,598
首先从堆叠RBM开始逐层学习特征 

13
00:00:57,598 --> 00:01:01,817
该过程即为DBN网络
寻找一组好的初始权重 

14
00:01:01,817 --> 00:01:07,506
寻找一组好的初始权重
接下来我们 

15
00:01:07,506 --> 00:01:10,569
使用一些局部搜索过程进行调优 

16
00:01:10,569 --> 00:01:16,696
上一个视频中展示了如何使用 "wake-sleep" 对比调优 

17
00:01:16,696 --> 00:01:21,620
从而使深度网络生成输入的能力更强 

18
00:01:21,620 --> 00:01:27,120
这节视频中使用反向传播调优一个模型 

19
00:01:27,120 --> 00:01:31,691
令其在判别中做的更好
这样能够克服反向传播 

20
00:01:31,691 --> 00:01:38,636
反向传播的一些常规限制
深度网络的学习更容易了 

21
00:01:38,636 --> 00:01:41,990
泛化能力也更好 

22
00:01:41,990 --> 00:01:44,911
我们需要理解为什么预训练权重之后 

23
00:01:44,911 --> 00:01:48,420
为什么预训练权重之后要做反向传播 

24
00:01:48,420 --> 00:01:53,395
有两个效应 一个与优化有关 

25
00:01:53,395 --> 00:01:59,020
另一个与泛化有关
预训练在训练大型网络上 

26
00:01:59,020 --> 00:02:03,274
效果很好 尤其是当每一层都有局部性时 

27
00:02:03,274 --> 00:02:08,899
以视觉为例 每一层都有局部感受野 

28
00:02:08,899 --> 00:02:14,235
这样在分隔较远的区域之间就不会互相干涉了 

29
00:02:14,235 --> 00:02:19,139
并且 并行学习一个很大的层比较容易 

30
00:02:19,139 --> 00:02:23,310
预训练时 直到已经学得合适的 

31
00:02:23,310 --> 00:02:26,456
已经学得合适的特征检测器之后
才开始反向传播 

32
00:02:26,456 --> 00:02:30,952
这些特征检测器对于判别很有帮助 

33
00:02:30,952 --> 00:02:35,768
所以这样的初始梯度相对随机值更为合理 

34
00:02:35,768 --> 00:02:39,364
反向传播也不需要做全局搜索 

35
00:02:39,364 --> 00:02:43,603
而只需从一个合理的起点开始局部搜索即可 

36
00:02:43,603 --> 00:02:48,419
除了更容易优化 预训练网络的过拟合水平也更低 

37
00:02:48,419 --> 00:02:51,901
这是因为最终权重中的多数信息 

38
00:02:51,901 --> 00:02:56,550
来自对输入向量分布的建模 

39
00:02:56,550 --> 00:03:01,005
这些输入向量 比如图片 

40
00:03:01,005 --> 00:03:04,427
显然比单纯的标签包含了更多信息 

41
00:03:04,427 --> 00:03:09,269
标签仅仅带有很少的比特 

42
00:03:09,269 --> 00:03:13,401
已经学得合适的表示从输入到输出的映射 

43
00:03:13,401 --> 00:03:18,115
然而图片携带了大量信息
可以构建一组图片的生成模型 

44
00:03:18,115 --> 00:03:22,803
标签中的信息仅仅用来做最后的调优 

45
00:03:22,803 --> 00:03:27,183
因为此时我们已经确定了特征检测器 

46
00:03:27,183 --> 00:03:32,030
我们不再浪费之前的信息以设计特征检测器 

47
00:03:32,030 --> 00:03:37,890
调优仅仅对在生成预训练阶段学到的 

48
00:03:37,890 --> 00:03:43,248
预训练阶段学到的特征检测器做微调 这些微调是 

49
00:03:43,248 --> 00:03:46,216
这些微调是得到正确的类别边界所必需的 

50
00:03:46,216 --> 00:03:50,979
需要强调 反向传播不是用来发现新特征的 

51
00:03:50,979 --> 00:03:55,120
发现新特征的 所以它不需要太多的标签数据 

52
00:03:55,120 --> 00:03:58,517
事实上 这种学习方式对于
大量不带标签的数据非常有效 

53
00:03:58,517 --> 00:04:03,341
因为生成性预训练可以很好地利用这些数据 

54
00:04:03,341 --> 00:04:07,650
不带标签的数据对于发现好的特征仍然非常有用 

55
00:04:07,650 --> 00:04:12,024
仍然非常有用 对于这种学习方法 

56
00:04:12,024 --> 00:04:16,270
一种常见的异议是 生成性预训练 

57
00:04:16,270 --> 00:04:20,511
生成性预训练学到了大量
与判别任务无关的特征 

58
00:04:20,511 --> 00:04:23,020
这对我们想让网络做的事情没有用处 

59
00:04:23,020 --> 00:04:27,382
比如你想让网络区分不同的形状 

60
00:04:27,382 --> 00:04:31,922
或者区分一种形状的不同姿态 

61
00:04:31,922 --> 00:04:35,692
它们需要迥然不同的特征 

62
00:04:35,692 --> 00:04:41,129
如果你提前并不知道任务是什么
你会不可避免地学到 

63
00:04:41,129 --> 00:04:44,314
学到一些永远用不上的特征
电脑性能差的时候 

64
00:04:44,314 --> 00:04:48,603
这是很严重的问题
但现在的电脑性能足够强大 

65
00:04:48,603 --> 00:04:51,982
我们能够承受学习多余特征的代价 

66
00:04:51,982 --> 00:04:57,181
因为在我们学到的所有特征中 

67
00:04:57,181 --> 00:05:00,301
总有一些特征远比
它们的原始输入有用 

68
00:05:00,301 --> 00:05:05,565
这在很大程度上弥补了 

69
00:05:05,565 --> 00:05:09,400
学到一些无关特征的问题 

70
00:05:09,400 --> 00:05:13,125
来看看对 MNIST 数据集的应用情况 

71
00:05:13,125 --> 00:05:18,600
现在完全非监督地学习了三个特征隐层 

72
00:05:18,600 --> 00:05:23,020
一旦完成了学习 从模型中就能够生成 

73
00:05:23,020 --> 00:05:26,089
看上去像真的数字的图像 

74
00:05:26,089 --> 00:05:29,399
它能从所有不同的类别中生成 

75
00:05:29,399 --> 00:05:34,093
从一类换到另一类 

76
00:05:34,093 --> 00:05:38,907
只需要一会儿 

77
00:05:38,907 --> 00:05:43,541
因为它从一个"山谷"跳向另一个时 

78
00:05:43,541 --> 00:05:46,670
需要停留一段时间 问题在于 

79
00:05:46,670 --> 00:05:50,281
这些特征对于判别任务有用处吗？ 

80
00:05:50,281 --> 00:05:54,072
只需要添加在最后添加
一个十路softmax层 

81
00:05:54,072 --> 00:05:58,827
然后用反向传播进行调优即可 

82
00:05:58,827 --> 00:06:03,173
看看是不是比单纯的判别性训练有效 

83
00:06:03,173 --> 00:06:07,386
这是置换不变的MNIST任务
置换不变的意思是 

84
00:06:07,386 --> 00:06:12,535
如果对所有像素做固定的随机置换 

85
00:06:12,535 --> 00:06:17,417
包括所有的测试和训练样例 

86
00:06:17,417 --> 00:06:21,028
算法的结果不会改变
这对于卷积网络显然是不成立的 

87
00:06:21,028 --> 00:06:25,627
如果对卷积网络使用这种置换 

88
00:06:25,627 --> 00:06:29,320
就毁掉了所有卷积网络学到的 

89
00:06:29,320 --> 00:06:34,965
与任务相关的空间特性 

90
00:06:34,965 --> 00:06:37,683
使用一般的反向传播 

91
00:06:37,683 --> 00:06:42,370
错误率很难低于 1.6% 

92
00:06:42,370 --> 00:06:47,308
John Platt 和我都艰难尝试过
对许多不同的网络结构 

93
00:06:47,308 --> 00:06:50,669
使用标准的反向传播算法 

94
00:06:50,669 --> 00:06:55,060
我们都很擅长这件事
事实上 

95
00:06:55,060 --> 00:07:00,289
通过限制隐层权重向量的长度
你可以超过 1.6%的结果 

96
00:07:00,289 --> 00:07:06,173
这样的限制会使结果稍好于 1.6% 

97
00:07:06,173 --> 00:07:12,309
支持向量机可以做到 1.4% 

98
00:07:12,309 --> 00:07:15,719
而这也是支持向量机 

99
00:07:15,719 --> 00:07:21,333
支持向量机取代反向传播算法的理由之一 

100
00:07:21,333 --> 00:07:26,037
如果你使用堆叠玻尔兹曼机
预训练一个网络 

101
00:07:26,037 --> 00:07:32,182
然后调优让它更好地
生成数字和标签的联合密度 

102
00:07:32,182 --> 00:07:35,748
这样你可以降到 1.25% 

103
00:07:35,748 --> 00:07:40,831
如果你训练堆叠玻尔兹曼机
接着在顶部 

104
00:07:40,831 --> 00:07:45,080
放置一个十路softmax输出 然后调优
你会得到 1.15% 

105
00:07:45,080 --> 00:07:49,519
再做一些微调 可以降到 1% 

106
00:07:49,519 --> 00:07:53,480
所以这比使用标准的反向传播好得多 

107
00:07:53,480 --> 00:07:59,026
也比 SVM 好 

108
00:07:59,026 --> 00:08:04,286
Yan LeCun 组的 Mackerie Yerenzato 表明 

109
00:08:04,286 --> 00:08:09,772
使用一种稍微不同的预训练方法 

110
00:08:09,772 --> 00:08:15,112
能让模型有
更多的数据和更好的先验 

111
00:08:15,112 --> 00:08:19,428
他们使用了另外六万张失真的数字图片 

112
00:08:19,428 --> 00:08:24,987
这样训练数据就多了很多
他们还使用了卷积多线性网络 

113
00:08:24,987 --> 00:08:28,279
Yan的组是调制这类网络最好的组 

114
00:08:28,279 --> 00:08:32,188
使用反向传播 他们把错误率降到了 0.49% 

115
00:08:32,188 --> 00:08:36,471
他们做非监督逐层预训练然后调优的时候 

116
00:08:36,471 --> 00:08:41,765
错误率降到了 0.39% 

117
00:08:41,765 --> 00:08:47,449
当时这是一项纪录 所以你可能记得 

118
00:08:47,449 --> 00:08:52,152
这张在第一节课中出现过的图 

119
00:08:52,152 --> 00:08:56,013
这是我给出的一系列神经网络的例子 

120
00:08:56,013 --> 00:09:01,764
当时我说我们通过预训练+反向传播调优 

121
00:09:01,764 --> 00:09:07,988
把错误率降低了 20.7% 在此之前 

122
00:09:07,988 --> 00:09:14,291
TIMIT 的记录是 24.4%
他们实际上 

123
00:09:14,291 --> 00:09:18,072
采用了把多个模型平均的方法
微软研究院的 Ding Li 

124
00:09:18,072 --> 00:09:23,540
继续合作改进了这一结果 

125
00:09:23,540 --> 00:09:27,222
这引发了语音识别领域的巨大变革 

126
00:09:27,222 --> 00:09:32,243
如果你看到这些新闻 

127
00:09:32,243 --> 00:09:37,532
你可能会想到微软研究主管
在博客中讨论这些 

128
00:09:37,532 --> 00:09:40,545
在博客中讨论 这些深度学习在语音识别领域
引发的巨大进步