这段视频中 我们来看如何先用 堆叠受限玻尔兹曼机的方式
学习一个深度信念网络 然后将其视为一个 可以进行判别性微调的深度神经网络 并非如上一个视频中的
调优使其更好地生成数据 而是更好地区分不同类别 这个方法效果非常好
它引发了神经网络的一次巨大复兴 在语音识别中它具有相当的影响力 许多主要的研究组现在转而
使用深度神经网络来降低错误率 我现在想探讨一下让深度网络
更擅长判别的调优过程 更擅长判别的调优过程 首先从堆叠RBM开始逐层学习特征 该过程即为DBN网络
寻找一组好的初始权重 寻找一组好的初始权重
接下来我们 使用一些局部搜索过程进行调优 上一个视频中展示了如何使用 "wake-sleep" 对比调优 从而使深度网络生成输入的能力更强 这节视频中使用反向传播调优一个模型 令其在判别中做的更好
这样能够克服反向传播 反向传播的一些常规限制
深度网络的学习更容易了 泛化能力也更好 我们需要理解为什么预训练权重之后 为什么预训练权重之后要做反向传播 有两个效应 一个与优化有关 另一个与泛化有关
预训练在训练大型网络上 效果很好 尤其是当每一层都有局部性时 以视觉为例 每一层都有局部感受野 这样在分隔较远的区域之间就不会互相干涉了 并且 并行学习一个很大的层比较容易 预训练时 直到已经学得合适的 已经学得合适的特征检测器之后
才开始反向传播 这些特征检测器对于判别很有帮助 所以这样的初始梯度相对随机值更为合理 反向传播也不需要做全局搜索 而只需从一个合理的起点开始局部搜索即可 除了更容易优化 预训练网络的过拟合水平也更低 这是因为最终权重中的多数信息 来自对输入向量分布的建模 这些输入向量 比如图片 显然比单纯的标签包含了更多信息 标签仅仅带有很少的比特 已经学得合适的表示从输入到输出的映射 然而图片携带了大量信息
可以构建一组图片的生成模型 标签中的信息仅仅用来做最后的调优 因为此时我们已经确定了特征检测器 我们不再浪费之前的信息以设计特征检测器 调优仅仅对在生成预训练阶段学到的 预训练阶段学到的特征检测器做微调 这些微调是 这些微调是得到正确的类别边界所必需的 需要强调 反向传播不是用来发现新特征的 发现新特征的 所以它不需要太多的标签数据 事实上 这种学习方式对于
大量不带标签的数据非常有效 因为生成性预训练可以很好地利用这些数据 不带标签的数据对于发现好的特征仍然非常有用 仍然非常有用 对于这种学习方法 一种常见的异议是 生成性预训练 生成性预训练学到了大量
与判别任务无关的特征 这对我们想让网络做的事情没有用处 比如你想让网络区分不同的形状 或者区分一种形状的不同姿态 它们需要迥然不同的特征 如果你提前并不知道任务是什么
你会不可避免地学到 学到一些永远用不上的特征
电脑性能差的时候 这是很严重的问题
但现在的电脑性能足够强大 我们能够承受学习多余特征的代价 因为在我们学到的所有特征中 总有一些特征远比
它们的原始输入有用 这在很大程度上弥补了 学到一些无关特征的问题 来看看对 MNIST 数据集的应用情况 现在完全非监督地学习了三个特征隐层 一旦完成了学习 从模型中就能够生成 看上去像真的数字的图像 它能从所有不同的类别中生成 从一类换到另一类 只需要一会儿 因为它从一个"山谷"跳向另一个时 需要停留一段时间 问题在于 这些特征对于判别任务有用处吗？ 只需要添加在最后添加
一个十路softmax层 然后用反向传播进行调优即可 看看是不是比单纯的判别性训练有效 这是置换不变的MNIST任务
置换不变的意思是 如果对所有像素做固定的随机置换 包括所有的测试和训练样例 算法的结果不会改变
这对于卷积网络显然是不成立的 如果对卷积网络使用这种置换 就毁掉了所有卷积网络学到的 与任务相关的空间特性 使用一般的反向传播 错误率很难低于 1.6% John Platt 和我都艰难尝试过
对许多不同的网络结构 使用标准的反向传播算法 我们都很擅长这件事
事实上 通过限制隐层权重向量的长度
你可以超过 1.6%的结果 这样的限制会使结果稍好于 1.6% 支持向量机可以做到 1.4% 而这也是支持向量机 支持向量机取代反向传播算法的理由之一 如果你使用堆叠玻尔兹曼机
预训练一个网络 然后调优让它更好地
生成数字和标签的联合密度 这样你可以降到 1.25% 如果你训练堆叠玻尔兹曼机
接着在顶部 放置一个十路softmax输出 然后调优
你会得到 1.15% 再做一些微调 可以降到 1% 所以这比使用标准的反向传播好得多 也比 SVM 好 Yan LeCun 组的 Mackerie Yerenzato 表明 使用一种稍微不同的预训练方法 能让模型有
更多的数据和更好的先验 他们使用了另外六万张失真的数字图片 这样训练数据就多了很多
他们还使用了卷积多线性网络 Yan的组是调制这类网络最好的组 使用反向传播 他们把错误率降到了 0.49% 他们做非监督逐层预训练然后调优的时候 错误率降到了 0.39% 当时这是一项纪录 所以你可能记得 这张在第一节课中出现过的图 这是我给出的一系列神经网络的例子 当时我说我们通过预训练+反向传播调优 把错误率降低了 20.7% 在此之前 TIMIT 的记录是 24.4%
他们实际上 采用了把多个模型平均的方法
微软研究院的 Ding Li 继续合作改进了这一结果 这引发了语音识别领域的巨大变革 如果你看到这些新闻 你可能会想到微软研究主管
在博客中讨论这些 在博客中讨论 这些深度学习在语音识别领域
引发的巨大进步