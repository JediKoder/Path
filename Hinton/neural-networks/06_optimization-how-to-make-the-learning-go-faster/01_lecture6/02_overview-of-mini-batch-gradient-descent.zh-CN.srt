1
00:00:00,000 --> 00:00:05,108
在这个视频中 我们将考察
用在神经网络中的随机梯度下降算法

2
00:00:05,108 --> 00:00:08,794
特别是一种被称为小批量（mini batch）的版本，

3
00:00:08,794 --> 00:00:13,838
它可能是大型神经网络系统中用的非常广泛的一种算法

4
00:00:13,838 --> 00:00:18,753
作为回顾，我们重温一下

5
00:00:18,753 --> 00:00:23,797
线性神经元的误差表面（error surface）的形状
误差表面是空间中这样一个曲面

6
00:00:23,797 --> 00:00:29,100
它的横轴对应神经网络的权重

7
00:00:29,100 --> 00:00:32,010
纵轴对应

8
00:00:32,010 --> 00:00:35,741
网络的误差
对于一个均方误差线性神经元

9
00:00:35,741 --> 00:00:38,710
它的误差曲面是一个二次的“碗”状面

10
00:00:38,710 --> 00:00:44,435
纵切面是抛物线
横切面

11
00:00:44,435 --> 00:00:47,882
椭圆
多层非线性网络的

12
00:00:47,882 --> 00:00:52,435
误差曲面要复杂地多
但只要权重不是太大

13
00:00:52,435 --> 00:00:57,481
它仍然是一个平滑的曲面
在局部仍然可以用一个“碗”状曲面的一部分来近似

14
00:00:57,481 --> 00:01:01,234
可能不是“碗”底的部分，但是

15
00:01:01,234 --> 00:01:05,972
一定会有一部分
能够很好的拟合（多层神经网络的误差）表面

16
00:01:05,972 --> 00:01:09,390
再看一下收敛速度

17
00:01:09,390 --> 00:01:13,771
当我们做全量（full batch）学习时，
如果误差曲面呈二次球面状

18
00:01:13,771 --> 00:01:18,023
很明显，我们要向下走
这样能减少误差

19
00:01:18,023 --> 00:01:23,177
问题是，梯度最大的方向
并不指向

20
00:01:23,177 --> 00:01:26,850
我们想去的地方
比如图上这个椭圆

21
00:01:26,850 --> 00:01:32,326
梯度最大的方向几乎
都与我们想去的方向垂直

22
00:01:32,326 --> 00:01:34,844
在沿着椭圆梯度较大的方向上

23
00:01:34,844 --> 00:01:39,675
在该方向上我们想前进较小距离

24
00:01:39,675 --> 00:01:44,079
椭圆上梯度较小的方向

25
00:01:44,079 --> 00:01:47,198
则是我们想前进较大距离的方向

26
00:01:47,198 --> 00:01:52,082
这样做很不正确。
你可能会认为要学习这样一个线性系统

27
00:01:52,082 --> 00:01:57,579
对一个非线性网络进行优化绝对不是一个好主意

28
00:01:57,579 --> 00:02:02,552
即便是对于非线性多层网络，
这类问题还是会出现。

29
00:02:02,552 --> 00:02:07,656
同样的问题也会出现，对于误差表面不是

30
00:02:07,656 --> 00:02:11,974
全局二次"碗"状
在局部他们都具有相同的

31
00:02:11,974 --> 00:02:15,246
属性
那就是都有倾向于某一个方向上有很大程度的弯曲

32
00:02:15,246 --> 00:02:18,780
而在另一些方向上显得很平缓

33
00:02:19,480 --> 00:02:25,725
一种导致学习失败的做法是，
选择的步长过大

34
00:02:25,725 --> 00:02:30,930
导致在曲率大的地方学习发生来回摆动

35
00:02:30,930 --> 00:02:34,288
把这种情况叫“山涧跳”

36
00:02:34,288 --> 00:02:38,137
当步长太大时往往导致发散

37
00:02:38,137 --> 00:02:43,525
我们期望的是，当在“山涧”上梯度小而稳定的地方

38
00:02:43,525 --> 00:02:46,744
我们快走

39
00:02:46,744 --> 00:02:51,642
在梯度大且不稳定的地方，我们慢走

40
00:02:51,642 --> 00:02:55,281
在这个方向上，

41
00:02:55,281 --> 00:02:58,500
移动一小步，梯度会改变符号

42
00:03:00,040 --> 00:03:05,196
在讲如何实现它之前，先简单介绍一下

43
00:03:05,196 --> 00:03:08,240
什么是随机梯度下降，以及为什么要使用它

44
00:03:08,540 --> 00:03:12,843
针对非常冗余的数据集，

45
00:03:12,843 --> 00:03:17,814
利用前半部分数据计算出来的某个权重的梯度

46
00:03:17,814 --> 00:03:22,300
几乎与用后一半数据计算出来的结果相差无几

47
00:03:22,300 --> 00:03:26,681
所以，在全部数据上进行梯度计算是对计算资源的一种浪费

48
00:03:26,681 --> 00:03:29,194
较好的做法是

49
00:03:29,194 --> 00:03:33,868
在一部分数据集上进行计算梯度、更新权重，而在余下的数据上

50
00:03:33,868 --> 00:03:36,906
针对更新后的权重进行梯度计算

51
00:03:36,906 --> 00:03:41,872
更极端一点，
可以在一个样本数据上进行梯度计算、

52
00:03:41,872 --> 00:03:46,546
权重更新，
然后在另一个样本上

53
00:03:46,546 --> 00:03:50,227
基于更新后的权重进行梯度计算

54
00:03:50,227 --> 00:03:55,397
这种做法也被称作在线学习
通常不采用这中极端的做法

55
00:03:55,397 --> 00:03:58,815
比较好的做法小批量，

56
00:03:58,815 --> 00:04:04,787
比如包含100 或者
1000个样本。 一个好处

57
00:04:04,787 --> 00:04:09,710
是可以用较少的计算资源进行梯度计算

58
00:04:09,710 --> 00:04:12,740
因为跟在线学习相比，计算的次数大大减少

59
00:04:12,740 --> 00:04:17,998
另一个优势体现在，
梯度计算时的

60
00:04:17,998 --> 00:04:22,762
并行计算能力
计算机非常擅长

61
00:04:22,762 --> 00:04:27,587
矩阵和矩阵的乘法
这方便你

62
00:04:27,587 --> 00:04:32,784
把一批权重应用到一批样本中
同时

63
00:04:32,784 --> 00:04:37,548
进行这些样本在下一层的活动

64
00:04:37,548 --> 00:04:40,950
这是一个矩阵乘法的问题

65
00:04:40,950 --> 00:04:44,910
（计算机）处理起来非常有效，尤其是
配备有GPU的计算机

66
00:04:44,910 --> 00:04:50,489
小批量使用，需要注意的一个地方是
尽量避免

67
00:04:50,489 --> 00:04:55,719
在一个上面的结果都相同，
而在另一个结果上

68
00:04:55,719 --> 00:05:01,158
都有一样的不同值
这会引起权重的不必要的晃动

69
00:05:01,158 --> 00:05:04,785
理想情况是，假如有10个类

70
00:05:04,785 --> 00:05:10,713
一个小批量包含10个或者100个样本

71
00:05:10,713 --> 00:05:14,200
希望这10类中的数据量基本相同

72
00:05:14,200 --> 00:05:19,374
有一种简单的近似做法
取出所有数据

73
00:05:19,374 --> 00:05:24,679
把他们随机组合，从中随机抽取小批量
但要避免抽出的小批量

74
00:05:24,679 --> 00:05:29,984
因为是从一个类别中取出的数据，
而对整体数据失去代表性

75
00:05:29,984 --> 00:05:34,755
简单的说了，神经网络有两种类型的学习算法

76
00:05:34,755 --> 00:05:39,045
一种是完全梯度学习，

77
00:05:39,045 --> 00:05:42,685
这种方法是使用全部的数据进行梯度的计算

78
00:05:42,685 --> 00:05:47,625
对于这种算法，
有很多做法可以用以加速学习过程

79
00:05:47,625 --> 00:05:51,525
还有一种非线性版本的变形叫做共轭梯度

80
00:05:51,525 --> 00:05:56,465
优化理论社区一直在研究

81
00:05:56,465 --> 00:05:59,780
针对平滑非线性函数的通用优化方法

82
00:05:59,780 --> 00:06:04,690
现在的多层神经网络的问题跟优化研究的内容非常不同

83
00:06:04,690 --> 00:06:07,589
要想使用优化理论的一些方法

84
00:06:07,589 --> 00:06:11,967
必须对其进行改进
以使其对多层神经网络有效

85
00:06:11,967 --> 00:06:15,209
但是，如果训练样本很大，并且包含大量冗余

86
00:06:15,209 --> 00:06:20,280
通常比较好的做法是使用小批量学习

87
00:06:20,560 --> 00:06:26,400
包含在小批量的样本数目有时候需要非常大，
但那并不太糟糕，因为

88
00:06:26,400 --> 00:06:28,760
大的小批量对计算来说更有效

89
00:06:30,380 --> 00:06:34,753
在来介绍一种基本的小批量梯度下降线性算法

90
00:06:34,753 --> 00:06:37,711
这是我们大多数人

91
00:06:37,711 --> 00:06:42,020
在包含大量冗余数据集上训练大规模网络时所常用的一种方法

92
00:06:42,020 --> 00:06:45,220
先选择初始学习步长

93
00:06:45,220 --> 00:06:50,555
观察网络是否输出满意的结果，
或者误差持续地

94
00:06:50,555 --> 00:06:55,356
变坏，震动的很剧烈
当发生这种情况时，减少学习步长

95
00:06:55,356 --> 00:06:58,125
同时也要看误差

96
00:06:58,125 --> 00:07:02,142
是否降地太慢。
要做好准备，误差可能会

97
00:07:02,142 --> 00:07:06,948
在验证集上有抖动
因为小批量梯度下降

98
00:07:06,948 --> 00:07:10,964
只是完全梯度的一种近似

99
00:07:10,964 --> 00:07:15,836
还有，不要一看到误差上升就着急减少学习步长

100
00:07:15,836 --> 00:07:20,510
要期望的是误差在总体趋势上是下降的

101
00:07:20,510 --> 00:07:25,974
如果从总体上呈下降趋势，
但是非常缓慢，你则要提高

102
00:07:25,974 --> 00:07:30,227
学习步长
如果有可能，你可以

103
00:07:30,227 --> 00:07:34,740
写一个简单的程序来自动化地
调整学习步长

104
00:07:35,000 --> 00:07:39,149
有一个很有帮助的做法是，
在小批量学习结束的时候

105
00:07:39,149 --> 00:07:42,246
减少学习步长

106
00:07:42,246 --> 00:07:46,570
这时因为
权重会随着

107
00:07:46,570 --> 00:07:50,194
梯度计算带来的抖动而抖动

108
00:07:50,194 --> 00:07:53,934
我们总希望得道一个理想的权重

109
00:07:53,934 --> 00:07:58,667
当降低学习率的时
震动可以得道有效的缓解

110
00:07:58,667 --> 00:08:02,700
从而得道一个对大多数小批量都有益的权重

111
00:08:03,240 --> 00:08:08,006
另一个调低学习步长的时机
是误差停止降低

112
00:08:08,006 --> 00:08:11,243
判断误差停止下降的标准

113
00:08:11,243 --> 00:08:15,245
则体现在验证集上

114
00:08:15,245 --> 00:08:19,835
验证集是指一些数据，既不用来作为模型训练的样本数据，

115
00:08:19,835 --> 00:08:22,543
也不用来评估模型性能的测试数据。
翻译：张晓刚