在这个视频中 我们将考察
用在神经网络中的随机梯度下降算法 特别是一种被称为小批量（mini batch）的版本， 它可能是大型神经网络系统中用的非常广泛的一种算法 作为回顾，我们重温一下 线性神经元的误差表面（error surface）的形状
误差表面是空间中这样一个曲面 它的横轴对应神经网络的权重 纵轴对应 网络的误差
对于一个均方误差线性神经元 它的误差曲面是一个二次的“碗”状面 纵切面是抛物线
横切面 椭圆
多层非线性网络的 误差曲面要复杂地多
但只要权重不是太大 它仍然是一个平滑的曲面
在局部仍然可以用一个“碗”状曲面的一部分来近似 可能不是“碗”底的部分，但是 一定会有一部分
能够很好的拟合（多层神经网络的误差）表面 再看一下收敛速度 当我们做全量（full batch）学习时，
如果误差曲面呈二次球面状 很明显，我们要向下走
这样能减少误差 问题是，梯度最大的方向
并不指向 我们想去的地方
比如图上这个椭圆 梯度最大的方向几乎
都与我们想去的方向垂直 在沿着椭圆梯度较大的方向上 在该方向上我们想前进较小距离 椭圆上梯度较小的方向 则是我们想前进较大距离的方向 这样做很不正确。
你可能会认为要学习这样一个线性系统 对一个非线性网络进行优化绝对不是一个好主意 即便是对于非线性多层网络，
这类问题还是会出现。 同样的问题也会出现，对于误差表面不是 全局二次"碗"状
在局部他们都具有相同的 属性
那就是都有倾向于某一个方向上有很大程度的弯曲 而在另一些方向上显得很平缓 一种导致学习失败的做法是，
选择的步长过大 导致在曲率大的地方学习发生来回摆动 把这种情况叫“山涧跳” 当步长太大时往往导致发散 我们期望的是，当在“山涧”上梯度小而稳定的地方 我们快走 在梯度大且不稳定的地方，我们慢走 在这个方向上， 移动一小步，梯度会改变符号 在讲如何实现它之前，先简单介绍一下 什么是随机梯度下降，以及为什么要使用它 针对非常冗余的数据集， 利用前半部分数据计算出来的某个权重的梯度 几乎与用后一半数据计算出来的结果相差无几 所以，在全部数据上进行梯度计算是对计算资源的一种浪费 较好的做法是 在一部分数据集上进行计算梯度、更新权重，而在余下的数据上 针对更新后的权重进行梯度计算 更极端一点，
可以在一个样本数据上进行梯度计算、 权重更新，
然后在另一个样本上 基于更新后的权重进行梯度计算 这种做法也被称作在线学习
通常不采用这中极端的做法 比较好的做法小批量， 比如包含100 或者
1000个样本。 一个好处 是可以用较少的计算资源进行梯度计算 因为跟在线学习相比，计算的次数大大减少 另一个优势体现在，
梯度计算时的 并行计算能力
计算机非常擅长 矩阵和矩阵的乘法
这方便你 把一批权重应用到一批样本中
同时 进行这些样本在下一层的活动 这是一个矩阵乘法的问题 （计算机）处理起来非常有效，尤其是
配备有GPU的计算机 小批量使用，需要注意的一个地方是
尽量避免 在一个上面的结果都相同，
而在另一个结果上 都有一样的不同值
这会引起权重的不必要的晃动 理想情况是，假如有10个类 一个小批量包含10个或者100个样本 希望这10类中的数据量基本相同 有一种简单的近似做法
取出所有数据 把他们随机组合，从中随机抽取小批量
但要避免抽出的小批量 因为是从一个类别中取出的数据，
而对整体数据失去代表性 简单的说了，神经网络有两种类型的学习算法 一种是完全梯度学习， 这种方法是使用全部的数据进行梯度的计算 对于这种算法，
有很多做法可以用以加速学习过程 还有一种非线性版本的变形叫做共轭梯度 优化理论社区一直在研究 针对平滑非线性函数的通用优化方法 现在的多层神经网络的问题跟优化研究的内容非常不同 要想使用优化理论的一些方法 必须对其进行改进
以使其对多层神经网络有效 但是，如果训练样本很大，并且包含大量冗余 通常比较好的做法是使用小批量学习 包含在小批量的样本数目有时候需要非常大，
但那并不太糟糕，因为 大的小批量对计算来说更有效 在来介绍一种基本的小批量梯度下降线性算法 这是我们大多数人 在包含大量冗余数据集上训练大规模网络时所常用的一种方法 先选择初始学习步长 观察网络是否输出满意的结果，
或者误差持续地 变坏，震动的很剧烈
当发生这种情况时，减少学习步长 同时也要看误差 是否降地太慢。
要做好准备，误差可能会 在验证集上有抖动
因为小批量梯度下降 只是完全梯度的一种近似 还有，不要一看到误差上升就着急减少学习步长 要期望的是误差在总体趋势上是下降的 如果从总体上呈下降趋势，
但是非常缓慢，你则要提高 学习步长
如果有可能，你可以 写一个简单的程序来自动化地
调整学习步长 有一个很有帮助的做法是，
在小批量学习结束的时候 减少学习步长 这时因为
权重会随着 梯度计算带来的抖动而抖动 我们总希望得道一个理想的权重 当降低学习率的时
震动可以得道有效的缓解 从而得道一个对大多数小批量都有益的权重 另一个调低学习步长的时机
是误差停止降低 判断误差停止下降的标准 则体现在验证集上 验证集是指一些数据，既不用来作为模型训练的样本数据， 也不用来评估模型性能的测试数据。
翻译：张晓刚