本视频将通过一个简单的抛硬币的例子
介绍用来拟合模型的贝叶斯方法 如果你已经了解过贝叶斯方法 你可以跳过这个视频
贝叶斯方法的主要思想是 不局限于考虑模型参数最可能的取值 我们考虑参数所有可能的取值 并找出每个取值在给定观测数据的条件下
有多大概率取到 贝叶斯框架假设我们对于任何事情 都有一个先验分布 也就是对于你任意感兴趣的事情
我都有某个先验的概率 这个概率刻画了这个事情发生的可能性
这个先验可能比较模糊 我们已有的数据给了我们一个似然项 我们把它和先验分布结合起来
就可以得到一个后验分布 似然项更青睐使得观测数据取得的概率
更高的参数 如果我们能够得到足够的数据
这个参数可能与先验信息不太一致 无论先验信息多么不靠谱
足够的数据都能在一定意义上纠正过来 并且最后 有了足够的数据
真实的参数取值就会被找到 也就说即使你的先验信息是错误的
你都有可能最终获得一个正确的假设 但是这可能会需要海量的数据
你可能会认为这不太可能 那么我们从一个抛硬币的例子开始 如果你只知道硬币可以被抛 然后抛完要么正面朝上要么反面朝上 如果我们还假设每次抛硬币 这些事件之间都是相互独立的 那么我们抛硬币的模型将有一个参数P P刻画了硬币正面朝上的概率 如果100次抛掷 有53次正面朝上
那么P的取值应该是多少呢 当然你要说P取值是0.53
那么你这么说的理由是什么呢 从频率学派的角度讲
这叫做最大似然估计 也就是找出使得能够得到观测数据最可能的p 然后这个p取值就是0.53
这个结论并不显然 我们推导一下 得到53次正面47次反面的概率可以这样表示
每次正面朝上就乘p 每次反面朝上就乘1-p 如果我们把所有p的项放在一起
把所有1-p的项也放在一起 那么就得到了p的53次方和1-p的47次方 如果问得到观测数据的概率是怎样依赖于p的
那么我们可以对p求导 然后得到这样一个表达式
然后我们另导函数为0 我们发现可以发现如果想最大化这个概率
就要令P取值为0.53 这就是极大似然方法
但是这里有个问题 就在我们使用极大似然
来决定一个模型的参数的时候 例如我们只抛掷一次硬币
然后我们得到了正面朝上 但是我们认为硬币正面朝上的概率 取值为1这显然是不合理的
不然这就意味着我们也认可了 硬币抛掷无数次都不可能出现反面朝上的情况 这看起来也太荒谬了
而且很直观的一个更好的猜测 就是p取值为0.5
但是我们要如何验证这一点呢 更重要的是 我们可以问
只求得一个答案这件事情本身是合理的吗 我们知道的不多
我们没有足够多的数据 所以我们也不能知道P的取值是多少
所以我们真正应该做的是 拒绝得到一个单一的答案
而是应该去得到一个概率分布 这个分布包含了所有可能的答案
一个像0.5这样的答案是很有可能的 一个像1这样的答案是不太可能的 如果我们先验地认为有一半的次数会正面朝上
现在我们要开始一个例子 我们从对参数的所有取值有一个先验分布出发 这里我们选择一个好处理的先验分布 不一定是一个完全符合我们先验认知的分布 然后我们会看到这个先验分布
将会被数据修正 如果我们采取贝叶斯方法的话
那么我们将从一个 所有p的取值概率都相同的分布开始 我们相信抛掷硬币存在偏差 或多或少的偏差都有可能存在
所以一些硬币一半的次数正面朝上 另一些硬币则是一直正面朝上 这两种硬币同等可能存在 我们现在观测到硬币正面朝上
那么我们要做的就是 对任意可能的取值p 找到其先验概率 然后乘以给定该取值时观测到正面朝上的概率 例如我们取P为1 则每次硬币都正面朝上 也就是观测到正面的概率是1 这时就不会出现其他可能
并且如果我们取P为0 那么观测到硬币正面朝上的
概率就是0 如果我们取P为0.5
那么观测到正面朝上的概率就是0.5 所以我们取一条红线 当作我们的先验分布
然后给每个点 乘以基于这个假设下观测到正面朝上的概率 然后我们就得到了一个倾斜的
未经归一化的后验分布 称其未归一化是因为直线下方面积不是1 然而对任意的概率分布
其所有可能事件的概率加起来应该是1 所以最后一步就是对后验分布归一化 我们按比例整理所有概率之后 曲线下方的面积就是1了
如果我们从均匀分布开始 就会得到一个三角形的分布 这是经过观测一边得到的 让我们再来一次
这次我们假设得到反面朝上 那么这时候我们的先验分布
就是刚才观测到正面朝上之后的后验分布 图中的绿色直线表示了 和P的取值相对应的假设下
我们得到硬币反面朝上的概率 例如如果P是1
那么我们能观测到反面朝上的概率就是0 我们再次用先验乘以似然 然后就得到了这样一个曲线 此外我们还要再次归一化后验分布
使得曲线下方的面积是1 那么这就是我们目前为止的后验分布了
这是在观测到一次正面一次反面后得到的 注意到这个分布是比较敏感的 在一次正面一次反面的观测后 P的取值既不是0也不是1
而且最可能的取值是在中间 如果我们持续做完剩下98次 并且持续使用同样的公式
也就是用上次试验后得到的后验分布作为 本次的先验分布
然后乘以P取不同值确定的似然函数 如果我们最后得到53次正面朝上
47次反面朝上 那最后得到的曲线就是这样的 这个分布的均值就是0.53 因为我们是从均匀分布开始的
所以最后曲线在0.53处有一个尖锐的峰值 但是同时也允许像0.49这样的值
也有一个比较合理的取值可能性 虽然不像0.53这样概率高
但也比较合理 但是像0.25这样的取值 在这个去曲线下就十分不可能了
我们可以用贝叶斯定理总结 等式中间这一项 是关于参数W和数据D的联合分布 对于监督式学习
数据将包括模型的输出值 所以我们这里假设
输入值和输入值分别对应的输出值 都是给定的
这也是我们观测到的数据 这个联合分布还可以重写为 关于单一变量的概率和条件概率的乘积
在右边写为W的概率乘以给定W下D的概率 左边写为W的概率乘以给定W下D的概率 在左右两边同时除以D的概率 这就得到了贝叶斯公式的一般形式 贝叶斯公式说明了特定取值的W
其给定D的后验概率 就是取值W的概率再乘以 给定W产生D的概率 并且这个概率要被D的概率归一化处理 数据D的概率可以通过对所有可能的W积分求得 被积函数是W的概率乘以给定W下D的概率 线下项的数值应该是
线上项所有W决定的数值的和 因为所有的概率加起来要是1
因为D的概率计算的时候对所有W进行了积分 所以这一项就与W具体的取值无关 所以当我们想求一个W使得后验概率最大 我们就可以忽略D的概率这一项
因为这一项和W无关 然而等式右边的其他两项都是依赖于W的
翻译: iChen | 审核:
Coursera Global Translator Community