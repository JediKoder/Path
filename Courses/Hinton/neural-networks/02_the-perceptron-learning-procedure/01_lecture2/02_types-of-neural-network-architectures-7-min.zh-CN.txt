在这个视频中 我将讲解不同种类的神经网络结构 所谓结构指的是 将神经元连接到一起的方式 迄今为止 实际应用中最常见的一种结构
是前馈神经网络 在该结构中 信息从输入层流入
沿着一个方向通过隐含层 直到输出层 另一种更有趣的结构是循环神经网络 RNN 信息在其中循环流动 这类网络可以长时间记忆信息 表现出各种有趣的振荡
但相对更难训练 因为比起它们的能力 其结构要更加复杂 不过最近人们在如何训练 RNN 上
取得了很大的进步 它们已经能够完成许多出色的任务了 最后一种结构是对称连接网络 该网络中的两个神经元之间 两个方向的权重是相同的 实际应用中 前馈神经网络是最常见的神经网络 如右图所示 第一层是输入单元 输出单元在上方最后一层
中间是一层或更多层的隐层单元 如果有多个隐层
就称为深度神经网络 这些网络计算输入和输出间的一系列变换 因此在每一层都能得到输入的新表示 前一层原本相似的地方 在后一层可能变得不同 而原本不同的地方也可能变得相似 比如在语音识别的例子中 我们希望通过网络后
不同的人说的同样的内容变得相似 而同一个人说的不同的内容差异变大 为了实现这一点 我们需要让每一层神经元信号的非线性输出 成为下一层神经元的输入 RNN 比前馈网络更强大  RNN 的连接图中包含了有向环路 这意味着如果从一个神经元开始
沿着箭头移动 有时候可能又回到了开始的神经元 RNN 参数的动态变化非常复杂 这让它们很难训练 现在有很多人在努力发现训练 RNN 的有效方法 因为如果可以有效训练 RNN 它们将发挥强大的性能 RNN 也更切合生物学 多隐层的 RNN
只是常规 RNN 的特例 只是少了隐层单元之间的相互连接 RNN 是对序列数据非常自然的建模方式 我们要做的是在隐层单元之间建立连接 因此这些隐层单元表现为在时序上很深的网络 在每个时间步 隐层单元的当前状态
决定了它下个时序的状态 不同于前馈网络 RNN 在每个时间步都使用相同的权重 看看那些红色箭头 隐层单元决定了它的下一个状态 红色箭头指示的权重矩阵
在每一步都是一样的 它们在每个时间戳获取输入
同时产生输出 而且使用相同的权重矩阵 RNN 具有在隐藏单元中 长时间记忆信息的能力 但训练其使用这种能力却很难 不过最近的算法已经能够实现训练了 现在要给你展现一下 RNN 的能力 这是一个 Ilya Sutskever 设计的 RNN 和上一张幻灯片中的 RNN 略有不同 它被用来预测一个序列中的下一个字符 Ilya 用了很多英文维基中的字符串来训练 它会看到一串英文字符 并试图预测下一个英文字符 Ilya 实际使用了86个不同的字符
包括标点符号 数字 大写字母等等 训练完成后 一种评估的方法是
看它是否会给实际出现的字符 分配较高的概率值 另一方法是看它能产生出什么样的文本 所以 你要做的就是给它一个字符串 让它预测下一个字符出现的概率 然后从它给处的概率分布中挑选出下一个字符 直接挑选概率最高的是没用的 如果这样做 过一会它就会开始说 the United States of the United States of the United
States 但是如果从概率分布中挑的话 比如 它说有1/100的机率是个Z
那么在100次中 只挑1次Z 这样才能看出它学到了多少 下一张幻灯片展示了它生成的一个文本的例子 只让它阅读维基百科来预测字符
看它学到了多少 请记住这段文字是逐字符生成的 它产生出了合理有意义的句子 而且都是由真正的英语单词组合而成 有时它会产生不正确的单词
但通常是有意义的 同时注意到 在句子中还包含有一些主题句 比如这句话
Severa Irish intelligence agents is in the Mediterranean region 虽然有些问题 但几乎是不错的英语了 还要注意最后这部分
such that it is the blurring of appearing on any well-paid type of box printer 有些关于外观 印刷的主题
而且语法相当不错 请记住 这都是逐个字符产生的 对称连接网络和 RNN 完全不同 在对称连接网络中
单元间的连接在两个方向权重相同 John Hopfield 和其他学者认识到 对称网络比 RNN 容易分析得多 原因在于对称网络服从的能量函数
制约了他们可以做的事 比如 不能建模环路 即在对称网络中 你无法回到起点