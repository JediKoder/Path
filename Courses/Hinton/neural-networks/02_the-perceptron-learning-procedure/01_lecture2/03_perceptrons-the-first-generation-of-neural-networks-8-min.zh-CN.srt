1
00:00:00,000 --> 00:00:04,075
在这一讲的视频中 我将讨论
感知器

2
00:00:05,050 --> 00:00:10,014
感知器是在1960年代早期发明的
起初 他们作为学习机

3
00:00:10,014 --> 00:00:14,053
看起来非常具有说服力
但是之后他们不再被偏爱

4
00:00:14,053 --> 00:00:19,073
因为 Minsky 和 Papert 指出
他们能做的十分有限

5
00:00:21,005 --> 00:00:26,055
在统计模式识别中
有统计的方法来识别

6
00:00:26,055 --> 00:00:31,082
模式
我们首先读取原始输入

7
00:00:31,082 --> 00:00:37,046
然后我们将它转化为一组特征激活或
特征激活向量

8
00:00:38,052 --> 00:00:43,006
我们基于常识
用手写的程序来做到这一点

9
00:00:43,006 --> 00:00:48,044
所以部分系统并不进行学习
我们分析问题并决定

10
00:00:48,044 --> 00:00:52,060
怎样的特征是好的
我们尝试一些特征看它们是否起作用

11
00:00:52,060 --> 00:00:57,086
如果不 我们就尝试更多的特征
最终会有一组特征

12
00:00:57,086 --> 00:01:01,096
通过使用一系列的学习阶段
让我们解决这个问题

13
00:01:03,032 --> 00:01:09,006
我们学习了如何设置每个特征激活的权重
为了得到一个单一的标量

14
00:01:09,006 --> 00:01:14,016
因此每组特征的权重代表了

15
00:01:14,016 --> 00:01:19,083
它有多明显
它是赞同还是反对你对于当前输入

16
00:01:19,083 --> 00:01:25,029
是你想要学习的模式的假设

17
00:01:25,029 --> 00:01:30,075
然后我们将所有的权重加起来
我们得到了

18
00:01:30,075 --> 00:01:36,050
赞同这是我们想学习的
模式的总的明显程度

19
00:01:36,081 --> 00:01:42,056
如果那个明显程度大于特定的阈值
我们决定那个输入向量

20
00:01:42,056 --> 00:01:48,001
是我们尝试学习的模式的
正面的例子

21
00:01:48,001 --> 00:01:54,006
感知器是一个统计学模式识别系统的
典型例子

22
00:01:54,095 --> 00:02:00,041
所以事实上有很多不同种类的感知器
但是被 Rosenblatt 叫做 α 感知器 的

23
00:02:00,041 --> 00:02:05,045
标准类型感知器包含一些输入

24
00:02:05,045 --> 00:02:10,098
这些输入将被转化成未来的活动
他们可能会被转化为

25
00:02:10,098 --> 00:02:15,062
看起来像神经元的东西
但是系统这一个层面并不会学习

26
00:02:15,091 --> 00:02:20,088
一旦你获得了这些特征的活性值
然后你学习到了一些权重值

27
00:02:20,088 --> 00:02:26,009
因此 你将特征活性值乘上权重值
然后你来决定它是否

28
00:02:26,009 --> 00:02:31,043
是一个你感兴趣的分类的例子
通过判断

29
00:02:31,043 --> 00:02:36,002
特征活性值乘权重值的总和
是否比阈值大

30
00:02:38,059 --> 00:02:43,050
感知器的历史十分有趣
他们在1960年代早期因为Frank Rosenblatt

31
00:02:43,050 --> 00:02:46,072
开始流行  
他写了一本很厚的书叫做

32
00:02:46,072 --> 00:02:51,020
神经动力学原理
在书中他描述了很多不同种类的

33
00:02:51,020 --> 00:02:53,087
感知器 
并且书中充满了各种想法

34
00:02:53,087 --> 00:02:58,065
书中最重要的东西
是一个非常强大的学习算法

35
00:02:58,065 --> 00:03:02,059
或者某个看起来像一个
强大的学习算法的东西

36
00:03:03,047 --> 00:03:08,095
许多重要的言论说明了使用这个学习算法
感知器可以做到什么

37
00:03:08,095 --> 00:03:12,082
例如 人们说他们可以

38
00:03:12,082 --> 00:03:18,059
区分水箱和卡车的照片
即使

39
00:03:18,059 --> 00:03:22,093
水箱和卡车的一部分
隐藏在森林的后面

40
00:03:23,069 --> 00:03:26,065
现在部分言论被证明是错误的

41
00:03:26,065 --> 00:03:31,057
在水箱和卡车的例子中
事实是水箱的照片

42
00:03:31,057 --> 00:03:36,006
是在晴天拍摄的
卡车照片是在多云天气下拍摄的

43
00:03:36,006 --> 00:03:39,038
感知器做的所有事情只是

44
00:03:39,038 --> 00:03:44,005
统计所有像素的亮度
这是人类非常

45
00:03:44,005 --> 00:03:47,019
不敏感的事情
我们只会注意到图片中的物体

46
00:03:47,019 --> 00:03:51,012
但感知器可以简单的学会
把总体亮度加起来

47
00:03:51,012 --> 00:03:54,081
那是一个给算法取了一个不好的名字
的例子

48
00:03:56,037 --> 00:04:02,097
1969年 Minsky 和 Papert 出版了一本书
叫 感知器

49
00:04:02,097 --> 00:04:07,003
这本书分析了感知器可以做的事情
和他们的限制性

50
00:04:08,021 --> 00:04:13,006
许多人认为这些限制性
也适用于所有的神经网络模型

51
00:04:13,006 --> 00:04:18,027
并且对于人工智能的普遍认知
是 Minsky 和 Papert 所展现的

52
00:04:18,027 --> 00:04:23,030
神经网络模型是没有意义的
并且他们不能学习任何复杂的东西

53
00:04:23,030 --> 00:04:26,086
Minsky 和 Papert 他们自己知道

54
00:04:26,086 --> 00:04:31,000
他们并没有证明这些
他们只是展示了那种

55
00:04:31,000 --> 00:04:35,090
使用了强大学习算法的感知器
不能做很多的事情

56
00:04:35,090 --> 00:04:39,015
或者不能通过学习来完成他们

57
00:04:39,015 --> 00:04:43,086
如果你自己手动设置输入的答案 
感知器可以完成他们

58
00:04:43,086 --> 00:04:46,070
但不是通过学习 
但是结果变得

59
00:04:46,070 --> 00:04:50,095
过度泛化了 而且当我1970年代
开始研究神经网络模型时

60
00:04:50,095 --> 00:04:55,042
研究人工智能的人们
一直告诉我Minsky 和 Papert

61
00:04:55,042 --> 00:04:57,086
已经证明了这些模型是不好的

62
00:04:59,056 --> 00:05:04,060
事实上 我们等一下将要看到的
感知器的会聚过程

63
00:05:04,060 --> 00:05:09,012
至今在处理有很大特征向量的工作时
仍然被广泛的使用

64
00:05:09,012 --> 00:05:14,011
因此 例如谷歌
使用感知器来从很大的特征向量中

65
00:05:14,011 --> 00:05:19,018
预测东西
所以 一个感知器的决策单元是

66
00:05:19,018 --> 00:05:23,021
一个二元临界神经元
我们之前见过这个

67
00:05:23,021 --> 00:05:27,093
现在只是给你们复习一下
它们计算从其他神经元那里得到的

68
00:05:27,093 --> 00:05:32,035
输入的加权和
它们加上一个偏差值就得到了它们的总的

69
00:05:32,035 --> 00:05:35,062
输入值
然后它们给出值为'1'的输出

70
00:05:35,062 --> 00:05:40,025
如果总和超过'0'
如果情况相反 将输出值为'0'

71
00:05:42,019 --> 00:05:47,029
对于学习偏差我们不想有
不同的学习规则

72
00:05:47,029 --> 00:05:50,027
结果是我们可以把偏差看作是
权重值

73
00:05:50,027 --> 00:05:55,055
如果我们在得到每个输入向量后
加入一个'1'在它前面

74
00:05:55,055 --> 00:06:00,033
那么我们将偏差当做第一个取值
总是为'1'的特征的权重

75
00:06:00,033 --> 00:06:03,043
所以偏差只是极限值的
基础值

76
00:06:03,095 --> 00:06:08,065
使用这个技巧 我们不需要
额外为了偏差的学习规则

77
00:06:08,065 --> 00:06:13,016
这正好与学习这个
额外输入的权重相同

78
00:06:16,063 --> 00:06:21,005
所以这就是感知器的
非常强大的学习过程

79
00:06:21,005 --> 00:06:25,071
并且这是一个一定会起作用的学习过程
这是一个非常好的性质

80
00:06:25,071 --> 00:06:30,031
当然你需要看接下来的文稿
关于为什么这个保证

81
00:06:30,031 --> 00:06:36,068
不像你想的那么好
我们首先在每个输入向量中都有

82
00:06:36,068 --> 00:06:41,031
这个额外的取值恒为'1'的部分
现在我们可以不考虑这些偏差了

83
00:06:41,081 --> 00:06:46,076
然后我们一直载入训练实例
使用我们喜欢的任何规则

84
00:06:46,076 --> 00:06:51,009
直到我们确认每个训练实例
都可以不需要间隔很长时间被载入

85
00:06:51,009 --> 00:06:54,028
我不会详细定义这个

86
00:06:54,028 --> 00:06:59,030
如果你是一个数学家 你可以
思考可能的好的定义

87
00:07:02,056 --> 00:07:07,009
现在已经选择了一个训练实例
你检查输出是否正确

88
00:07:07,009 --> 00:07:09,092
如果是正确的
你不需要改变权重值

89
00:07:10,072 --> 00:07:15,090
如果输出单元的输出为'0'
而它应该输出'1' 换句话说

90
00:07:15,090 --> 00:07:21,002
它的输出与我们尝试学习的实际模式
不符合

91
00:07:21,002 --> 00:07:25,059
当这个情况发生时
我们只需要将把输入向量加到

92
00:07:25,059 --> 00:07:29,086
感知器的权重向量上
相反的 如果输出单元输出'1'

93
00:07:29,086 --> 00:07:35,029
当它应该输出'0'时
我们从权重向量中减去

94
00:07:35,029 --> 00:07:40,063
输入向量
然后非常惊人的 这个简单的

95
00:07:40,063 --> 00:07:45,073
学习过程确保了可以帮你找到
一组对于所有的训练实例都可以

96
00:07:45,073 --> 00:07:50,067
找到正确结果的权重值
这个的局限性是它只能在

97
00:07:50,067 --> 00:07:55,060
它是怎样一组权重时使用
但是对于许多有趣的问题 它们并没有

98
00:07:55,060 --> 00:07:58,051
这样一组权重
这样一组权重是否存在

99
00:07:58,051 --> 00:08:01,007
很大程度上取决于你使用的特征

100
00:08:01,007 --> 00:08:05,088
所以结果是对于很多问题
决定使用的特征十分困难

101
00:08:05,088 --> 00:08:08,056
如果你使用的是恰当的特征

102
00:08:08,056 --> 00:08:12,066
学习它们可能会变得非常简单
如果你没有使用正确的特征

103
00:08:12,066 --> 00:08:16,081
学习它们将变得不可能
并且所有的工作都是取决于特征的
翻译 by Lilian Li