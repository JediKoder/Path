这个视频，我们将会讨论线性神经元的误差曲面。 理解了误差曲面的形状，
我们就会对 线性神经元的学习过程中所发生的事情了解的更多。 在训练线性神经元的权重时，
我们可以对所发生的事情有一个很好的几何学理解。 我们可以假设一个 类似于之前理解感知器时所用的空间 但是多了一个维度。
现在假设一个空间， 其所有水平的维度都是权重， 此外还有一个垂直的维度，代表着误差。 在这个空间里，
水平平面上的点代表不同的权重集合。 高度代表相应权重上， 所有训练数据错误的加和。 对线性神经元而言，
每组权重的集合对于的错误组成了误差曲面。 这个曲面是个二次的碗状。 如果垂直切开， 你总能得到一个抛物线。
如果水平切开，总是一个椭圆形。 这仅仅适用于平方误差。 当我们开始讨论多层，非线性神经网络时， 误差曲面会变得更加复杂。 当参数还不是很多的时候，
误差曲面还会是光滑的， 但可能存在局部最优。
使用上述误差曲面， 我们可以得到当我们使用
德尔塔定律来进行梯度下降学习时的场景。 德尔塔定律， 计算了误差相对于参数的导数。 我们相当于在误差曲面进行最急速的下降。 当我们使用得到的导数来改变权重时， 换个方式理解，
当我们从上方来观察误差曲线时， 我们会看到椭圆的轮廓线。 德尔塔定律，如上图，
在椭圆轮廓线上指明了正确的方向。 其发生在批量学习的场景，
在这个场景，梯度是在所有的训练样本上加和得到的。 此外，我们也进行在线学习， 对于每个训练样本，
我们都会依照这个样本上计算出来的梯度来修改权重。 这和我们在感知器学习中所做的类似。 你可以看到，
权重的调整使我们 向一个其中一个控制面移动。 在右图中，
有两个训练样本。 要让第一个样本正确，
我们必须在其中一条蓝线上， 要让第二个样本正确，我们必须在另一条蓝线上。 因为，我们从其中一个红点开始， 然后我们开始计算第一个样本的梯度，
德尔塔定律 会让我们垂直的奔向那条线。
当我们考虑另一个样本的时候， 我们则垂直的奔向另一条线。 当我们在两个样本间不停交换时， 我们呈锯齿状移动，直至最终解，
即最终两条线的交点。 这一点上的权重参数可以同时满足两个样本。 使用误差曲面， 我们也能理解何种条件可以让训练很慢。 如果训练样本对应的先几乎平行，
那么误差曲面的椭圆会很扁， 当我们观察梯度时， 其有个不好的属性。 请看图中红色的箭头，
梯度在我们不想移动太多的方向很大， 但在我们需要移动很长的方向则很小。 所以梯度很快带领我们越过谷底， 即椭圆的短轴。 我们要很长的时间， 才能沿着峡谷即椭圆的长轴前进。 这不是我们想要的。
我们想得到一个梯度，在越过峡谷方向前进一点点， 沿着峡谷方向一大步，但是我们得到不是这个。 所以，简单的最快梯度下降，
每次使用学习率乘以误差的导数来调整参数权重， 在图示中这种很扁的 误差曲面的情况下会遇到极大的困难。