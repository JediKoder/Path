1
00:00:00,000 --> 00:00:04,052
В этом видео я собираюсь рассказать о трёх типах машинного обучения:

2
00:00:04,052 --> 00:00:08,057
обучение с учителем, обучение с подкреплением и обучение без учителя.

3
00:00:08,057 --> 00:00:13,027
Вообще-то всю первую половину курса мы будем обсуждать обучение с учителем.

4
00:00:13,027 --> 00:00:17,079
Вторая половина будет посвящена обучению без учителя.

5
00:00:17,079 --> 00:00:22,049
А обучение с подкреплением не будет затронуто в этом курсе, так как мы не можем

6
00:00:22,049 --> 00:00:26,060
охватить всё. Обучение может быть разделено на три больших

7
00:00:26,060 --> 00:00:30,067
группы алгоритмов. В обучении с учителем вы пытаетесь

8
00:00:30,067 --> 00:00:35,092
предсказать выходной сигнал исходя из входного вектора, так что смысл

9
00:00:35,092 --> 00:00:41,017
обучения с учителем ясен. В обучении с подкреплением, вы пытаетесь

10
00:00:41,017 --> 00:00:46,607
выбрать действия или их последовательность, которые ведут к получению наибольшего количества "поощрений",

11
00:00:46,607 --> 00:00:53,030
а поощрения могут возникать лишь время от времени.
При обучении без учителя вы пытаетесь

12
00:00:53,030 --> 00:00:59,577
выяснить наиболее приемлемое представление входных данных, а что это значит мы выясним позже.

13
00:00:59,577 --> 00:01:03,795
Обучение с учителем в свою очередь можно разделить на два 

14
00:01:03,795 --> 00:01:08,121
различных направления. В регрессионном анализе целевым результатом является вещественное

15
00:01:08,121 --> 00:01:14,135
число или целый вектор вещественных чисел, к примеру цена акции через шесть месяцев

16
00:01:14,135 --> 00:01:21,059
или температура завтра в полдень. И цель - получить результат чтоль близкий, насколько это возможно,

17
00:01:21,059 --> 00:01:25,399
к верному значению. В классификации целью является

18
00:01:25,399 --> 00:01:29,364
метка класса. Простейший пример - выбор между единицей

19
00:01:29,364 --> 00:01:32,606
и нулем, между позитивным и негативным случаями.

20
00:01:32,606 --> 00:01:37,636
Но очевидно, что мы можем иметь несколько различных меток, как если мы

21
00:01:37,636 --> 00:01:44,492
классифицируем рукописные цифры. Обучение с учителем работает, изначально

22
00:01:44,492 --> 00:01:49,512
выбирая модель отношений, на всем наборе прецедентов, которые мы подготовили

23
00:01:49,512 --> 00:01:53,422
в качестве кандидатов. Модель отношений сожно считать

24
00:01:53,422 --> 00:01:59,825
функцией, которая на входе получает вектор и некоторые параметры и на выходе дает результат - y.

25
00:01:59,825 --> 00:02:03,836
То есть, модель отношений - это просто способ выявления соответствия

26
00:02:03,836 --> 00:02:10,939
входных данных и результата с использованием числовых параметров W. Затем мы подгоняем эти

27
00:02:10,939 --> 00:02:16,394
числовые параметры, чтобы соответствие подходило для обучающей выборке данных.

28
00:02:16,394 --> 00:02:22,046
Под словом "подходить" мы имеем ввиду минимизацию расхождения между целевым результатом

29
00:02:22,046 --> 00:02:27,255
по каждому обучающему примеру и реальным результатом, полученным системой машинного обучения.

30
00:02:27,255 --> 00:02:32,591
И очевидным измерением этого расхождения, если мы используем вещественные значения 

31
00:02:32,591 --> 00:02:38,746
в качестве выходных данных, является квадрат разности между выходом нашей системы y и 

32
00:02:38,746 --> 00:02:44,057
правильным результатом t, разделенный пополам, чтобы сократить двойку 

33
00:02:44,057 --> 00:02:47,453
в производной. Для классификации вы могли бы использовать эту 

34
00:02:47,453 --> 00:02:51,994
меру, но есть другие более подходящие способы, о которых мы поговорим позднее, и 

35
00:02:51,994 --> 00:02:56,203
эти более подходящие способы обычно, к тому же, работают лучше.

36
00:02:56,203 --> 00:03:03,055
В обучении с подкреплением результатом является последовательность сигналов, и вы должны 

37
00:03:03,055 --> 00:03:07,080
принимать решение по этим сигналам на основании спонтанных сигналов подкрепления.

38
00:03:07,080 --> 00:03:12,516
Цель, преследуемая при выборе каждого сигнала, в максимизации ожидаемой суммы будущих

39
00:03:12,516 --> 00:03:17,139
поощрений, и мы обычно используем дисконтирующий множитель, так что не нужно заглядывать слишком

40
00:03:17,139 --> 00:03:20,472
далеко в будущее. Мы считаем, что поощрения из далекого будущего 

41
00:03:20,472 --> 00:03:24,592
не так важны, как поощрения, которые вы получите достаточно скоро.

42
00:03:24,592 --> 00:03:29,538
Обучение с подкреплением трудное. Оно трудно потому, что поощрения

43
00:03:29,538 --> 00:03:34,451
обычно отстают во времени, по этому сложно узнать, какой именно сигнал был ошибочным

44
00:03:34,451 --> 00:03:38,007
в длинной последовательности сигналов. Оно также трудно из-за того, что единичный

45
00:03:38,007 --> 00:03:41,879
сигнал подкрепления, особенно тот, что появляется лишь спонтанно, не дает много

46
00:03:41,879 --> 00:03:45,082
информации, на основании которой происходят изменения параметров.

47
00:03:45,082 --> 00:03:50,235
Так, обычно, вы не можете обучить миллионы параметров, используя обучение с подкреплением,

48
00:03:50,235 --> 00:03:53,830
в то время как при обучении с учителем и без учителя, можете.

49
00:03:53,830 --> 00:03:57,798
Обычно, в обучении с подкреплением вы пытаетесь обучить десятки 

50
00:03:57,798 --> 00:04:00,755
параметров, или, может быть, 1000 параметров, но не миллионы.

51
00:04:00,755 --> 00:04:04,827
В этом курсе мы не можем затронуть всё, и поэтому мы не будем изучать

52
00:04:04,827 --> 00:04:08,552
обучение с подкреплением, дае несмотря на то, что это важная тема.

53
00:04:08,552 --> 00:04:14,350
Обучение без учителя будет рассмотрено во второй половине этого курса.

54
00:04:14,350 --> 00:04:20,040
На протяжении почти 40 лет сообщество машинного обучения в основном игнорировало 

55
00:04:20,040 --> 00:04:24,282
обучение без учителя, за исключением некоторых очень ограниченных его форм, называющихся кластерами.

56
00:04:24,282 --> 00:04:28,990
Фактически, они использовали те определения машинного обучения, которые исключали его.

57
00:04:28,990 --> 00:04:34,481
Так, они определили машинное обучение в некоторых книгах как установление соответствия между входными

58
00:04:34,481 --> 00:04:37,589
и выходными данными. И многие исследователи считали, что

59
00:04:37,589 --> 00:04:40,822
кластеризация - единственная форма обучения без учителя.

60
00:04:40,822 --> 00:04:46,870
Одной из причин этого является то, что сложно сформулировать цель обучения без учителя.

61
00:04:46,870 --> 00:04:50,518
Одна из главных целей - получить внутреннее

62
00:04:50,518 --> 00:04:54,879
представление входных данных, это полезно для последующего обучения с учителем или

63
00:04:54,879 --> 00:04:59,188
обучения с подкреплением. И причина, по которой мы хотим делать это

64
00:04:59,188 --> 00:05:04,481
двумя стадиями, в том, что мы не хотим использовать, например, результаты обучения

65
00:05:04,481 --> 00:05:08,503
с подкреплением, для установки параметров нашей визуальной системы.

66
00:05:08,503 --> 00:05:13,310
Так вы можете вычислить расстояние до поверхности, используя неравенство между 

67
00:05:13,310 --> 00:05:17,076
изображениями, которые получают два ваших глаза. Но вы не хотите учиться производить этот

68
00:05:17,076 --> 00:05:21,003
расчет расстояния, постоянно спотыкаясь и корректируя 

69
00:05:21,003 --> 00:05:24,566
параметры вашей визуальной системы каждый раз, когда вы споткнулись.

70
00:05:24,566 --> 00:05:29,100
Вам пришлось бы спотыкаться очень большое количество раз и 

71
00:05:29,100 --> 00:05:33,474
существуют намного лучшие способы научиться соединять два изображения, основываясь исключительно на информации

72
00:05:33,474 --> 00:05:37,799
из входных данных. Другие цели обучения без учителя - 

73
00:05:37,799 --> 00:05:42,194
предоставить компактные, малой размерности представления входных данных.

74
00:05:42,194 --> 00:05:47,149
Так, такие входные данные большой размерности, как изображения, обычно, соотносятся прямо или косвенно 

75
00:05:47,149 --> 00:05:51,599
с множеством образов малой размерности, или с несколькими такими множествами, как в случае 

76
00:05:51,599 --> 00:05:55,584
с рукописными цифрами. Это значит, что даже если у вас есть 

77
00:05:55,584 --> 00:06:00,605
миллион пикселей, то в действительности нет миллиона степеней свободы в том, что

78
00:06:00,605 --> 00:06:04,118
может получиться в результате. Возможно, будет лишь несколько сотен степеней

79
00:06:04,118 --> 00:06:08,025
свободы в результате, тогда наша задача - уйти от

80
00:06:08,025 --> 00:06:12,617
представления в миллион пикселей к сотням тысяч степеней свободы, которые

81
00:06:12,617 --> 00:06:15,804
будут говорить, соответственно, где мы находимся в множестве образов.

82
00:06:15,804 --> 00:06:18,342
Также нам нужно знать, какое множество мы рассматриваем.

83
00:06:18,342 --> 00:06:24,321
Очень ограниченная форма данного подхода - метод главных компонент, который является линейным.

84
00:06:24,321 --> 00:06:29,064
Он предполагает, что есть одно множество образов, и это множество - плоскость в пространстве 

85
00:06:29,064 --> 00:06:33,323
высокой размерности. Другое определение обучения

86
00:06:33,323 --> 00:06:37,846
без учителя или другая цель обучения без учителя - получить

87
00:06:37,846 --> 00:06:41,746
экономичное представление входных данных c точки зрения выявленных признаков.

88
00:06:41,746 --> 00:06:46,605
Если, например, мы можем представить входные данные в бинарном виде, то это

89
00:06:46,605 --> 00:06:51,552
обычно экономично, так как тогда используется только один бит для обозначения состояния бинарного

90
00:06:51,552 --> 00:06:54,600
признака. Или же мы могли бы использовать большое количество

91
00:06:54,600 --> 00:06:59,330
признаков с вещественными значениями, но утверждать, что для каждого входного значения почти все эти

92
00:06:59,330 --> 00:07:03,481
признаки - нули. В этом случае для каждого входного значения нам нужно только

93
00:07:03,481 --> 00:07:07,107
предмтавить несколько вещественных чисел, и это экономично.

94
00:07:07,107 --> 00:07:13,711
Как я говорил ранее, другим определением обучения без учителя или другой целью

95
00:07:13,711 --> 00:07:18,543
обучения без учителя является нахождение кластеров входных данных, и кластеризация

96
00:07:18,543 --> 00:07:23,969
могла бы рассмартиваться в очень разделенном коде, когда мы имеем один признак на кластер 

97
00:07:23,969 --> 00:07:30,062
и утверждаем, что все признаки, кроме одного, нулевые, и что один признак имеет

98
00:07:30,062 --> 00:07:33,814
значение единица. Так, кластеризация - действительно только крайний

99
00:07:33,814 --> 00:07:36,037
случай поиска разделенных параметров.