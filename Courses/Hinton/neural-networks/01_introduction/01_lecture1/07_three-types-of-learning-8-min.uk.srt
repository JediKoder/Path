1
00:00:00,000 --> 00:00:04,052
В цьому відео я хочу поговорити про три різних типи машинного навчання

2
00:00:04,052 --> 00:00:08,057
навчання з учителем, навчання з підкріпленням і навчання без учителя.

3
00:00:08,057 --> 00:00:13,027
В загальних рисах, перша половина курсу буде про навчання з учителем.

4
00:00:13,027 --> 00:00:17,079
Друга половина курсу буде більше про навчання без учителя і

5
00:00:17,079 --> 00:00:22,049
навчання з підкріпленням не буде покрито в цьому курсі, тому що мине можемо покрити

6
00:00:22,049 --> 00:00:26,060
все. Навчання може розділятися на три загальні

7
00:00:26,060 --> 00:00:30,067
групи алгоритмів. В навчання з учителем ви намагаєтесь

8
00:00:30,067 --> 00:00:35,092
прогнозувати вихід коли заданий вхідний вектор, отож  дуже зрозуміло, що є метою

9
00:00:35,092 --> 00:00:41,017
навчання з учителем.  В навчання з підтвердженням ви намагаєтесь

10
00:00:41,017 --> 00:00:46,607
вибрати дію або послідовність дій, щоб максимізувати нагороду яку ви отримаєте і

11
00:00:46,607 --> 00:00:53,030
нагороди можуть з'являтися іноді. В навчання без вчителя ви намагаєтесь

12
00:00:53,030 --> 00:00:59,577
знати  гарне  внутрішнє представлення входів і ми пізніше дійдемо до того

13
00:00:59,577 --> 00:01:03,795
що це значить. Навчання з учителем само по собі може бути розділени на два

14
00:01:03,795 --> 00:01:08,121
різних типи. В регресійному, цільовий вихід це дробові

15
00:01:08,121 --> 00:01:14,135
числа, або цілий вектор дробових чисел, таких як ціна акцій за період шість місяців,

16
00:01:14,135 --> 00:01:21,059
або температура опівдні вчора. І мета в тому, щоб отримати корректние значення янайближче

17
00:01:21,059 --> 00:01:25,399
до правильного значення. В классфікації метою є

18
00:01:25,399 --> 00:01:29,364
міткакласу. Найпростіший клас - це вибір між одиницею

19
00:01:29,364 --> 00:01:32,606
і нулем. Між позитивним і негативним класом.

20
00:01:32,606 --> 00:01:37,636
Але очевидно, ми можемо мати багато альтернативних міток(класів), так як коли ми

21
00:01:37,636 --> 00:01:44,492
класифікуємо рукописні символи. Навчання з учителем працює з попередньо

22
00:01:44,492 --> 00:01:49,512
визначенною моделлю класів, це є повнийнабір моделей, що приготовані виступити

23
00:01:49,512 --> 00:01:53,422
кандидатами. Ви можете думати про клас моделі як про

24
00:01:53,422 --> 00:01:59,825
функцію, що бере вхфдний вектор і деякі параметри і видає вихідне значення Y

25
00:01:59,825 --> 00:02:03,836
Так, модель класів це просто форма подання.

26
00:02:03,836 --> 00:02:10,939
Вхід і вихід використовує деякі числові параметри W і потім ми змінюємо ці

27
00:02:10,939 --> 00:02:16,394
числові параметри, щоб зробити подання покриваючим всі данні навчання з учителем

28
00:02:16,394 --> 00:02:22,046
Що ми маємо на увазі під покриттям - мінімізацію неспівпадіння між цільовим виводом

29
00:02:22,046 --> 00:02:27,255
кожної спроби тренування і наявний вивводом створенним системою машинного навчання.

30
00:02:27,255 --> 00:02:32,591
І очевидним виміром цього неспівпадіння, якщо ми використовуємо дробові значення як

31
00:02:32,591 --> 00:02:38,746
вивід, це квадрат різниці між виводом нашої системи і

32
00:02:38,746 --> 00:02:44,057
правильним виводом, і і ми ставимо половину,так що вона відміняє два коли

33
00:02:44,057 --> 00:02:47,453
ми диференцюємо. Для классифікації ви можете використовувати

34
00:02:47,453 --> 00:02:51,994
це вимірюкання, але існують більш чутливі вимірювання, до яких ми дійдемо пізніше, і

35
00:02:51,994 --> 00:02:56,203
ці більш чутливі вимірювання типово працюють краще.

36
00:02:56,203 --> 00:03:03,055
В навчанні з підтвердженням, вихід з даної послідовності дії і ви повинні

37
00:03:03,055 --> 00:03:07,080
прийняти рішення по цих діях базуючись на випадкових заохоченнях

38
00:03:07,080 --> 00:03:12,516
Метою вибору кожної дії є максимізація очікуваної суми майбутніх

39
00:03:12,516 --> 00:03:17,139
заохочень і ми типово використовуємо фактор дисконтування так щоб ви не дивилися

40
00:03:17,139 --> 00:03:20,472
дуже далеко в майбутнє. Ми скажемо що заохочення далеко в майбутньому

41
00:03:20,472 --> 00:03:24,592
не враховується так сильно, як заохочення, що ви отримаєте швидко.

42
00:03:24,592 --> 00:03:29,538
Навчання з підтвердженням є складним. Воно є складним через те що заохочення

43
00:03:29,538 --> 00:03:34,451
типово відкладені, так що важко знати яка дія була неправильно в

44
00:03:34,451 --> 00:03:38,007
довгій послідовності дій. Це також важко тому що скалярне

45
00:03:38,007 --> 00:03:41,879
заохочення, особливо що трапляється випадково не є інформативним

46
00:03:41,879 --> 00:03:45,082
, що було б базою для зміни параметрів.

47
00:03:45,082 --> 00:03:50,235
Так, типово, ви не можете навчити міліон параметрів за допомогою навчання з підтвердженням.

48
00:03:50,235 --> 00:03:53,830
Де навчання з вчителем і навчання без учителя зможуть.

49
00:03:53,830 --> 00:03:57,798
Типово, в навчанні з підтвердженням, ви намагаєтесь вивчити сотні

50
00:03:57,798 --> 00:04:00,755
параметрів, або навіть тисячу параметрів, але не міліон.

51
00:04:00,755 --> 00:04:04,827
В цьому курсі, ми не можемо покрити все, і також ми не збираємось покрити

52
00:04:04,827 --> 00:04:08,552
навчання з підтвердженням, навіть якщо це важлива тема.

53
00:04:08,552 --> 00:04:14,350
Навчання без вчителя, буде розглянуто в другій частині курсу.

54
00:04:14,350 --> 00:04:20,040
Близько 40 років, спільнота машинного навчання ігнорувала навчання без вчителя

55
00:04:20,040 --> 00:04:24,282
крім однієї дуже обмеженої форми, що називається кластеризацією.

56
00:04:24,282 --> 00:04:28,990
По факту, вони використовували визначення машинного навчання, що виключають її.

57
00:04:28,990 --> 00:04:34,481
Так, вони визначають машинне навчання, в деяких джерелах, як поданння з входів на

58
00:04:34,481 --> 00:04:37,589
виходи. І багато дослідників думали що

59
00:04:37,589 --> 00:04:40,822
кластеризація - це тільки одна форма мавшинного навчання.

60
00:04:40,822 --> 00:04:46,870
Одна  з причин цього, це те що  важко сказати що є метою навчання без вчителя

61
00:04:46,870 --> 00:04:50,518
Одна головна мета - отримати внутрішнє

62
00:04:50,518 --> 00:04:54,879
поданя інформації, що є корисним для наступного навчання з учителем чи

63
00:04:54,879 --> 00:04:59,188
навчання з підтвердженням. І причиною з якою ми можемо хотіти робити в

64
00:04:59,188 --> 00:05:04,481
дві стадії , це якщо ми наприклад не хочемо використовувати виграші з навчання з підтвердженям

65
00:05:04,481 --> 00:05:08,503
для того щоб встановити парметри для наших візуальних систем.

66
00:05:08,503 --> 00:05:13,310
Так ви можете розрахувати відстань до поверхні використовуючи відмінності

67
00:05:13,310 --> 00:05:17,076
в зображеннях, які ви отримуєте з ваших глаз. Але ви не хочете вчитися робити

68
00:05:17,076 --> 00:05:21,003
це обчислення дистанції часто згинаючи палець і підлаштовуючи

69
00:05:21,003 --> 00:05:24,566
параметри вашої візуальної системи кожен раз коли згинається палець.

70
00:05:24,566 --> 00:05:29,100
Це викличе згинання вашого пальця дуже велику кількість разів і є

71
00:05:29,100 --> 00:05:33,474
значно кращеі шляхи вивчити порівняння двох зображень, яке б базувалося б на інформації

72
00:05:33,474 --> 00:05:37,799
з входів. Друга мета для навчання без вчителя, це

73
00:05:37,799 --> 00:05:42,194
надання компактної, малорозмірної форми подання вхідних даних.

74
00:05:42,194 --> 00:05:47,149
Багаторозмірні входи, такі як зображення знаходять на або близько

75
00:05:47,149 --> 00:05:51,599
низькорозмірного подання. Або декілька подань у випадку

76
00:05:51,599 --> 00:05:55,584
рукописних чисел. Що означає, якщо ви маєте

77
00:05:55,584 --> 00:06:00,605
міліон пікселів. ценасправді не міліон ступенів свободи які можуть

78
00:06:00,605 --> 00:06:04,118
трапитися. Тут може бути тільки сотні степенів

79
00:06:04,118 --> 00:06:08,025
свободи, що можуть зустрітися. Тож, те що ми робимо, це рухаємось від

80
00:06:08,025 --> 00:06:12,617
міліонів пікселів до представлення цих декількох сот ступенів свободи які

81
00:06:12,617 --> 00:06:15,804
будуть згідно того, де ми знаходимось серед різноманіття виборки.

82
00:06:15,804 --> 00:06:18,342
Також ми потребуємо знати де в просторі станів ми знаходимося.

83
00:06:18,342 --> 00:06:24,321
Дуже обмежена форма цього аналіз головних компонент, які є лінійними

84
00:06:24,321 --> 00:06:29,064
Він припускає що це один набір і цей набір є проекцією

85
00:06:29,064 --> 00:06:33,323
великорозмірного простору. Інше визначення навчання без вчителя

86
00:06:33,323 --> 00:06:37,846
або інша мета навчання без вчителя це надати

87
00:06:37,846 --> 00:06:41,746
економне подання для вхідних даних з точки зору вивчених особливостей.

88
00:06:41,746 --> 00:06:46,605
Якщо, наприклад ми можемо подати вхідні дані з точки зору бінарних особливостей, це

89
00:06:46,605 --> 00:06:51,552
як правило економніше тому що це бере тільки один біт, для двійкового стану

90
00:06:51,552 --> 00:06:54,600
властивості. Альтернативно ми можемо викорстовувати велику кількість

91
00:06:54,600 --> 00:06:59,330
властивостей заданих дробовими числами але дано, що майже всі з цих

92
00:06:59,330 --> 00:07:03,481
властивостей задані нулями. В такому випадку для кожного входу ми потребуємо тільки декілька дробових чисел

93
00:07:03,481 --> 00:07:07,107
для представлення і це економічніше.

94
00:07:07,107 --> 00:07:13,711
Як згадувалося раніше, інше призначення навчання без вчителя, або інша мета

95
00:07:13,711 --> 00:07:18,543
навчання без вчителя, це знайти кластери у входах і кластеризація

96
00:07:18,543 --> 00:07:23,969
може бути розглянута як дуже розсіяне кодування, коли ми маємо одну властивість на кластер

97
00:07:23,969 --> 00:07:30,062
і ми стверджуємо, що всі інші властвості крім цієї нуль, і ця властивість має

98
00:07:30,062 --> 00:07:33,814
значення одиниці. Так, кластеризація це тільки екстремальний

99
00:07:33,814 --> 00:07:36,037
випадок знаходження розсіяних властивостей.