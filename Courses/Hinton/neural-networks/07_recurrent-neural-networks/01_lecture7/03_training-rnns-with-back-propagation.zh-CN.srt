1
00:00:00,000 --> 00:00:04,919
在这一讲的视频中 我将讲解随着时间的反向传播算法

2
00:00:04,919 --> 00:00:08,780
这是训练或者循环你的神经网络的标准做法

3
00:00:09,500 --> 00:00:14,493
这个算法非常的简单 一旦你看到

4
00:00:14,493 --> 00:00:19,243
循环神经网络和一个每个时间步长是一层的前馈神经网络

5
00:00:19,243 --> 00:00:23,847
是等效的 我也会讲解给循环神经网络提供

6
00:00:23,847 --> 00:00:28,060
输入和理想输出的方法

7
00:00:28,940 --> 00:00:35,070
这张图展示了一个简单的
有着三个互相连接的神经元的循环网络

8
00:00:35,070 --> 00:00:41,750
我们假设在那些连接之间的
时间延迟为1

9
00:00:41,750 --> 00:00:47,960
并且这个网络以离散时间运行
所以时钟是整数刻度

10
00:00:49,160 --> 00:00:55,029
理解如何训练循环神经网络的关键是
要明白

11
00:00:55,029 --> 00:01:01,368
循环神经网络真的只是和前馈网络一模一样

12
00:01:01,368 --> 00:01:08,595
而前馈神经网络只是把循环网络扩展而已
所以循环神经网络从初始化

13
00:01:08,595 --> 00:01:12,840
一些状态开始
这张图的底部是 0时刻

14
00:01:13,100 --> 00:01:19,698
然后用这些连接来得到一个新的状态

15
00:01:19,698 --> 00:01:24,026
如时刻1所示
你接着再用相同的权重来得到

16
00:01:24,026 --> 00:01:29,625
有一个新的状态
接着再用相同的权重来得到另一个新的

17
00:01:29,625 --> 00:01:33,530
状态 以此类推
所以它只是一个不断前进的前馈网络

18
00:01:33,530 --> 00:01:38,540
而权重有一个约束
每一层都要是相同的

19
00:01:39,040 --> 00:01:44,028
当有了权重约束后
反向传播就适合用于学习

20
00:01:44,028 --> 00:01:49,458
我们在卷积网络中已经看到过
提醒一下 事实上我们可以

21
00:01:49,458 --> 00:01:55,814
很容易地在反向传播中加入线性约束
所以我们像以往一样

22
00:01:55,814 --> 00:01:59,534
计算梯度 就好像权重没有被约束

23
00:01:59,534 --> 00:02:03,960
接着我们就修改梯度
这样我们就保持了约束

24
00:02:04,580 --> 00:02:10,166
所以如果我们想让W1等于W2
我们就从相同权重出发 接着需要确保

25
00:02:10,166 --> 00:02:13,801
改变W1和改变W2是相等的

26
00:02:13,801 --> 00:02:19,051
然后我们只要简单地对W1求导

27
00:02:19,051 --> 00:02:23,897
对W2求导 然后
对导数相加或者取平均 然后

28
00:02:23,897 --> 00:02:27,600
用所得的相同结果对W1和W2更新

29
00:02:28,600 --> 00:02:32,953
所以如果权重一开始就满足约束
他们会继续

30
00:02:32,953 --> 00:02:37,160
满足约束
随着时间的反向传播算法

31
00:02:37,160 --> 00:02:42,537
正如其名 你可以把一个循环网络想成
是一个共享权重的前进的前馈网络

32
00:02:42,537 --> 00:02:47,136
前馈网络
用反向传播来训练

33
00:02:47,136 --> 00:02:50,570
所以我们可以考虑一个
在时间域内的算法

34
00:02:50,570 --> 00:02:55,076
前向的每次传播在每个时间片
建立一堆的活动

35
00:02:55,076 --> 00:03:00,709
而反向的每次传播把这堆活动一片片地取出
然后计算误差导数

36
00:03:00,709 --> 00:03:05,017
这就是为什么它叫做随着时间的反向传播

37
00:03:05,017 --> 00:03:08,198
在反向传播后 我们可以

38
00:03:08,198 --> 00:03:13,235
把每个在不同时间段的权重导数相加

39
00:03:13,235 --> 00:03:16,284
然后改变所有的相同权重

40
00:03:16,284 --> 00:03:21,387
通过用与所有导数和或者均值成比例的值
用来更新

41
00:03:21,387 --> 00:03:25,402
有一个令人麻烦的额外问题

42
00:03:25,402 --> 00:03:30,610
如果我们没有指定所有单元的初始状态
比如 如果一些但是是隐藏或者输出层

43
00:03:30,610 --> 00:03:35,818
那么我们需要把他们初始化成
一些特定的状态

44
00:03:35,818 --> 00:03:38,992
我们可以就把这些初始化状态

45
00:03:38,992 --> 00:03:43,937
设置成默认值0.5 但是
这也许不会让系统工作的那么好

46
00:03:43,937 --> 00:03:48,460
相比于把它初始化成一些
更明智的初始值

47
00:03:48,760 --> 00:03:51,389
事实上我们可以学习到
初始状态

48
00:03:51,389 --> 00:03:56,408
我们可以把它们当做参数
而不是活动 然后就可以像学习权重

49
00:03:56,408 --> 00:04:00,292
一样学习它们 我们用一些随机值来初始化

50
00:04:00,292 --> 00:04:03,937
这些初始状态 这包括所有的非

51
00:04:03,937 --> 00:04:08,657
输入单元的所有其他单元 接着再
在每次一系列的训练后 我们反向

52
00:04:08,657 --> 00:04:12,362
沿着时间序列一直传播到
初始状态

53
00:04:12,362 --> 00:04:16,783
这就给了我们初始状态的误差函数的导数

54
00:04:16,783 --> 00:04:21,254
我们接着就只需要用这些导数

55
00:04:21,254 --> 00:04:25,086
来调整初始状态
我们用梯度下降法 这就

56
00:04:25,086 --> 00:04:28,440
给了我们不太相同的初始化状态

57
00:04:29,460 --> 00:04:34,344
有许多方法可以让我们为
循环神经网络提供输入

58
00:04:34,344 --> 00:04:38,351
我们可以 比如 指定好所有单元的初始状态

59
00:04:38,351 --> 00:04:43,423
这是我们能够最直接就想到的
当我们考虑一个像是有着权重约束的前馈网络

60
00:04:43,423 --> 00:04:49,681
一样的循环网络
我们可以指定一部分单元的初始化状态

61
00:04:49,681 --> 00:04:55,233
或者我们可以指定
在每个时刻的一部分单元的状态

62
00:04:55,233 --> 00:05:00,995
这也许是对输入序列数据的最
直接的方法

63
00:05:00,995 --> 00:05:04,298
相似地 我们有许多方法来指定

64
00:05:04,298 --> 00:05:09,217
循环网络的输出
当我们把它想成是有权重约束的

65
00:05:09,217 --> 00:05:14,488
前馈神经网络时
最自然的事就是指定所有单元的

66
00:05:14,488 --> 00:05:20,612
理想的最后状态 
如果我们是要训练它来

67
00:05:20,612 --> 00:05:26,398
适用于一些吸引子 我们也许想要指定
不仅仅是最后时刻的

68
00:05:26,398 --> 00:05:31,684
理想状态也包括一些时刻的状态
事实上这就会使它在那些状态下

69
00:05:31,684 --> 00:05:35,720
沉淀 而不是传播某些状态
然后在某些其它地方停止

70
00:05:36,100 --> 00:05:41,864
所以通过指定最后的一些状态
我们可以迫使它来学习吸引子

71
00:05:41,864 --> 00:05:47,701
并且这非常的简单 因为我们可以
反向传播在每个时刻得到的

72
00:05:47,701 --> 00:05:51,933
导数 所以反向传播从顶开始

73
00:05:51,933 --> 00:05:55,289
带着最后时刻的导数

74
00:05:55,289 --> 00:06:01,199
接着当我们沿着顶端之后的这条线
向后 我们为它加入导数

75
00:06:01,199 --> 00:06:06,015
以此类推 所以在许多不同层
求导数

76
00:06:06,015 --> 00:06:12,148
是几乎不费力的 或者我们
可以指定我们认为是输出单元的一部分单元的

77
00:06:12,148 --> 00:06:15,854
设计活动

78
00:06:15,854 --> 00:06:21,159
这是很自然的方法 训练一个
可以得到连续输出值的

79
00:06:21,159 --> 00:06:23,268
循环神经网络
翻译 Slyne