1
00:00:00,000 --> 00:00:05,009
下面我们来浅谈一些科学家感兴趣而

2
00:00:05,009 --> 00:00:08,050
工程师并不太感兴趣的问题

3
00:00:08,050 --> 00:00:11,090
所以 诸位工程师可以忽视这集视频

4
00:00:12,040 --> 00:00:18,058
在计算机科学领域 有一场持续百年的争论

5
00:00:18,058 --> 00:00:23,066
关于特征向量的概念表示和

6
00:00:23,066 --> 00:00:28,050
概念间相互联系表示的关系

7
00:00:28,050 --> 00:00:34,069
我们刚看到的系谱图学习算法跟这个争论有很大关系

8
00:00:34,069 --> 00:00:38,072
我们现在对认知科学做一个简要的介绍

9
00:00:38,072 --> 00:00:43,024
长期以来有一个争论

10
00:00:43,024 --> 00:00:46,042
对于什么是概念 有两种不同的理论

11
00:00:47,020 --> 00:00:52,006
特征理论认为一个概念是一个很大的语义特征集合

12
00:00:52,035 --> 00:00:56,012
这对于解释概念之间的相似性很好

13
00:00:56,012 --> 00:00:59,057
而且对机器学习这类领域很便利

14
00:00:59,057 --> 00:01:02,088
因为我们喜欢处理向量

15
00:01:02,088 --> 00:01:07,056
结构理论认为概念的含义在于

16
00:01:07,056 --> 00:01:13,026
它和其他概念之间的关系 因此概念上的知识最好

17
00:01:13,026 --> 00:01:16,051
不用向量来表征 而是用关系图

18
00:01:16,051 --> 00:01:22,044
在1970年代初 Marvin Minsky使用感知机的限制作为证据

19
00:01:22,044 --> 00:01:27,056
反对特征向量 偏向于关系图表示法

20
00:01:27,056 --> 00:01:32,044
我认为这两方都不对 因为它们都

21
00:01:32,044 --> 00:01:37,020
相信这两种理论是敌对的 但其实不是这样的

22
00:01:37,020 --> 00:01:42,069
一个神经网络可以使用语义特征向量来实现一个关系图

23
00:01:42,069 --> 00:01:48,018
在学习系谱图的神经网络中 我们可以考虑显式推导

24
00:01:48,018 --> 00:01:53,073
当我给你第一个人和他的关系 你就能说出第二个人

25
00:01:54,012 --> 00:01:59,073
为了推出这个结论 神经网络不用按照一系列

26
00:01:59,073 --> 00:02:04,007
推导规则 它通过网络将信息向前传递

27
00:02:04,007 --> 00:02:09,009
在神经网络的角度

28
00:02:09,009 --> 00:02:14,035
在直觉上答案是明显的 如果你对细节保持观察

29
00:02:14,035 --> 00:02:19,040
就会发现有很多概率特征在互相影响着

30
00:02:19,040 --> 00:02:24,007
我们之所以叫它们微特征 是为了强调它们不像

31
00:02:24,007 --> 00:02:28,059
显式的意识特征 在真实的大脑中 可能有

32
00:02:28,059 --> 00:02:34,025
上百万这种微特征和相互联系 因为这种相互联系

33
00:02:34,025 --> 00:02:39,091
我们可以在显式推理上更进一步 
即 我们所相信的是

34
00:02:39,091 --> 00:02:45,022
我们所看到的一些问题的答案
这里没有介于中间的意识的阶段

35
00:02:45,022 --> 00:02:50,040
但是有很多在神经元之间相互作用的计算

36
00:02:50,040 --> 00:02:55,044
我们可以使用显式的规则来进行有意识的精密推理

37
00:02:55,044 --> 00:03:00,045
但是很多我们认为是常识的推理

38
00:03:00,045 --> 00:03:06,007
尤其是反逻辑的推理 只在看到答案时有效 
没有介于之间的意识过程

39
00:03:06,007 --> 00:03:09,018
即使当我们做一些意识推理

40
00:03:09,018 --> 00:03:14,052
我们不得不考虑一些方法来观察什么规则适用 为了避免

41
00:03:14,052 --> 00:03:19,077
无限的回归 因此 当很多人考虑

42
00:03:19,077 --> 00:03:25,009
在神经网络中实现一个关系图 只用假设你应该

43
00:03:25,009 --> 00:03:30,096
使一个神经元与关系图中的一个节点连接

44
00:03:30,096 --> 00:03:34,033
和两个神经元与一个二进制关系图的连接

45
00:03:34,033 --> 00:03:39,026
但是这个方法是无效的 首先 关系

46
00:03:39,026 --> 00:03:42,047
有不同的偏好 它们是不同种类的

47
00:03:42,047 --> 00:03:45,347
关系 比如母亲的关系 姑姑的关系 和

48
00:03:45,347 --> 00:03:48,050
神经网络中的一个连接仅仅有一个好处

49
00:03:48,050 --> 00:03:54,087
它不是不同类型的连接 我们也需要转变关系

50
00:03:54,087 --> 00:03:57,074
比如A在B和C之间

51
00:03:58,034 --> 00:04:03,080
我们仍然不知道在神经网络中实现知识关系正确的方式

52
00:04:03,080 --> 00:04:07,060
但是很可能

53
00:04:07,060 --> 00:04:13,042
很多神经元被用作我们所知道的概念 每一个

54
00:04:13,042 --> 00:04:18,039
神经元很可能负责处理很多不同的概念

55
00:04:18,039 --> 00:04:21,056
这被称作分布式表征

56
00:04:22,013 --> 00:04:26,043
这是一个多对多的在概念和神经元之间的映射 翻译 Naiding Zhou