在本视频中 
我们会介绍卷积神经网络 在手写数字识别中的应用
这是神经网络在1980s几个成功应用之一 由 Yann LeCun 和他的合作者们开发的
深度卷积神经网络 在识别手写数字方面表现出色 并投入到实际应用当中
这是为数不多的 能够在当时的计算机硬件条件下训练 并且表现很好的深度神经网络应用之一 卷积神经网络起源于重复特征的思想 由于物体会到处移动
在不同的像素点上出现 如果我们有 一个特征检测器在图像的某个位置起作用
有很大可能 这个特征检测器能够在其他位置派上用场 所以这里的思路是在所有不同的位置 采用相同的特征检测器
右边的这个图展示了 三个特征检测器
它们分别是各自的复制品 每个包含对应于9个像素点的权重
这些权重对于 这三个特征检测器来说是完全相等的
也就是说三根红色的箭头有着一样的权重 在学习的时候 我们保持这些红色的箭头 拥有一样的权重 绿色的箭头 拥有一样的权重
而红色跟绿色箭头会有 不同的权重
我们也可以尝试在不同的比例 和方位上复制特征
但这样会更困难复杂并且 不一定是个好主意
在不同的位置复制特征大大 减少了需要学习的自由参数的数量 这3个检测器中的27个像素只有9个 不同的权重值
我们需要的不止一种特征 所以我们会有很多的特征图 (feature maps) 每一个特征图提取图像不同位置的 相同特征 不同的特征图 会学习提取不同的特征
这就让图像的每一个区块 能够表示为很多不同的特征的集合 重复特征的思想与反向传播并不冲突 很容易用反向传播来训练模型
实际上我们可以对反向传播做一些简单的 改动来考虑权重之间的任意线性约束 具体的做法是我们先照常计算梯度 然后在参数更新前对梯度进行修改
来使它们满足 这些线性约束
这样参数在更新之后也会满足 相同的约束
举个最简单的例子 我们想让两个 权重保持相等
比如 w1 = w2 要满足这个约束 我们可以初始化
w1 等于 w2 然后我们只要保证 w1 的更新总是
与 w2 的更新值相等 方法是分别计算 w1 的梯度 和 w2 的梯度
然后用两个梯度的和或者平均 来更新 w1 以及 w2
通过使用这样的权重约束 我们能够促使反向传播学习
重复的特征检测器 很多文献对重复特征检测器所起的作用 有误解
很多人认为它们的作用是 平移不变性 这是不正确的 至少从神经元的活动来说是不准确的 如果你观察神经元的活动
重复特征达到的效果是 同变性而不是不变性
举个例子应该更好理解 这里是一幅图片 黑点表示
被激活的神经元 这是一幅平移后的图片
我们可以观察到这些黑点也 跟着平移了
也就是说图片发生了变化 而从这幅图片中提取的特征也发生了
相同的变化 这是同变性而不是不变性
不过确实有不变的地方 那就是学到的知识
如果你学到了重复的特征 检测器 你知道如何在某个位置检测
一种特征 那么你同样可以 在另一个位置检测同样的特征
很重要的一点是我们 在神经元激活层面达到的是同变性
在权重方面达到的是不变性 如果你想在神经元活动层面达到一定的
不变性 你需要做的是 对重复特征做池化 (pooling) 的操作 你能够在深度神经网络的每一层
达到少部分的平移不变性 通过将所有邻接的特征检测器的输出做平均 这么做的优势之一是减少了
下一层网络的输入数量 这样我们就可以有更多的特征图
能够在下一层中学到更多的不同种类的 特征 实际上更好的一种做法是 取四个邻接特征检测器输出的最大值
而不是平均值 但是这么做有一个问题
那就是在做了几层这样的 池化操作之后
我们丢失了图片中物体的精确位置信息 如果我们只是想识别图片是不是一张人脸
的话这个问题不大 几个眼睛、一个鼻子和 一张嘴在大致相近的位置出现已经 是人脸的很好证据
但是如果你想要识别出这张脸是谁 你需要利用五官之间精确的相对空间关系 而那些信息已经在这些 卷积神经网络中丢失了
我们之后还会讨论到这个问题 卷积神经网络的第一个非常成功的应用 是 Yann LeCun 和他的合作者们开发的一个
用于手写数字的识别器 这个模型有很多个隐层 每一层都会有很多个特征图 隐层之间有池化相邻单元的操作 然后再将池化的结果传到下一层
另外我们还用到了一个宽的网络 能够识别多个字符
而且就算数字有重叠也能识别 这样你就不需要将单个字符分割出来 再输入到神经网络中 人们经常遗忘的一点是他们使用了
一种精巧的方法来训练 一个完整的系统
他们并不只是训练针对单个字符的 识别器
他们训练了一个完整的系统 输入图片的像素点
你就能得到对应的邮政编码 在训练的时候这个系统使用了现在被称为
最大间距的方法 不过那个时候这个概念还没有被提出 他们提出的这个神经网络模型曾经 负责北美百分之十的支票的识别任务 这有着很大的实用性价值
关于这个模型有一些非常好的演示 在 Yann 的网站上 大家应该去看一下 都看一遍
这些演示展示了 这个模型应对不同大小、方向、位置
的数字 相互重叠的数字以及 不同的背景噪点的能力
大部分方法不具备这种能力 LeNet5 模型的结构是这样的 首先有一堆输入像素点
然后有一系列的 特征图和子采样层 (即池化层)
例如在 C1 这一层中有6个 不同的特征图 每一个的大小是28x28
这些特征图包含着 3x3的特征检测器提取的特征 这些检测器的权重都是共享的 所以每个特征图只有9个参数 使得训练过程更加高效
需要的数据量大大减少 在特征图之后是他们称为子采样的操作
现在被称为池化操作 你把 C1 中一堆相邻的重复特征 合并到一起 这样得到一个较小的特征图
作为下一层的输入 从而提取更为复杂的特征 随着层数的提升
你会得到越来越复杂的特征 它们对位置也越来越不敏感
这张图列出了 LeNet5 识别错的数字 从这可以看出它处理的数据是很复杂的 一共有10,000个测试样本
这是它识别错误的所有82个样本 识别成功率在99%以上
但是大部分的错误都是 人们觉得很容易就能识别的数字 所以这里还有一些提升的空间
没有人知道人类在这个数据集上的错误率 据估计在20到30个错误之间 当然有可能 LeNet5 识别正确的数字
而人会识别错 所以在估计错误率的时候要小心 你不能只看这82个样本然后
统计哪些你能识别出来哪些不能 你还得去看所有其他 LeNet5 识别 正确的样本 而你有可能识别不出 我们现在要讨论一个很宽泛的概念
如何在机器学习问题中加入先验知识 这个对神经网络也非常适用 就像 LeNet5 一样 我们可以通过
设计网络结构来加入先验知识 我们可以有局部连接性 可以对权重进行约束
也可以根据需要解决的任务来 选择合适的神经元激活函数 这比直接手动加工特征要系统化得多 但是这样还是会让网络模型偏向
我们脑中的某种解决问题的特定方式 我们对如何做物体识别有一个思路 那就是逐部提取越来越高层次的特征 并在空间的不同位置对相同特征进行提取 我们就促使网络模型按照这样的方式工作 有另一种整合先验知识的方式
能够赋予网络模型更大的自由度 我们可以利用先验知识来 获得更多的训练数据
这方面最早的例子之一是 Hofmann 和 Tresp 的炼钢炉模型 他们想知道炼钢炉的产物和 不同的输入变量之间的关系
他们实际上使用的是一个 Fortran 模拟器 用于模拟炼钢炉的环境 当然模拟器跟真实的环境是有差别的
模拟器采取了各种的近似方法 他们有真实的数据和一个模拟器 他们利用模拟器生成了一些合成数据 再把这些合成数据跟真实数据合并起来
他们发现这样做比仅仅使用 真实数据要更为有效
如果我没记错 他们的那个巨型 Fortran 模拟器仅仅贡献了几十个样本 就算这样它们还是起到了作用
当然 如果使用了大量的 合成数据会使训练时间大大增长 所以就训练时间而言
像 LeNet5 那样以限制连接和约束权重 的方式来整合知识是效率更高的方式 但随着计算速度的加快
这种生成合成数据的方式 表现得越来越好 特别是它使得优化方法能够找到更精巧的 利用多层网络的方法
而这些方法我们可能都没有考虑过 实际上我们可能永远不能完全理解它的原理 如果我们只是想要获得一个问题的较优解
这可能问题不大 利用合成数据的思想
有一种暴力途径来解决 手写数字识别
LeNet5 从不变性的知识出发 设计了模型中的局部连接性、权重共享
以及池化操作 他们达到了80个错误
通过加入包括合成数据在内的更多技巧 Marc'Aurelio Ranzato 把错误降到了40个 由 Jürgen Schmidhuber 领导的瑞士的
一个研究小组以创造合成数据 的方式来注入知识
他们在创造有指导性的合成数据 方面做了很多努力
对每一个真实的训练样本 他们通过变换得到更多的训练样本 之后他们在 GPU 上训练了一个很大的网络
网络模型有很多层 每一层有很多隐藏节点 GPU 让计算的速度加快了13倍 由于他们加入的各种合成数据 模型没有过拟合
如果他们只是用 GPU 训练一个大型网络 那么会发生很严重的过拟合现象 在训练数据上表现完美但是
在测试数据上则很差 他们联合使用了这三种技巧 生成了很多合成数据
然后在 GPU 上训练了一个大型网络 他们就这样成功地达到了35个错误 这里列出了那35个错误 顶部的数字代表了正确的答案
底部的两个数字则是模型预测的 前两名
我们可以观察到正确答案几乎 总在前两名中 只有5个样本的正确答案没有在前两名
的预测中 通过搭建几个 类似这个的不同模型并使用
投票的方式来决定预测的数据 他们成功地将错误数降到25个 而这正好接近人类的错误率 这个研究提出的一个问题是
人们怎么判断错30个的模型 是不是真的比40个错误的模型要优越 这个差别真的显著吗?
令人没想到的是这个要 依具体识别的错误而定
错误数量并不能提供足够的 信息 你需要知道哪些样本模型识别错了 哪些样本模型识别对了
这个统计检验叫做 McNemar 检验 由于用到了特定的错误信息
它比只利用错误数量的检验方法要更为 敏感 举个例子 我们来看这个2x2的表
在左上角显示的是 模型1和模型2都预测错误的样本数 29个
在右下角显示的是 两个模型同时预测正确的样本数 在 McNemar 检验中
你可以直接忽略这些黑色的数字 你感兴趣的是那些
模型1预测正确但模型2 预测错误的样本 或者反过来模型2
预测正确而模型1预测错误的样本 从表中可以看出这两个数值的比值
是1比11 而这个比值其实是 很显著的
模型2肯定比模型1更好 而这几乎不可能是偶然的结果 相比之下我们来看这张表 模型1预测错误为40个 模型2 则为30个 但现在的情况是模型1
在15个样本上表现比模型2要好 而模型2则在25个样本上表现更为出色 这个差别并不显著
所以模型2要优于模型1的结论 置信度不高
翻译 Yijun Xiao