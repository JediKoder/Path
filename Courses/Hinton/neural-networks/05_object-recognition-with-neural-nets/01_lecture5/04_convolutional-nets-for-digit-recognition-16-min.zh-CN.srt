1
00:00:00,000 --> 00:00:05,428
在本视频中 
我们会介绍卷积神经网络

2
00:00:05,428 --> 00:00:10,669
在手写数字识别中的应用
这是神经网络在1980s几个成功应用之一

3
00:00:10,669 --> 00:00:15,801
由 Yann LeCun 和他的合作者们开发的
深度卷积神经网络

4
00:00:15,801 --> 00:00:21,004
在识别手写数字方面表现出色

5
00:00:21,004 --> 00:00:25,889
并投入到实际应用当中
这是为数不多的

6
00:00:25,889 --> 00:00:30,648
能够在当时的计算机硬件条件下训练

7
00:00:30,648 --> 00:00:33,440
并且表现很好的深度神经网络应用之一

8
00:00:33,440 --> 00:00:38,250
卷积神经网络起源于重复特征的思想

9
00:00:38,250 --> 00:00:43,124
由于物体会到处移动
在不同的像素点上出现 如果我们有

10
00:00:43,124 --> 00:00:48,126
一个特征检测器在图像的某个位置起作用
有很大可能

11
00:00:48,126 --> 00:00:51,461
这个特征检测器能够在其他位置派上用场

12
00:00:51,461 --> 00:00:56,720
所以这里的思路是在所有不同的位置

13
00:00:56,720 --> 00:01:01,016
采用相同的特征检测器
右边的这个图展示了

14
00:01:01,016 --> 00:01:04,943
三个特征检测器
它们分别是各自的复制品

15
00:01:04,943 --> 00:01:10,128
每个包含对应于9个像素点的权重
这些权重对于

16
00:01:10,128 --> 00:01:15,046
这三个特征检测器来说是完全相等的
也就是说三根红色的箭头有着一样的权重

17
00:01:15,046 --> 00:01:19,599
在学习的时候 我们保持这些红色的箭头

18
00:01:19,599 --> 00:01:24,517
拥有一样的权重 绿色的箭头

19
00:01:24,517 --> 00:01:28,888
拥有一样的权重
而红色跟绿色箭头会有

20
00:01:28,888 --> 00:01:32,895
不同的权重
我们也可以尝试在不同的比例

21
00:01:32,895 --> 00:01:37,752
和方位上复制特征
但这样会更困难复杂并且

22
00:01:37,752 --> 00:01:41,347
不一定是个好主意
在不同的位置复制特征大大

23
00:01:41,347 --> 00:01:45,460
减少了需要学习的自由参数的数量

24
00:01:45,940 --> 00:01:52,067
这3个检测器中的27个像素只有9个

25
00:01:52,067 --> 00:01:56,497
不同的权重值
我们需要的不止一种特征

26
00:01:56,497 --> 00:01:59,303
所以我们会有很多的特征图 (feature maps)

27
00:01:59,303 --> 00:02:05,283
每一个特征图提取图像不同位置的

28
00:02:05,283 --> 00:02:10,736
相同特征 不同的特征图

29
00:02:10,736 --> 00:02:15,667
会学习提取不同的特征
这就让图像的每一个区块

30
00:02:15,667 --> 00:02:19,149
能够表示为很多不同的特征的集合

31
00:02:19,149 --> 00:02:24,733
重复特征的思想与反向传播并不冲突

32
00:02:24,733 --> 00:02:29,447
很容易用反向传播来训练模型
实际上我们可以对反向传播做一些简单的

33
00:02:29,447 --> 00:02:34,960
改动来考虑权重之间的任意线性约束

34
00:02:36,100 --> 00:02:39,488
具体的做法是我们先照常计算梯度

35
00:02:39,488 --> 00:02:44,537
然后在参数更新前对梯度进行修改
来使它们满足

36
00:02:44,537 --> 00:02:49,785
这些线性约束
这样参数在更新之后也会满足

37
00:02:49,785 --> 00:02:54,780
相同的约束
举个最简单的例子 我们想让两个

38
00:02:54,780 --> 00:02:57,780
权重保持相等
比如 w1 = w2

39
00:02:59,100 --> 00:03:03,345
要满足这个约束 我们可以初始化
w1 等于 w2

40
00:03:03,345 --> 00:03:09,520
然后我们只要保证 w1 的更新总是
与 w2 的更新值相等

41
00:03:09,520 --> 00:03:14,680
方法是分别计算 w1 的梯度

42
00:03:14,680 --> 00:03:19,576
和 w2 的梯度
然后用两个梯度的和或者平均

43
00:03:19,576 --> 00:03:24,869
来更新 w1 以及 w2
通过使用这样的权重约束

44
00:03:24,869 --> 00:03:29,170
我们能够促使反向传播学习
重复的特征检测器

45
00:03:29,170 --> 00:03:34,529
很多文献对重复特征检测器所起的作用

46
00:03:34,529 --> 00:03:39,094
有误解
很多人认为它们的作用是

47
00:03:39,094 --> 00:03:41,940
平移不变性 这是不正确的

48
00:03:42,280 --> 00:03:46,698
至少从神经元的活动来说是不准确的

49
00:03:46,698 --> 00:03:51,769
如果你观察神经元的活动
重复特征达到的效果是

50
00:03:51,769 --> 00:03:56,260
同变性而不是不变性
举个例子应该更好理解

51
00:03:57,080 --> 00:04:02,070
这里是一幅图片 黑点表示
被激活的神经元

52
00:04:02,070 --> 00:04:06,870
这是一幅平移后的图片
我们可以观察到这些黑点也

53
00:04:06,870 --> 00:04:09,967
跟着平移了
也就是说图片发生了变化

54
00:04:09,967 --> 00:04:14,380
而从这幅图片中提取的特征也发生了
相同的变化

55
00:04:14,380 --> 00:04:20,921
这是同变性而不是不变性
不过确实有不变的地方

56
00:04:20,921 --> 00:04:24,622
那就是学到的知识
如果你学到了重复的特征

57
00:04:24,622 --> 00:04:29,806
检测器 你知道如何在某个位置检测
一种特征 那么你同样可以

58
00:04:29,806 --> 00:04:35,124
在另一个位置检测同样的特征
很重要的一点是我们

59
00:04:35,124 --> 00:04:39,843
在神经元激活层面达到的是同变性
在权重方面达到的是不变性

60
00:04:39,843 --> 00:04:45,160
如果你想在神经元活动层面达到一定的
不变性 你需要做的是

61
00:04:45,160 --> 00:04:48,550
对重复特征做池化 (pooling) 的操作

62
00:04:48,550 --> 00:04:54,048
你能够在深度神经网络的每一层
达到少部分的平移不变性

63
00:04:54,048 --> 00:04:58,389
通过将所有邻接的特征检测器的输出做平均

64
00:04:58,389 --> 00:05:04,250
这么做的优势之一是减少了
下一层网络的输入数量

65
00:05:04,250 --> 00:05:09,319
这样我们就可以有更多的特征图
能够在下一层中学到更多的不同种类的

66
00:05:09,319 --> 00:05:13,770
特征 实际上更好的一种做法是

67
00:05:13,770 --> 00:05:18,530
取四个邻接特征检测器输出的最大值
而不是平均值

68
00:05:18,530 --> 00:05:22,054
但是这么做有一个问题
那就是在做了几层这样的

69
00:05:22,054 --> 00:05:26,999
池化操作之后
我们丢失了图片中物体的精确位置信息

70
00:05:26,999 --> 00:05:30,549
如果我们只是想识别图片是不是一张人脸
的话这个问题不大

71
00:05:30,549 --> 00:05:33,958
几个眼睛、一个鼻子和

72
00:05:33,958 --> 00:05:38,308
一张嘴在大致相近的位置出现已经

73
00:05:38,308 --> 00:05:42,363
是人脸的很好证据
但是如果你想要识别出这张脸是谁

74
00:05:42,363 --> 00:05:47,183
你需要利用五官之间精确的相对空间关系

75
00:05:47,183 --> 00:05:50,298
而那些信息已经在这些

76
00:05:50,298 --> 00:05:54,060
卷积神经网络中丢失了
我们之后还会讨论到这个问题

77
00:05:54,460 --> 00:05:59,768
卷积神经网络的第一个非常成功的应用

78
00:05:59,768 --> 00:06:05,146
是 Yann LeCun 和他的合作者们开发的一个
用于手写数字的识别器

79
00:06:05,146 --> 00:06:08,180
这个模型有很多个隐层

80
00:06:08,480 --> 00:06:11,811
每一层都会有很多个特征图

81
00:06:11,811 --> 00:06:16,513
隐层之间有池化相邻单元的操作

82
00:06:16,513 --> 00:06:21,869
然后再将池化的结果传到下一层
另外我们还用到了一个宽的网络

83
00:06:21,869 --> 00:06:26,768
能够识别多个字符
而且就算数字有重叠也能识别

84
00:06:26,768 --> 00:06:29,707
这样你就不需要将单个字符分割出来

85
00:06:29,707 --> 00:06:33,300
再输入到神经网络中

86
00:06:33,780 --> 00:06:39,613
人们经常遗忘的一点是他们使用了
一种精巧的方法来训练

87
00:06:39,613 --> 00:06:43,750
一个完整的系统
他们并不只是训练针对单个字符的

88
00:06:43,750 --> 00:06:47,955
识别器
他们训练了一个完整的系统

89
00:06:47,955 --> 00:06:53,585
输入图片的像素点
你就能得到对应的邮政编码

90
00:06:53,585 --> 00:06:59,011
在训练的时候这个系统使用了现在被称为
最大间距的方法

91
00:06:59,011 --> 00:07:02,131
不过那个时候这个概念还没有被提出

92
00:07:02,131 --> 00:07:06,586
他们提出的这个神经网络模型曾经

93
00:07:06,586 --> 00:07:11,210
负责北美百分之十的支票的识别任务

94
00:07:11,210 --> 00:07:15,960
这有着很大的实用性价值
关于这个模型有一些非常好的演示

95
00:07:15,960 --> 00:07:19,380
在 Yann 的网站上 大家应该去看一下

96
00:07:19,380 --> 00:07:22,610
都看一遍
这些演示展示了

97
00:07:22,610 --> 00:07:28,522
这个模型应对不同大小、方向、位置
的数字 相互重叠的数字以及

98
00:07:28,522 --> 00:07:33,821
不同的背景噪点的能力
大部分方法不具备这种能力

99
00:07:33,821 --> 00:07:37,200
LeNet5 模型的结构是这样的

100
00:07:37,640 --> 00:07:43,604
首先有一堆输入像素点
然后有一系列的

101
00:07:43,604 --> 00:07:49,143
特征图和子采样层 (即池化层)
例如在 C1 这一层中有6个

102
00:07:49,143 --> 00:07:54,969
不同的特征图 每一个的大小是28x28
这些特征图包含着

103
00:07:54,969 --> 00:07:58,101
3x3的特征检测器提取的特征

104
00:07:58,101 --> 00:08:01,233
这些检测器的权重都是共享的

105
00:08:01,233 --> 00:08:04,583
所以每个特征图只有9个参数

106
00:08:04,583 --> 00:08:09,900
使得训练过程更加高效
需要的数据量大大减少

107
00:08:11,020 --> 00:08:17,660
在特征图之后是他们称为子采样的操作
现在被称为池化操作

108
00:08:17,660 --> 00:08:22,515
你把 C1 中一堆相邻的重复特征

109
00:08:22,515 --> 00:08:26,898
合并到一起

110
00:08:26,898 --> 00:08:34,263
这样得到一个较小的特征图
作为下一层的输入

111
00:08:34,263 --> 00:08:40,230
从而提取更为复杂的特征

112
00:08:40,230 --> 00:08:47,316
随着层数的提升
你会得到越来越复杂的特征

113
00:08:47,316 --> 00:08:53,121
它们对位置也越来越不敏感
这张图列出了 LeNet5 识别错的数字

114
00:08:53,121 --> 00:08:57,724
从这可以看出它处理的数据是很复杂的

115
00:08:57,724 --> 00:09:02,396
一共有10,000个测试样本
这是它识别错误的所有82个样本

116
00:09:02,396 --> 00:09:07,754
识别成功率在99%以上
但是大部分的错误都是

117
00:09:07,754 --> 00:09:11,601
人们觉得很容易就能识别的数字

118
00:09:11,601 --> 00:09:16,243
所以这里还有一些提升的空间
没有人知道人类在这个数据集上的错误率

119
00:09:16,243 --> 00:09:18,746
据估计在20到30个错误之间

120
00:09:18,746 --> 00:09:23,517
当然有可能 LeNet5 识别正确的数字
而人会识别错

121
00:09:23,517 --> 00:09:26,718
所以在估计错误率的时候要小心

122
00:09:26,718 --> 00:09:31,548
你不能只看这82个样本然后
统计哪些你能识别出来哪些不能

123
00:09:31,548 --> 00:09:34,864
你还得去看所有其他 LeNet5 识别

124
00:09:34,864 --> 00:09:38,880
正确的样本 而你有可能识别不出

125
00:09:39,260 --> 00:09:44,679
我们现在要讨论一个很宽泛的概念
如何在机器学习问题中加入先验知识

126
00:09:44,679 --> 00:09:49,274
这个对神经网络也非常适用

127
00:09:49,274 --> 00:09:54,831
就像 LeNet5 一样 我们可以通过
设计网络结构来加入先验知识

128
00:09:54,831 --> 00:09:57,260
我们可以有局部连接性

129
00:09:57,260 --> 00:10:01,688
可以对权重进行约束
也可以根据需要解决的任务来

130
00:10:01,688 --> 00:10:04,721
选择合适的神经元激活函数

131
00:10:04,721 --> 00:10:08,907
这比直接手动加工特征要系统化得多

132
00:10:08,907 --> 00:10:13,517
但是这样还是会让网络模型偏向
我们脑中的某种解决问题的特定方式

133
00:10:13,517 --> 00:10:17,581
我们对如何做物体识别有一个思路

134
00:10:17,581 --> 00:10:21,160
那就是逐部提取越来越高层次的特征

135
00:10:21,160 --> 00:10:24,011
并在空间的不同位置对相同特征进行提取

136
00:10:24,011 --> 00:10:26,620
我们就促使网络模型按照这样的方式工作

137
00:10:27,520 --> 00:10:32,775
有另一种整合先验知识的方式
能够赋予网络模型更大的自由度

138
00:10:32,775 --> 00:10:37,081
我们可以利用先验知识来

139
00:10:37,081 --> 00:10:43,602
获得更多的训练数据
这方面最早的例子之一是

140
00:10:43,602 --> 00:10:48,710
Hofmann 和 Tresp 的炼钢炉模型

141
00:10:48,710 --> 00:10:53,843
他们想知道炼钢炉的产物和

142
00:10:53,843 --> 00:10:59,043
不同的输入变量之间的关系
他们实际上使用的是一个 Fortran 模拟器

143
00:10:59,043 --> 00:11:02,268
用于模拟炼钢炉的环境

144
00:11:02,268 --> 00:11:07,600
当然模拟器跟真实的环境是有差别的
模拟器采取了各种的近似方法

145
00:11:07,600 --> 00:11:10,430
他们有真实的数据和一个模拟器

146
00:11:10,430 --> 00:11:15,630
他们利用模拟器生成了一些合成数据

147
00:11:15,630 --> 00:11:20,434
再把这些合成数据跟真实数据合并起来
他们发现这样做比仅仅使用

148
00:11:20,434 --> 00:11:24,113
真实数据要更为有效
如果我没记错 他们的那个巨型

149
00:11:24,113 --> 00:11:28,279
Fortran 模拟器仅仅贡献了几十个样本

150
00:11:28,279 --> 00:11:32,385
就算这样它们还是起到了作用
当然 如果使用了大量的

151
00:11:32,385 --> 00:11:35,585
合成数据会使训练时间大大增长

152
00:11:35,585 --> 00:11:40,718
所以就训练时间而言
像 LeNet5 那样以限制连接和约束权重

153
00:11:40,718 --> 00:11:45,669
的方式来整合知识是效率更高的方式

154
00:11:45,669 --> 00:11:50,560
但随着计算速度的加快
这种生成合成数据的方式

155
00:11:50,560 --> 00:11:53,700
表现得越来越好

156
00:11:54,620 --> 00:12:00,221
特别是它使得优化方法能够找到更精巧的

157
00:12:00,221 --> 00:12:03,476
利用多层网络的方法
而这些方法我们可能都没有考虑过

158
00:12:03,476 --> 00:12:07,640
实际上我们可能永远不能完全理解它的原理

159
00:12:08,060 --> 00:12:12,192
如果我们只是想要获得一个问题的较优解
这可能问题不大

160
00:12:12,192 --> 00:12:16,783
利用合成数据的思想
有一种暴力途径来解决

161
00:12:16,783 --> 00:12:22,227
手写数字识别
LeNet5 从不变性的知识出发

162
00:12:22,227 --> 00:12:26,360
设计了模型中的局部连接性、权重共享
以及池化操作

163
00:12:26,620 --> 00:12:32,450
他们达到了80个错误
通过加入包括合成数据在内的更多技巧

164
00:12:32,450 --> 00:12:38,725
Marc'Aurelio Ranzato 把错误降到了40个

165
00:12:38,725 --> 00:12:45,960
由 Jürgen Schmidhuber 领导的瑞士的
一个研究小组以创造合成数据

166
00:12:45,960 --> 00:12:51,886
的方式来注入知识
他们在创造有指导性的合成数据

167
00:12:51,886 --> 00:12:57,360
方面做了很多努力
对每一个真实的训练样本

168
00:12:57,360 --> 00:13:01,662
他们通过变换得到更多的训练样本

169
00:13:01,662 --> 00:13:08,494
之后他们在 GPU 上训练了一个很大的网络
网络模型有很多层 每一层有很多隐藏节点

170
00:13:08,494 --> 00:13:12,774
GPU 让计算的速度加快了13倍

171
00:13:12,774 --> 00:13:17,499
由于他们加入的各种合成数据

172
00:13:17,499 --> 00:13:21,650
模型没有过拟合
如果他们只是用 GPU 训练一个大型网络

173
00:13:21,650 --> 00:13:26,721
那么会发生很严重的过拟合现象

174
00:13:26,721 --> 00:13:29,876
在训练数据上表现完美但是
在测试数据上则很差

175
00:13:29,876 --> 00:13:32,535
他们联合使用了这三种技巧

176
00:13:32,535 --> 00:13:37,730
生成了很多合成数据
然后在 GPU 上训练了一个大型网络

177
00:13:37,730 --> 00:13:40,390
他们就这样成功地达到了35个错误

178
00:13:40,390 --> 00:13:43,050
这里列出了那35个错误

179
00:13:43,050 --> 00:13:48,625
顶部的数字代表了正确的答案
底部的两个数字则是模型预测的

180
00:13:48,625 --> 00:13:52,092
前两名
我们可以观察到正确答案几乎

181
00:13:52,092 --> 00:13:55,152
总在前两名中

182
00:13:55,152 --> 00:14:00,607
只有5个样本的正确答案没有在前两名
的预测中 通过搭建几个

183
00:14:00,607 --> 00:14:06,142
类似这个的不同模型并使用
投票的方式来决定预测的数据

184
00:14:06,142 --> 00:14:09,076
他们成功地将错误数降到25个

185
00:14:09,076 --> 00:14:12,477
而这正好接近人类的错误率

186
00:14:12,477 --> 00:14:17,745
这个研究提出的一个问题是
人们怎么判断错30个的模型

187
00:14:17,745 --> 00:14:20,946
是不是真的比40个错误的模型要优越

188
00:14:20,946 --> 00:14:26,304
这个差别真的显著吗?
令人没想到的是这个要

189
00:14:26,304 --> 00:14:30,746
依具体识别的错误而定
错误数量并不能提供足够的

190
00:14:30,746 --> 00:14:34,236
信息 你需要知道哪些样本模型识别错了

191
00:14:34,236 --> 00:14:38,424
哪些样本模型识别对了
这个统计检验叫做

192
00:14:38,424 --> 00:14:43,564
McNemar 检验 由于用到了特定的错误信息
它比只利用错误数量的检验方法要更为

193
00:14:43,564 --> 00:14:46,420
敏感 举个例子

194
00:14:47,480 --> 00:14:53,268
我们来看这个2x2的表
在左上角显示的是

195
00:14:53,268 --> 00:14:58,233
模型1和模型2都预测错误的样本数

196
00:14:58,233 --> 00:15:02,012
29个
在右下角显示的是

197
00:15:02,012 --> 00:15:06,681
两个模型同时预测正确的样本数

198
00:15:06,681 --> 00:15:11,646
在 McNemar 检验中
你可以直接忽略这些黑色的数字

199
00:15:11,646 --> 00:15:17,575
你感兴趣的是那些
模型1预测正确但模型2

200
00:15:17,575 --> 00:15:22,170
预测错误的样本 或者反过来模型2
预测正确而模型1预测错误的样本

201
00:15:22,170 --> 00:15:27,874
从表中可以看出这两个数值的比值
是1比11 而这个比值其实是

202
00:15:27,874 --> 00:15:32,278
很显著的
模型2肯定比模型1更好

203
00:15:32,278 --> 00:15:35,311
而这几乎不可能是偶然的结果

204
00:15:35,311 --> 00:15:38,776
相比之下我们来看这张表

205
00:15:38,776 --> 00:15:42,535
模型1预测错误为40个 模型2

206
00:15:42,535 --> 00:15:48,870
则为30个 但现在的情况是模型1
在15个样本上表现比模型2要好

207
00:15:48,870 --> 00:15:52,988
而模型2则在25个样本上表现更为出色

208
00:15:52,988 --> 00:15:59,797
这个差别并不显著
所以模型2要优于模型1的结论

209
00:15:59,797 --> 00:16:01,540
置信度不高
翻译 Yijun Xiao