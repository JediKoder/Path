1
00:00:00,000 --> 00:00:04,900
这总是个问题 推测被用作

2
00:00:04,900 --> 00:00:09,377
手写数字识别的神经网络是否可以大规模的被用作

3
00:00:09,377 --> 00:00:13,008
人们所称的实际任务 也就是 识别物体于

4
00:00:13,008 --> 00:00:16,154
高分辨率的颜色图像中 而且场景是杂乱无章时

5
00:00:16,154 --> 00:00:20,571
所以你不得不做一些事情 比如图像分割 处理3D

6
00:00:20,571 --> 00:00:23,233
视角 处理5-foot list

7
00:00:23,233 --> 00:00:28,254
很多不同的物体在周围 你不是很确定哪一个是想要的

8
00:00:28,254 --> 00:00:32,218
诸如此类 自从本课程开始 我们已经得到

9
00:00:32,218 --> 00:00:37,147
让方面一些有意思的新结论 在我第一节课中 我描述了

10
00:00:37,147 --> 00:00:42,011
Alex Krizhevsky建立的网络 展示了它善于识别物体

11
00:00:42,011 --> 00:00:44,929
但是在那个时候它还没有被

12
00:00:44,929 --> 00:00:48,366
最好的计算机视觉系统进行标准测试

13
00:00:48,366 --> 00:00:52,211
现在有了 人们致力于研究Emenise很多年了

14
00:00:52,211 --> 00:00:58,200
渐渐地提升了它们识别手写数字的能力

15
00:00:58,980 --> 00:01:04,339
很多计算机视觉研究者认为这是浪费时间 如果你想要

16
00:01:04,339 --> 00:01:09,698
能够在有颜色的图片中识别真实的物体 因为他们认为

17
00:01:09,698 --> 00:01:13,637
从Emnise中学到的东西无法推广到那个领域

18
00:01:13,637 --> 00:01:16,478
这样想是很合理的

19
00:01:16,478 --> 00:01:20,546
这里有很多原因来解释为什么这是一个更加困难的任务

20
00:01:20,546 --> 00:01:24,614
首先 这里有很多很多不同的物体

21
00:01:24,614 --> 00:01:29,263
尽管我们仅仅识别一千种类别 这仍然是一百的倍数

22
00:01:29,263 --> 00:01:34,149
第二 这里有很多的像素点 尽管

23
00:01:34,149 --> 00:01:41,825
我们使用大量仅仅是256*256像素的图片 这仍然是

24
00:01:41,825 --> 00:01:47,576
100或者300倍于28*28的灰度图像 
另外一个因素是在真实的场景中 你不得不

25
00:01:47,576 --> 00:01:51,929
处理三维世界中的二维图片

26
00:01:51,929 --> 00:01:54,529
这样很多信息都会丢失

27
00:01:54,529 --> 00:01:58,655
真实场景有杂乱的类别 可能在手写中没有出现

28
00:01:58,655 --> 00:02:03,347
在手写中可能有重叠的字母 这需要分割 但是

29
00:02:03,347 --> 00:02:07,813
你没有东西模糊其他物体来遮挡物体的一大部分

30
00:02:07,813 --> 00:02:10,470
你没有很多不同种类的

31
00:02:10,470 --> 00:02:14,298
物体 在同一场景下 你也没有一些

32
00:02:14,298 --> 00:02:19,642
在真实场景中的光照变化 所以问题是同种

33
00:02:19,642 --> 00:02:25,486
在手写数字识别中表现的很好的卷积神经网络

34
00:02:25,486 --> 00:02:31,615
也会在真实颜色的图片中表现的很好吗 
在真实颜色图片的领域我们很可能

35
00:02:31,615 --> 00:02:37,321
需要去结合一些先验知识
因为 如果我们努力用sera san的方法去做

36
00:02:37,321 --> 00:02:43,101
而不结合一些知识 通过

37
00:02:43,101 --> 00:02:47,555
产生额外的训练样本来得到知识 则计算量

38
00:02:47,555 --> 00:02:51,810
对于现在的计算机来说是很大的 所以最近有一个竞赛

39
00:02:51,810 --> 00:02:59,300
基于ImageNet的数据库 ImageNet事实上有超过

40
00:02:59,300 --> 00:03:04,559
一百万的图片 但是有120万图片的子集

41
00:03:04,559 --> 00:03:09,021
被选来做分类的任务 正确的为这些图片贴上标签

42
00:03:09,021 --> 00:03:15,359
现在这些图片被手工标签 有一千种不同的类别 但是这不

43
00:03:15,359 --> 00:03:18,462
太可靠 一张图片里可能有两个

44
00:03:18,462 --> 00:03:22,542
在这一千种类别中出现的物体 但仅仅标记了一个

45
00:03:22,542 --> 00:03:27,139
所以 为了让这个任务更可行 
计算机视觉系统被允许做五个预测

46
00:03:27,139 --> 00:03:29,667
它被认为是正确的 如果这五个之中有一个

47
00:03:29,667 --> 00:03:33,690
与人工给的标签一致

48
00:03:33,690 --> 00:03:38,325
这里也有一个定位任务 理由是

49
00:03:38,325 --> 00:03:42,453
很多计算机视觉系统使用特征袋方法

50
00:03:42,453 --> 00:03:47,723
对于整个图片或者四分之一图片 它们知道特征

51
00:03:47,723 --> 00:03:52,735
是什么 但是不知道特征在哪里
这使得它们识别物体的同时

52
00:03:52,735 --> 00:03:57,316
但是并不知道物体究竟在哪里
这不像人类的行为

53
00:03:57,316 --> 00:04:02,430
除非是大脑好奇部位损伤 即平衡综合征的人

54
00:04:02,430 --> 00:04:05,619
他们可以识别物体但是不确定在哪

55
00:04:05,619 --> 00:04:10,553
所以对于定位任务 
你不得不在物体周围放一个盒子 一旦你已经

56
00:04:10,553 --> 00:04:15,607
识别到了它 然后把它放进盒子 
至少与正确的盒子有50%的交集

57
00:04:15,607 --> 00:04:19,155
在这个任务中 人们尝试了一些

58
00:04:19,155 --> 00:04:24,806
现存最好的计算机视觉方法 
牛津大学的领头团队和

59
00:04:24,806 --> 00:04:30,532
法国国家信息与自动化研究所 和 施乐欧洲研究中心

60
00:04:30,532 --> 00:04:35,960
以及其他不同的大学努力 都觉得这很难

61
00:04:36,900 --> 00:04:42,246
计算机视觉系统通常使用复杂的多级系统

62
00:04:42,246 --> 00:04:47,954
这些系统早期时通常使用手工调节来

63
00:04:47,954 --> 00:04:53,156
使用一些数据优化参数
现在这些系统是

64
00:04:53,156 --> 00:04:58,060
使用的学习算法
但是他们不能学习到所有的东西

65
00:04:58,060 --> 00:05:02,904
方式是深度神经网络被训练来做后向传播

66
00:05:02,904 --> 00:05:07,993
他们没有端到端的学习 在早期特征检测器中使用的参数

67
00:05:07,993 --> 00:05:12,899
被制定决策分类中的作用大小所影响

68
00:05:12,899 --> 00:05:16,210
因此这里有一些测试集的样本

69
00:05:16,210 --> 00:05:21,656
来告诉你数据是怎么样的
你已经看到一些例子 在第一个课程里

70
00:05:21,656 --> 00:05:25,966
但是这里还有一些
所以你可以很明显的看到

71
00:05:25,966 --> 00:05:29,652
图像中的物体是什么 虽然很多东西都缺失了

72
00:05:29,652 --> 00:05:32,276
它没有耳朵 没有腿

73
00:05:32,276 --> 00:05:36,837
预测结果是在Alex Krizhevsky的深度神经网络中
没有归一化的概率

74
00:05:36,837 --> 00:05:40,647
你可以看到它有十足的把握说

75
00:05:40,647 --> 00:05:45,250
这是一只猎豹 如果不是猎豹 
它会认为可能是一只豹子

76
00:05:45,250 --> 00:05:49,292
它也给出了其他的可能性 比如一只雪豹

77
00:05:49,292 --> 00:05:52,283
这不是一只雪豹的颜色 也不是埃及猫

78
00:05:52,283 --> 00:05:56,824
这是另外一个例子
这张图片中有很多物体

79
00:05:56,824 --> 00:06:00,534
但是感兴趣的只占很小一部分像素

80
00:06:00,534 --> 00:06:04,984
神经网络正确的将它归为子弹头列车
但是他也有其他的假设 比如地铁

81
00:06:04,984 --> 00:06:08,192
或者电力动车 这都是有可能的假设

82
00:06:08,192 --> 00:06:12,833
如果你看图片 有很多物体都可以被作为标签

83
00:06:12,833 --> 00:06:17,301
比如 屋顶 占了图片很大一部分空间 比列车或者

84
00:06:17,301 --> 00:06:20,510
支撑屋顶和人行道的柱子

85
00:06:20,510 --> 00:06:23,386
或者背景部分的公寓楼

86
00:06:23,569 --> 00:06:28,221
在这种图片中 你不得不处理这种情况

87
00:06:28,221 --> 00:06:32,198
图片中有很多可以选择地目标 最后一张图片展示了不同种类的

88
00:06:32,198 --> 00:06:34,952
例子 这里没有背景杂物

89
00:06:34,952 --> 00:06:39,298
物体被很好的分离出来 可能是目录用的图片或者什么

90
00:06:39,298 --> 00:06:42,494
神经网络第一次没有预测正确

91
00:06:42,494 --> 00:06:45,897
但是在最可能的五个预测中

92
00:06:45,897 --> 00:06:49,174
但是这里神经网络并不是很有把握

93
00:06:49,174 --> 00:06:54,405
他们的概率都近似 网络正确的意识到他

94
00:06:54,405 --> 00:06:56,989
不是真的清楚 如果你看另外左边两个的

95
00:06:56,989 --> 00:06:59,888
概率 他们都有很好的把握

96
00:06:59,888 --> 00:07:04,993
如果你眯起你的眼睛以至于你不能很好地看到图片
你就知道它为什么

97
00:07:04,993 --> 00:07:08,082
可能认为这是一个煎锅或者听筒

98
00:07:08,082 --> 00:07:13,769
那么系统如何处理这些数据呢
计算机视觉系统有差错率

99
00:07:13,769 --> 00:07:17,479
你能注意到的一件事是最好的

100
00:07:17,479 --> 00:07:22,515
系统都很相似 所以东京大学得到了

101
00:07:22,515 --> 00:07:27,660
26.1%的差错率 这里我只是报道每个团队最好的系统

102
00:07:27,660 --> 00:07:32,492
牛津大学有很好的计算机视觉团队 通常

103
00:07:32,492 --> 00:07:37,861
被认为是欧洲最好的团队 但是还是有26%的差错率

104
00:07:37,861 --> 00:07:43,297
位于施乐公园中心的法国国家信息与自动化研究所 也是

105
00:07:43,297 --> 00:07:48,867
一个很好的计算机视觉团队 得到了27%的差错率
所以你可以从中猜测得到

106
00:07:48,867 --> 00:07:54,169
26%是很难得的 如果能超过26%你就能与最好的

107
00:07:54,169 --> 00:07:58,880
计算机视觉系统并驾齐驱了
Alex Krizhevsky的神经网络得到了

108
00:07:58,880 --> 00:08:01,463
16%的差错率 这是一个很大的飞跃

109
00:08:01,463 --> 00:08:06,974
通常 在这样的竞争中你不会看到像这么大的飞跃

110
00:08:06,974 --> 00:08:10,850
因此Alex Krizhevsky的网络像这样工作的

111
00:08:10,850 --> 00:08:15,265
它是一个很深的卷积神经网络 由Yann Le Cu所创始

112
00:08:15,265 --> 00:08:20,020
率先被由于数字识别 然后Yann应用它来识别真实

113
00:08:20,020 --> 00:08:22,737
物体 我们将用所有的

114
00:08:22,737 --> 00:08:28,001
从Yann和[未知]各个不同的团队学习的课

115
00:08:28,001 --> 00:08:30,548
来为做真正的视觉建立深度神经网络

116
00:08:30,548 --> 00:08:34,964
它有七个隐藏层 这比
通常都深 而且没有计算

117
00:08:34,964 --> 00:08:38,700
一些最大池化层 靠前的是卷积层

118
00:08:38,700 --> 00:08:43,133
我们很可能不需要本地感受野 不用任何

119
00:08:43,133 --> 00:08:47,514
权重 如果我们有一个很强的计算机的话
但是通过使他们卷积 你减少了

120
00:08:47,514 --> 00:08:52,000
很多参数 所以减少了很多你需要的训练数据

121
00:08:52,000 --> 00:08:54,798
这同时也减少了计算所花的时间

122
00:08:54,798 --> 00:08:59,178
最后两层总是被连接着的
这也是大多数参数所处的地方

123
00:08:59,178 --> 00:09:01,749
我想这里大约有一千六百万

124
00:09:01,749 --> 00:09:04,543
的参数介于每两层之间

125
00:09:04,543 --> 00:09:09,572
最后两层所做的是寻找到本地特征的组合

126
00:09:09,572 --> 00:09:14,850
这些特征是被之前的层提取出来的
显然这通常也是很多

127
00:09:14,850 --> 00:09:18,451
组合所寻找的 这解释了为什么你需要很多

128
00:09:18,451 --> 00:09:21,990
参数 激活函数是修正过的

129
00:09:21,990 --> 00:09:26,398
线性单元位于每个隐藏层中 这比逻辑

130
00:09:26,398 --> 00:09:31,136
单元训练的更快 也更加有效
大多数人都谨慎的使用深度

131
00:09:31,136 --> 00:09:36,266
神经网络 来从真实图片中识别物体
用我定义过的

132
00:09:36,266 --> 00:09:39,732
线性单元 我们也用竞争规则化

133
00:09:39,732 --> 00:09:45,350
在层内抑制一个单元的活跃性
如果附近的其他的单元

134
00:09:45,350 --> 00:09:50,304
十分活跃 这对于强度差异性有很大帮助

135
00:09:50,304 --> 00:09:53,655
因此 你可能有一个边缘检测器

136
00:09:53,655 --> 00:09:57,005
因为一些微弱的边缘而变得有些活跃

137
00:09:57,005 --> 00:10:02,129
这是很不相关的 
如果这附近有更多强烈的活动

138
00:10:02,129 --> 00:10:07,253
这里也有很多其他的技巧
我们用来有效的提升网络的普适性

139
00:10:07,253 --> 00:10:10,209
首先 我们使用

140
00:10:10,209 --> 00:10:13,100
通过变换强化数据的技巧

141
00:10:13,440 --> 00:10:18,637
这里有一个方法用来对图片在竞争中下采样到256*256维

142
00:10:18,637 --> 00:10:23,835
但相反 Alex Krizhevsky从这些图片
随机取了224*224

143
00:10:23,835 --> 00:10:28,375
的分块 这给他更多图片去训练

144
00:10:28,375 --> 00:10:31,270
也帮助他处理了平移和变化

145
00:10:31,270 --> 00:10:34,428
尽管它们是卷积网络

146
00:10:34,428 --> 00:10:38,573
但还是有用的 它也使用了图片的左右翻转

147
00:10:38,573 --> 00:10:41,600
这再一次加倍了数据量

148
00:10:41,600 --> 00:10:45,944
他没有使用角反射 
因为重力是很重要的

149
00:10:45,944 --> 00:10:51,004
左右翻转其实没有改变事物表面的样子 除非

150
00:10:51,004 --> 00:10:54,398
它们是书写类的事物
在测试的时候 他没有只用一个

151
00:10:54,398 --> 00:10:57,255
分块 他使用了很多不同的分块

152
00:10:57,255 --> 00:11:02,018
四个角 中间部分 这给他了五个区域
然后是所有的左右翻转

153
00:11:02,018 --> 00:11:06,304
这一共有十个 他在网络中用了所有的十张图片

154
00:11:06,304 --> 00:11:10,174
然后结合他们的观点 
在顶层 大多数参数在的地方

155
00:11:10,174 --> 00:11:14,341
他使用了一个新的规则化方法 叫做Dropout层

156
00:11:14,341 --> 00:11:18,157
这是很有用的 帮助防止网络过拟合

157
00:11:18,157 --> 00:11:21,160
这在他的结果中占了一定的比例

158
00:11:21,160 --> 00:11:25,710
我将在以后的课程中详细的描述Dropout

159
00:11:25,710 --> 00:11:30,568
但是现在 dropout的基本想法是
在每次给出一个训练样本的时候

160
00:11:30,568 --> 00:11:33,726
你从一层中忽略一半的隐藏单元

161
00:11:33,726 --> 00:11:38,645
这意味着其他存活下来的隐含单元不能依赖

162
00:11:38,645 --> 00:11:43,442
他们得到的总和
他们不能学会修补错误

163
00:11:43,442 --> 00:11:48,179
被其他隐含层遗留下来的错误
因为其他隐含单元可能

164
00:11:48,179 --> 00:11:52,188
被忽视了 无论是修复一个不存在的错误

165
00:11:52,188 --> 00:11:57,350
所以他们不得不变得更加利己
他们不得不各自做有用的事情

166
00:11:57,350 --> 00:12:02,224
但是他们仍然不得不去做有用的事情
不同于那些留存下来的单元做的

167
00:12:02,224 --> 00:12:05,247
所以dropout阻止了太多

168
00:12:05,247 --> 00:12:10,661
隐含单元之间的合作
很多合作对

169
00:12:10,661 --> 00:12:14,584
拟合数据很有用
但是如果测试集的分布

170
00:12:14,584 --> 00:12:18,926
显著不同 那么所有的合作
都会造成过拟合

171
00:12:18,926 --> 00:12:23,888
Alex没法不用强大的硬件来完成这个工作 但是硬件

172
00:12:23,888 --> 00:12:28,602
现在只需要几千美元 
Alex是一个很厉害的程序员

173
00:12:28,602 --> 00:12:33,750
它使用了高效的方法实现卷积和神经单元 在两个英伟达

174
00:12:33,750 --> 00:12:37,906
GTX 580的图形处理器上
每一个都有500个快速的核

175
00:12:37,906 --> 00:12:42,619
它们擅长于数学运算 而不擅长于其他任务

176
00:12:42,619 --> 00:12:45,869
图形处理器很擅长于做矩阵运算

177
00:12:45,869 --> 00:12:49,803
矩阵乘法 所以如果你把隐含层的激励向量堆在一起

178
00:12:49,803 --> 00:12:54,370
并基于很多训练样本 这样得到一个矩阵

179
00:12:54,370 --> 00:12:58,996
现在你将这个矩阵和权重矩阵相乘来得到

180
00:12:58,996 --> 00:13:01,983
下一隐含层的所有训练样本的激励

181
00:13:01,983 --> 00:13:06,140
如果这些矩阵都很大
GPU会有很大的优势

182
00:13:06,140 --> 00:13:10,159
他们给你大约30倍的速度
他们到内存间也有很高的带宽

183
00:13:10,159 --> 00:13:14,410
这对神经网络是很需要的
因为在神经网络中你一直希望

184
00:13:14,410 --> 00:13:17,676
得到另外一个权重 这样你可以用激励乘上它

185
00:13:17,676 --> 00:13:21,720
这里有上百万的权重 因此你无法将他们保存在高速缓存中

186
00:13:22,300 --> 00:13:28,750
用所有的这些硬件优势 他可以一周内训练出最终的网络

187
00:13:28,750 --> 00:13:33,075
你也可以把十个分块联系起来 在测试时

188
00:13:33,075 --> 00:13:36,124
很快 所以测试时你可以用帧频来运行

189
00:13:36,124 --> 00:13:39,229
在未来我们将能够

190
00:13:39,229 --> 00:13:42,390
将这种神经网络延伸到很多核心上

191
00:13:42,390 --> 00:13:46,660
随着核价格的降低 Google已经能用这个实验了

192
00:13:46,660 --> 00:13:51,318
而且如果我们能在状态间通讯的足够快 我们将能

193
00:13:51,318 --> 00:13:55,643
在更多的核心上做更大的网络
Google已经模拟了

194
00:13:55,643 --> 00:13:59,580
17亿连接的网络 我认为这还会继续增大

195
00:14:01,320 --> 00:14:06,417
随着核心变得越来越便宜 数据集越来越大 
这些深度神经网络将

196
00:14:06,417 --> 00:14:10,794
比老式计算机视觉系统改进得更快

197
00:14:10,794 --> 00:14:15,411
因为他们没有涉及太多的人工操作 他们可以更好地

198
00:14:15,411 --> 00:14:18,650
使用大规模数据集和大规模运算

199
00:14:18,650 --> 00:14:24,595
所以我们已经开启了一个大的突破口 我认为没有必要往回看了

200
00:14:24,595 --> 00:14:27,781
我想 从现在开始所有最好的物体

201
00:14:27,781 --> 00:14:33,160
识别系统 至少对于一些静态图片
将会使用大的深度神经网络

202
00:14:33,160 --> 00:14:39,106
这里有其他的应用领域 
我们可以学到同样的东西

203
00:14:39,106 --> 00:14:43,416
Vladimir Nee使用了一个本地区域的网络

204
00:14:43,416 --> 00:14:48,129
没有用卷积去从航拍照片中提取道路

205
00:14:48,129 --> 00:14:53,040
这些事很杂乱的城市地区的航拍图

206
00:14:53,040 --> 00:14:57,167
他也使用了多层修正线性单元

207
00:14:57,167 --> 00:15:02,991
他使用了一张相对大的图片分块 预测了中央16*16

208
00:15:02,991 --> 00:15:08,740
像素区域是道路的一块或者不是

209
00:15:08,740 --> 00:15:14,342
比较好的事情是训练数据有很多标签

210
00:15:14,342 --> 00:15:18,518
可用 这是因为地图告诉你哪里

211
00:15:18,518 --> 00:15:22,332
是道路中心线 哪里是差不多固定的宽度

212
00:15:22,332 --> 00:15:27,544
地图中的向量可以告诉你道路的中心线在哪

213
00:15:27,544 --> 00:15:30,595
你可以估测哪些像素可能是道路

214
00:15:30,595 --> 00:15:34,980
然而 这个任务很难
这通常有一个视觉

215
00:15:34,980 --> 00:15:39,684
问题 道路会被建筑物所遮挡
因为飞机不是

216
00:15:39,684 --> 00:15:42,417
垂直往下拍照的

217
00:15:42,417 --> 00:15:46,548
他们被树挡住了 也被那些

218
00:15:46,548 --> 00:15:50,597
停在路边的车挡住 建筑物旁的阴影

219
00:15:50,597 --> 00:15:55,338
主要的光照取决于是晴天还是阴天

220
00:15:55,338 --> 00:15:58,355
举个例子 这里有很小的视角点变化

221
00:15:58,355 --> 00:16:03,034
飞机基本上是朝下看的
但是在任何一个大的照片中它无法

222
00:16:03,034 --> 00:16:05,804
保证每一个像素点都是垂直向下看的

223
00:16:05,804 --> 00:16:09,313
数据中最坏的问题是不正确的标签

224
00:16:09,313 --> 00:16:13,623
你得到错误的标签 因为地图不是完美无缺的被记录

225
00:16:13,623 --> 00:16:18,610
对于大多数的目的 你不需要地图被记录精准到几米

226
00:16:18,610 --> 00:16:21,953
在这个数据中的一个像素大约是一平方米

227
00:16:21,953 --> 00:16:27,263
如果地图的记录偏差超过三米 你将会得到

228
00:16:27,263 --> 00:16:31,459
三个错误的标签 对每条路的像素来说

229
00:16:31,459 --> 00:16:36,507
另外 一个严重的问题
人们制作地图的时候不得不武断地

230
00:16:36,507 --> 00:16:40,900
决定哪个被记为道路 哪个被记为巷道

231
00:16:41,200 --> 00:16:45,947
因此在大多数的地图中 你看到一些东西
你不知道怎么判断

232
00:16:45,947 --> 00:16:49,557
这是被记为一条道路还是一条巷道

233
00:16:49,557 --> 00:16:54,104
因此你很容易困惑 不知道从地图中得到哪个标签

234
00:16:54,104 --> 00:16:59,185
在很大图片块上训练的大规模神经网络
使用几百万个样例

235
00:16:59,185 --> 00:17:03,264
我认为 这是在这个任务上真正的希望

236
00:17:03,264 --> 00:17:06,340
很难说人们可以做什么

237
00:17:06,340 --> 00:17:10,566
这里是数据的样子 这是多伦多的一部分

238
00:17:10,566 --> 00:17:14,929
如果你了解多伦多 你可以通过道路的角度来判断

239
00:17:14,929 --> 00:17:20,111
在多伦多照片上方 我放了从图片中提取的两个分块

240
00:17:20,111 --> 00:17:23,656
如果你看到那些分块 你可以

241
00:17:23,656 --> 00:17:27,405
知道很难去判断这是哪条路

242
00:17:27,405 --> 00:17:30,746
在右边 这是[UNKNOWN]系统的输出

243
00:17:30,746 --> 00:17:36,336
绿色是被正确识别出来的道路 红色代表系统

244
00:17:36,336 --> 00:17:39,200
认为可能是路 但是实际上不是

245
00:17:39,200 --> 00:17:43,471
事实上 那是一个停车场 
但是可以看到为什么它可能认为

246
00:17:43,471 --> 00:17:44,045
是一条路 翻译 Naiding Zhou