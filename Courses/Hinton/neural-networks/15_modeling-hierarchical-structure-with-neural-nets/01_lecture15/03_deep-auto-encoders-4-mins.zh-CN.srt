1
00:00:00,000 --> 00:00:05,055
在这一讲的视频中 我们将要探讨
深度自编码训练中的一些问题 

2
00:00:05,055 --> 00:00:08,797
人们在很久以前 大概20世纪80年代中期
就已经在考虑这些问题了 

3
00:00:08,797 --> 00:00:13,852
但是他们不能很好地进行训练 

4
00:00:13,852 --> 00:00:16,609
使深度自编码的效果能够
显著地超越主成分分析 

5
00:00:16,609 --> 00:00:21,664
有很多关于这些问题的论文被发表但是
没有一种方法表现出令人印象深刻的性能 

6
00:00:21,664 --> 00:00:25,144
在我们研发了一次预训练一层神经网络 

7
00:00:25,144 --> 00:00:28,230
的方法之后 

8
00:00:28,230 --> 00:00:33,128
Russ Salakhutdinov和我应用这些
方法来预训练深度自编码神经网络 

9
00:00:33,128 --> 00:00:37,468
并且第一次获得了比主成分分析更好的 

10
00:00:37,468 --> 00:00:41,560
深度自编码表示 

11
00:00:41,560 --> 00:00:46,517
深度自编码网络经常看上去
像是一个很好的降维方法 

12
00:00:46,517 --> 00:00:51,539
因为表面上看它们似乎
应该比主成分分析工作得更好 

13
00:00:51,539 --> 00:00:55,387
它们提供了灵活的双向映射机制 

14
00:00:55,387 --> 00:00:58,387
并且这种映射可以是非线性的 

15
00:00:58,387 --> 00:01:03,540
它们的学习时间复杂度应该是线性的
或者在某些情况下 是优于线性的 

16
00:01:03,540 --> 00:01:08,367
在学习完毕之后
网络中的编码过程是相当快速的 

17
00:01:08,367 --> 00:01:12,150
因为这只是每一层网络的
一次矩阵乘法运算而已 

18
00:01:12,150 --> 00:01:18,041
不幸的是 使用后向传播算法
优化深度自编码网络 

19
00:01:18,041 --> 00:01:21,498
是非常困难的 比较典型的做法
就是人们尝试使用小的初始权重 

20
00:01:21,498 --> 00:01:27,012
然后 与深度网络一样
后向传播梯度失败 

21
00:01:27,012 --> 00:01:31,364
他们永远也没有成功
但是现在我们有更好的方法 

22
00:01:31,364 --> 00:01:35,518
来优化它们
我们可以使用非监督的、以层为单位的 

23
00:01:35,518 --> 00:01:39,702
预训练 
或者我们可以更聪明地初始化权重 

24
00:01:39,702 --> 00:01:45,978
就像echo语句一样工作
第一个成功的深水编码网络 

25
00:01:45,978 --> 00:01:50,404
由Russ Salakhutdinov和我在2006年训练成功 

26
00:01:50,404 --> 00:01:56,197
我们将它们应用在了数字编码中
我们从784个像素的数字入手 

27
00:01:56,197 --> 00:01:59,657
之后我们使用三个隐藏层 

28
00:01:59,657 --> 00:02:05,210
将它们在中间层编码成30个实数单元 

29
00:02:05,210 --> 00:02:09,620
然后我们将这30个实数单元解码成 

30
00:02:09,620 --> 00:02:15,440
784个重构的像素点
我们使用一叠受限玻尔兹曼机 

31
00:02:15,440 --> 00:02:19,530
来初始化编码的权重 

32
00:02:19,530 --> 00:02:25,744
然后我们使用那些权重的转置
来初始化解码网络 

33
00:02:25,744 --> 00:02:30,348
因此 这784个像素点在一开始 

34
00:02:30,348 --> 00:02:36,799
就通过一个权重矩阵进行了重构
该矩阵只是编码网络的 

35
00:02:36,799 --> 00:02:42,448
权重矩阵的转置
但是在四个受限玻尔兹曼机 

36
00:02:42,448 --> 00:02:47,940
被训练以及展开
从而得到用于解码的转置矩阵之后 

37
00:02:47,940 --> 00:02:54,123
我们应用后向传播算法来最小化 

38
00:02:54,123 --> 00:02:57,335
这784个像素点的重构误差
在这里我们使用交叉熵误差 

39
00:02:57,335 --> 00:03:03,277
因为这些像素点被表示成了逻辑斯蒂单元 

40
00:03:03,277 --> 00:03:08,255
因此这些误差被后向传播到整个网络 

41
00:03:08,255 --> 00:03:12,030
我们一旦开始后向传播这些误差 

42
00:03:12,030 --> 00:03:17,331
用于重构像素点的权重
将会变得和编码这些像素点的权重不同 

43
00:03:17,331 --> 00:03:21,716
尽管看上去它们很相似 

44
00:03:21,716 --> 00:03:23,680
这工作的非常好 

45
00:03:24,760 --> 00:03:31,025
因此 如果你看第一行
这是从每个数字类别中随机抽样出来的数字 

46
00:03:31,025 --> 00:03:37,371
如果你看第二行 这是由深度自编码网络 

47
00:03:37,371 --> 00:03:43,320
利用中间层中30个线性隐藏神经元
重构完的上一行中对应的数字 

48
00:03:43,320 --> 00:03:47,886
数据被压缩成30个实数然后进行重构 

49
00:03:47,886 --> 00:03:52,391
如果你看一下8这个数字 你会发现重构出的8 

50
00:03:52,391 --> 00:03:56,279
更加的好
它摆脱了原始8中的瑕疵 

51
00:03:56,279 --> 00:03:59,735
因为它并没有编码这一瑕疵的能力 

52
00:03:59,735 --> 00:04:04,795
如果你和线性主成分分析进行比较
你会发现这一方法更好 

53
00:04:04,795 --> 00:04:07,449
一个到30个实数的线性映射 

54
00:04:07,449 --> 00:04:10,905
并不能工作地像它表示这些数据一样好
翻译：Lyndon Li