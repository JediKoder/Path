在这个视频中  我将描述一种组合大量
神经网络模型 而无须分别训练大量模型的新方法 这是一种叫做 “dropout”的方法 最近在赢得比赛中获得了很大的成功 对于每一个训练实例 我们随机省略了一些
隐结点 因此 对于不同的训练案例  我们以
使用不同结构的神经网络而结束 我们可以把它认为不同的训练案例有
不同的模型 然后 问题是我们如何能在只有一种训练案例时
训练一种模型 我们如何能在训练时把这些模型
有效地平均到一起（average together） 答案就是我们需要用大量的
权重共享的策略 我们想以结合 多个模型的输出来描述这
两种不同的方法 在混合模型中 我们通过平均输出概率来
结合模型 所以 如果模型A分配的概率是0.3 0.2 0.5
有三种不同的答案 模型B分配的概率是 0.1 0.8 0.1 结合的模型将简单地分配
概率的平均值 结合模型的一种不同的方法是用
概率的乘积 这里 我们对相同的概率取集合平均值 所以 像之前一样 模型A和模型B
将又分配相同的概率 但是现在 我们是这样做的 把概率
中每一对乘在一起 然后 取平方根 那是几何平均值 几何平均值通常总和小于1 所以 我们必须除以几何平均值的总和
来归一化概率的分布 以使它的加和能再一次等于 1 你将注意到在一个乘积中 一个模型的小的
概率输出 对于其他的模型具有否决权 现在我想描述 一种平均大量神经网络的有效的方法  该方法能给
我们一种做正确贝叶斯事情的可供替代的方法 这种可供替代的方法可能没有 像做正确的贝叶斯的事情那么好 但是
它却很实用 因此 考虑到含有一个隐藏层的神经网络
如右图所示 每一次我们展示一个训练的例子时 我们将要做的事随机地以 0.5 的概率 以 0.5 的概率省略（emit是否为omit）每一个
隐藏层中的单元 所以  这里我们删去隐藏层的三个单元 我们通过缺乏那些隐藏单元的网络
来运行这个实例 这意味着我们要从2^H种结构中随机进行采样 这里H是隐藏单元的数目 这是
大量的结构体系 当然 所有的这些结构表现的是权重 那就是说无论何时我们用一个隐藏层单元 
它将在其它的结构中获得相同的权重 所以 我们可以把 "dropout"认为是
一种模型平均值的形式 我们从2^H种模型中进行采样 事实上 这些模型中的绝大多数将从不
被采样 一个模型的采样将仅仅得到一个训练的实例 那是一种非常极端的包装形式 对于不同模型的
训练集都是不同的 但它们也都很小 所有这些模型之间的权重共享的策略
意味着每一个模型都被其他的模型 强烈的正则约束 这是一种 比L2范数或L1范数惩罚项更好的正则化矩阵
因子 使权重更加接近于0 通过与其他的模型共享权重 模型获得的正则约束将趋向于
把权重拉近正确的值 问题仍然是测试时间我们做什么 所以 我们能够采样很多结构 可能是一百 然后取输出分布的
几何平均值 但那将是很大的工作量 这是我们能做的很简单的事情 我们
用所有的隐藏层单元 但是我们有它们的输出权重 所以在采样时
它们实际上有和预期相同的效果 事实证明用全部隐藏神经元的
一半的输出权重 能准确地计算几何平均值 所有2^H个模型已经用到的预测值
假如我们用 softmax 的输出组 如果我们不仅有一层隐藏层 我们可以简单地在每一层用 以概率为0.5 的
"dropout"的策略 在测试时 我们有全部隐含层的输出权重 这给了我们我平时所说的
均值的网络 所以 我们用一个有所有单元
但权重减半的网络 当我们有多个隐含层时 这和计算每一个
“dropout”模型是不太一样的 但是它是一种好的近似而且
它的速度也很快 我们能用"dropout"的方法运行了很多随机
模型 然后对那些随机模型取平均 这个均值网络将有一个优点 在答案中它将给我们一个
不确定性的观点 输入层怎么样? 我们也能很好地在那里使用相同的技巧 我们在输入中使用“dropout”的策略 但我们用较高
的概率保持输入的值 这个技巧已经在叫做去噪的自动编码机中
所应用 该方法是由蒙特利尔大学的 Pascal Vincent, Hugo Laracholle和Yoshua Bengio所研制的 它取得的效果非常好 "dropout"方法是如何工作的? 由Alex Krizhevsky研发的破纪录的目标识别的网络 已经打破了记录 即使没有用到"dropout"的策略 但是使用"dropout"的策略能打破的更多一些 
通常来讲 如果你有一个深度神经网络 而且它是过度拟合的   "dropout"的策略 
将极大地减少错误的数目 我认为任何网络都需要提前终止 以防止过度拟合 但是使用"dropout"的方法
能做得更好 当然 这需要更长的训练时间
也需要对更多的隐含单元取均值 如果你得到一个深度神经网络并且
它不是过拟合的 你应该使用更大的网络并且使用
"dropout"的策略 那是假定你有足够的计算能力 这是思考"dropout"策略的另一种方式 这是
我最初想到的方法 你将看到它和混合专家的网络有点相关 当所有的专家合作时将出现什么问题 什么避免了专门化?  如果一个隐含神经元
知道其他哪几个隐含神经元起作用 它将会在训练集上相互适应其它的隐含的单元 这意味着什么  训练一个隐含的神经元的
真实的信号是 试着去修复当所有其它的隐含
神经元遗留的错误的说 那是反向传递来训练每一个隐含单元的权重 现在 这将造成在隐含神经元之间的
复杂的相互自适应 当数据发生变化时将出现什么问题 因此 一个新的测试数据集 如果你依赖复杂的相互自适应策略
在训练集上得到正确的事情 在新的测试集上很可能效果不会非常好 就像这个观点 一个大的复杂的密谋团体
总会包括一些确定会出错的人 因为总有一些你没有想到的事情 如果有大量的人被卷入其中 他们中总有人
会表现出意想不到的行为 于是其他的人也会做错事 如果你想密谋 最好有一些小的
密谋的团体 然后 当意外的事情发生时 这些小的密谋团体中的很多将失败 
但是它们中的一些仍会成功 因此 使用"dropout"的策略 我们会迫使
一个隐含的神经元 与许多其它集合中的隐含的单元组合地工作 这使它更有可能做对对个案有用的事情 而不仅仅是有用 由于其他的特别的隐含的单元
与它进行合作 但这也通常趋于去做对个体有用的事情 与其他隐含单元做的事情有一定的区别 它需要一些边际效应的事情 鉴于它的合作者通常会实现的事情 我想这是有"dropout"的策略的网络有
很好的表现性能的原因